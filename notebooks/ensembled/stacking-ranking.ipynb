{"cells":[{"cell_type":"markdown","metadata":{},"source":["### Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T21:30:42.290752Z","iopub.status.busy":"2021-12-13T21:30:42.290324Z","iopub.status.idle":"2021-12-13T21:30:44.813935Z","shell.execute_reply":"2021-12-13T21:30:44.813044Z","shell.execute_reply.started":"2021-12-13T21:30:42.290611Z"},"papermill":{"duration":7.63065,"end_time":"2021-12-13T15:32:53.613901","exception":false,"start_time":"2021-12-13T15:32:45.983251","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["from __future__ import annotations\n","\n","import numpy as np\n","import os\n","import random\n","import shutil\n","import statistics\n","import typing as t\n","from pathlib import Path\n","\n","import pandas as pd\n","import torch\n","import torch.nn.functional as torch_f\n","import typing_extensions as t_ext\n","import wandb\n","from torch.optim import Optimizer, AdamW\n","from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n","from torch.utils.data import Dataset, DataLoader, Sampler\n","from torch.utils.tensorboard.writer import SummaryWriter\n","from tqdm.notebook import tqdm\n","from transformers.models.auto.configuration_auto import AutoConfig\n","from transformers.models.auto.modeling_auto import AutoModel\n","from transformers.models.auto.tokenization_auto import AutoTokenizer\n","from transformers.optimization import get_cosine_schedule_with_warmup\n","from wandb.wandb_run import Run as WAndBRun"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def seed_everything(seed: int):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","\n","seed_everything(42)"]},{"cell_type":"markdown","metadata":{},"source":["### Datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T21:30:44.832059Z","iopub.status.busy":"2021-12-13T21:30:44.83171Z","iopub.status.idle":"2021-12-13T21:30:44.845976Z","shell.execute_reply":"2021-12-13T21:30:44.845075Z","shell.execute_reply.started":"2021-12-13T21:30:44.83202Z"},"papermill":{"duration":0.032712,"end_time":"2021-12-13T15:32:53.693077","exception":false,"start_time":"2021-12-13T15:32:53.660365","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["class RandomSubsetPerEpochSampler(Sampler[int]):\n","\n","    @staticmethod\n","    def _build_index(data_source: t.Sized) -> t.List[int]:\n","        index = list(range(len(data_source)))\n","        random.shuffle(index)\n","        return index\n","\n","    def __init__(self, data_source: t.Sized, samples_per_epoch: int):\n","        super().__init__(data_source)\n","        self._data_source = data_source\n","        self._samples_per_epoch = samples_per_epoch\n","        self._index: t.List[int] = self._build_index(data_source)\n","        self._real_epoch = 0\n","        self._step = -1\n","    \n","    def _sample_one(self) -> int:\n","        if not self._index:\n","            self._index = self._build_index(self._data_source)\n","            self._real_epoch += 1\n","        return self._index.pop()\n","\n","    def __iter__(self) -> t.Iterator[int]:\n","        self._step += 1\n","        return iter([self._sample_one() for _ in range(self._samples_per_epoch)])\n","\n","    def __len__(self) -> int:\n","        return self._samples_per_epoch\n","\n","    @property\n","    def step(self) -> int:\n","        return self._step\n","\n","    @property\n","    def real_epoch(self) -> int:\n","        return self._real_epoch\n","\n","    @property\n","    def frac_left(self) -> float:\n","        return len(self._index) / len(self._data_source)\n","\n","    @property\n","    def frac_consumed(self) -> float:\n","        return 1.0 - self.frac_left\n","\n","\n","class MetricBasedRandomSubsetPerEpochSampler(RandomSubsetPerEpochSampler):\n","\n","    def __init__(\n","            self,\n","            data_source: t.Sized,\n","            samples_per_epoch: int,\n","            thresholded_samples_per_epoch_list: t.List[t.Tuple[float, int]],):\n","        super().__init__(data_source, samples_per_epoch)\n","        self._samples_per_epoch_init = samples_per_epoch\n","        self._thresholded_samples_per_epoch_list = thresholded_samples_per_epoch_list\n","\n","    def adjust_samples_per_epoch(self, metric_val: float):\n","        self._samples_per_epoch = self._samples_per_epoch_init\n","        for threshold, samples_per_epoch in self._thresholded_samples_per_epoch_list:\n","            if metric_val >= threshold:\n","                self._samples_per_epoch = samples_per_epoch\n","\n","\n","class PairDataset(Dataset):\n","\n","    def __init__(\n","            self,\n","            pair_df: pd.DataFrame,\n","            feature_df: pd.DataFrame) -> None:\n","        super().__init__()\n","        self._pair_df = pair_df\n","        self._feature_df = feature_df\n","        self._feature_col_list = [col for col in list(self._feature_df.columns) if col.startswith('score_')]\n","        print(self._feature_col_list)\n","\n","    @property\n","    def num_features(self) -> int:\n","        return len(self._feature_col_list)\n","\n","    def __len__(self) -> int:\n","        return len(self._pair_df)\n","\n","    def __getitem__(self, idx: int) -> t.Tuple[torch.Tensor, torch.Tensor]:\n","        record = self._pair_df.iloc[idx]\n","        more_feature_record = self._feature_df[self._feature_df['comment_id'] == record['more_toxic_id']].iloc[0]\n","        less_feature_record = self._feature_df[self._feature_df['comment_id'] == record['less_toxic_id']].iloc[0]\n","        more_feature_tensor = torch.tensor([more_feature_record[col] for col in self._feature_col_list], dtype=torch.float32)\n","        less_feature_tensor = torch.tensor([less_feature_record[col] for col in self._feature_col_list], dtype=torch.float32)\n","        return more_feature_tensor, less_feature_tensor\n"]},{"cell_type":"markdown","metadata":{},"source":["### Model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T21:30:44.848904Z","iopub.status.busy":"2021-12-13T21:30:44.848506Z","iopub.status.idle":"2021-12-13T21:30:44.859529Z","shell.execute_reply":"2021-12-13T21:30:44.858898Z","shell.execute_reply.started":"2021-12-13T21:30:44.848854Z"},"papermill":{"duration":0.020746,"end_time":"2021-12-13T15:32:53.724218","exception":false,"start_time":"2021-12-13T15:32:53.703472","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["class Model(torch.nn.Module):\n","\n","    def __init__(self, num_features: int):\n","        super(Model, self).__init__()\n","        self.fc = torch.nn.Linear(num_features, 1)\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        return self.fc(x)"]},{"cell_type":"markdown","metadata":{},"source":["### Metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T21:30:44.861454Z","iopub.status.busy":"2021-12-13T21:30:44.860954Z","iopub.status.idle":"2021-12-13T21:30:44.878684Z","shell.execute_reply":"2021-12-13T21:30:44.877857Z","shell.execute_reply.started":"2021-12-13T21:30:44.861415Z"},"papermill":{"duration":0.02804,"end_time":"2021-12-13T15:32:53.762852","exception":false,"start_time":"2021-12-13T15:32:53.734812","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["class _Metric:\n","\n","    def compute(self) -> float:\n","        raise NotImplementedError()\n","\n","    def reset(self):\n","        raise NotImplementedError()\n","\n","    def compute_and_reset(self) -> float:\n","        value = self.compute()\n","        self.reset()\n","        return value\n","\n","\n","class Accuracy(_Metric):\n","\n","    def __init__(self):\n","        self._num_correct = 0\n","        self._num_total = 0\n","\n","    def update(self, is_correct_tensor: torch.Tensor):\n","        num_correct = (is_correct_tensor == 1).int().sum().item()\n","        num_total = num_correct + (is_correct_tensor != 1).int().sum().item()\n","        self._num_correct += num_correct\n","        self._num_total += num_total\n","\n","    def compute(self) -> float:\n","        assert self._num_total > 0\n","        return self._num_correct / self._num_total\n","\n","    def reset(self):\n","        self._num_correct = 0\n","        self._num_total = 0\n","\n","\n","class _FloatListMetric(_Metric):\n","\n","    def __init__(self):\n","        self._value_list: t.List[float] = []\n","\n","    def update(self, value_tensor: torch.Tensor):\n","        self._value_list.extend(value_tensor.flatten().tolist())\n","\n","    def reset(self):\n","        self._value_list.clear()\n","\n","\n","class FloatListMean(_FloatListMetric):\n","\n","    def compute(self) -> float:\n","        return statistics.mean(self._value_list)\n","\n","\n","class FloatListStd(_FloatListMetric):\n","\n","    def compute(self) -> float:\n","        return statistics.stdev(self._value_list)"]},{"cell_type":"markdown","metadata":{},"source":["### Loggers"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["_C = t.TypeVar('_C')\n","\n","\n","class ContextManagerList(t.Generic[_C]):\n","\n","    def __init__(self, cm_list: t.List[t.ContextManager[_C]]):\n","        self._cm_list = cm_list\n","\n","    def __enter__(self) -> t.List[_C]:\n","        return [cm.__enter__() for cm in self._cm_list]\n","\n","    def __exit__(self, *args, **kwargs):\n","        for cm in self._cm_list:\n","            cm.__exit__(*args, **kwargs)\n","\n","\n","class Logger:\n","\n","    def __enter__(self) -> Logger:\n","        return self\n","\n","    def __exit__(self, *args, **kwargs):\n","        pass\n","\n","    def log_params(self, params: t.Dict[str, t.Any]):\n","        raise NotImplementedError()\n","\n","    def log_metrics(self, step: int, metrics: t.Dict[str, float]):\n","        raise NotImplementedError()\n","\n","\n","class StdOutLogger(Logger):\n","\n","    def log_params(self, params: t.Dict[str, t.Any]):\n","        print('Using params:')\n","        for param in sorted(params.keys()):\n","            print(f'\\t{param} = {params[param]}')\n","\n","    def log_metrics(self, step: int, metrics: t.Dict[str, float]):\n","        print(f'Step {step} metrics:')\n","        for m in sorted(metrics.keys()):\n","            print(f'\\t{m} = {metrics[m]:.8f}')\n","\n","\n","class TensorBoardLogger(Logger):\n","\n","    def __init__(self, log_dir: str, metric_whitelist: t.Optional[t.Set[str]] = None) -> None:\n","        self._metric_whitelist = metric_whitelist\n","        self._writer = SummaryWriter(log_dir=log_dir)\n","\n","    def log_params(self, params: t.Dict[str, t.Any]):\n","        pass  # TODO: handle hyperparams properly.\n","\n","    def log_metrics(self, step: int, metrics: t.Dict[str, float]):\n","        for metric_key, metric_val in metrics.items():\n","            if self._metric_whitelist is None or metric_key in self._metric_whitelist:\n","                self._writer.add_scalar(tag=metric_key, scalar_value=metric_val, global_step=step)\n","\n","\n","class WAndBLogger(Logger):\n","\n","    def __init__(self, user_name: str, api_key: str, project: str, run_id: str):\n","        wandb.login(key=api_key)\n","        self._user_name = user_name\n","        self._project = project\n","        self._run_id = run_id\n","        self._run: t.Optional[WAndBRun] = None\n","    \n","    @property\n","    def run(self) -> WAndBRun:\n","        assert self._run is not None\n","        return self._run\n","\n","    def __enter__(self) -> WAndBLogger:\n","        self._run = wandb.init(project=self._project, entity=self._user_name, run_id=self._run_id)\n","        return self\n","    \n","    def log_params(self, params: t.Dict[str, t.Any]):\n","        self.run.config.update(params)\n","\n","    def log_metrics(self, step: int, metrics: t.Dict[str, float]):\n","        self.run.log(step=step, data=metrics)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Loss"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def margin_ranking_loss(\n","        more_scores: torch.Tensor,\n","        less_scores: torch.Tensor,\n","        margin: torch.Tensor,\n","        device: str,) -> torch.Tensor:\n","    return torch.maximum(torch.tensor(0.0, device=device), less_scores - more_scores + margin).mean()"]},{"cell_type":"markdown","metadata":{},"source":["### Iteration functions"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T21:30:44.881104Z","iopub.status.busy":"2021-12-13T21:30:44.880154Z","iopub.status.idle":"2021-12-13T21:30:44.914119Z","shell.execute_reply":"2021-12-13T21:30:44.913266Z","shell.execute_reply.started":"2021-12-13T21:30:44.881064Z"},"papermill":{"duration":0.044539,"end_time":"2021-12-13T15:32:53.818306","exception":false,"start_time":"2021-12-13T15:32:53.773767","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def do_train_iteration(\n","        data_loader: DataLoader,\n","        model: Model,\n","        device: str,\n","        optimizer: Optimizer,\n","        scheduler: CosineAnnealingWarmRestarts,\n","        train_margin_list: t.Optional[t.List[float]] = None,\n","        train_decision_margin: float = 0.3,\n","        accumulate_gradient_steps: int = 1) -> t.Tuple[float, t.Dict[str, float]]:\n","    sampler: RandomSubsetPerEpochSampler = t.cast(RandomSubsetPerEpochSampler, data_loader.sampler)\n","    loss_metric = FloatListMean()\n","    train_score_mean = FloatListMean()\n","    train_score_std = FloatListStd()\n","    if train_margin_list is None:\n","        train_margin_list = [train_decision_margin]\n","    assert train_decision_margin in train_margin_list\n","    train_accuracy_dict = {m: Accuracy() for m in train_margin_list}\n","\n","    model.train()\n","    data_iter = tqdm(enumerate(data_loader), desc='Training', total=len(data_loader))\n","    for step, (x_more, x_less) in data_iter:\n","        score_more = model(x_more.to(device))\n","        score_less = model(x_less.to(device))\n","        loss = margin_ranking_loss(score_more, score_less, margin=train_decision_margin, device=device)\n","        loss.backward()\n","\n","        if (step + 1) % accumulate_gradient_steps == 0:\n","            optimizer.step()\n","            optimizer.zero_grad()\n","            scheduler.step()\n","\n","        with torch.no_grad():\n","            score_more_cpu = score_more.cpu()\n","            train_score_mean.update(score_more_cpu)\n","            train_score_std.update(score_more_cpu)\n","            score_less_cpu = score_less.cpu()\n","            train_score_mean.update(score_less_cpu)\n","            train_score_std.update(score_less_cpu)\n","\n","            loss_metric.update(loss.cpu())\n","            for m, a in train_accuracy_dict.items():\n","                a.update(score_more - score_less > m)\n","        epoch_str = f'epoch: {sampler.real_epoch} [{sampler.frac_consumed:.4f}]'\n","        accuracy_str = ', '.join([f'acc_{m}: {train_accuracy_dict[m].compute():.4f}' for m in sorted(train_accuracy_dict.keys())])\n","        data_iter.set_description(\n","            f'{epoch_str} loss: {loss_metric.compute():.4f}, {accuracy_str} '\n","            f'score_mean: {train_score_mean.compute():.6f}, score_std: {train_score_std.compute():.6f}')\n","\n","    train_metrics_to_track = {f'train_accuracy_{m}': a.compute() for m, a in train_accuracy_dict.items()}\n","    loss_val = loss_metric.compute_and_reset()\n","    return train_accuracy_dict[0.0].compute(), {\n","        'train_loss': loss_val,\n","        'train_score_mean': train_score_mean.compute(),\n","        'train_score_std': train_score_std.compute(),\n","        **train_metrics_to_track,\n","    }"]},{"cell_type":"markdown","metadata":{},"source":["### Main function"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T21:30:45.729874Z","iopub.status.busy":"2021-12-13T21:30:45.729205Z","iopub.status.idle":"2021-12-13T21:30:45.744702Z","shell.execute_reply":"2021-12-13T21:30:45.74378Z","shell.execute_reply.started":"2021-12-13T21:30:45.729823Z"},"papermill":{"duration":0.028452,"end_time":"2021-12-13T15:32:56.900841","exception":false,"start_time":"2021-12-13T15:32:56.872389","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def main(\n","        pair_df: pd.DataFrame,\n","        feature_df: pd.DataFrame,\n","        to_checkpoint: str,\n","        logger_list: t.List[Logger],\n","        num_epochs: int,\n","        batch_size: int,\n","        num_workers: int,\n","        device: str,\n","        lr: float,\n","        num_warmup_steps: int,\n","        train_margin_list: t.List[float],\n","        train_decision_margin: float,\n","        validate_every_n_samples_init: t.Optional[int] = None,\n","        accumulate_gradient_steps: int = 1,):    \n","    train_dataset = PairDataset(\n","        pair_df=pair_df,\n","        feature_df=feature_df)\n","    sampler = RandomSubsetPerEpochSampler(\n","        data_source=train_dataset,\n","        samples_per_epoch=validate_every_n_samples_init if validate_every_n_samples_init else len(train_dataset))\n","    train_data_loader = DataLoader(\n","        dataset=train_dataset,\n","        sampler=sampler,\n","        batch_size=batch_size,\n","        num_workers=num_workers,\n","        pin_memory=True)\n","    model = Model(num_features=train_dataset.num_features).to(device)\n","    optimizer = AdamW(model.parameters(), lr=lr)\n","    scheduler = get_cosine_schedule_with_warmup(\n","        optimizer=optimizer,\n","        num_warmup_steps=num_warmup_steps,\n","        num_training_steps=len(train_data_loader) // accumulate_gradient_steps * num_epochs)\n","\n","    best_accuracy = 0.0\n","    while sampler.real_epoch < num_epochs:\n","        accuracy, train_metrics_to_track = do_train_iteration(\n","            data_loader=train_data_loader,\n","            model=model,\n","            device=device,\n","            optimizer=optimizer,\n","            scheduler=scheduler,\n","            train_margin_list=train_margin_list,\n","            train_decision_margin=train_decision_margin,\n","            accumulate_gradient_steps=accumulate_gradient_steps)\n","        for logger in logger_list:\n","            logger.log_metrics(step=sampler.step, metrics={**train_metrics_to_track,})\n","        if accuracy > best_accuracy:\n","            print(f'Best accuracy improved from {best_accuracy} to {accuracy}. Saving the model.')\n","            torch.save(model.state_dict(), to_checkpoint)\n","            best_accuracy = accuracy"]},{"cell_type":"markdown","metadata":{},"source":["### Parameter definitions"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T21:30:44.917294Z","iopub.status.busy":"2021-12-13T21:30:44.917102Z","iopub.status.idle":"2021-12-13T21:30:44.927821Z","shell.execute_reply":"2021-12-13T21:30:44.926808Z","shell.execute_reply.started":"2021-12-13T21:30:44.917271Z"},"papermill":{"duration":0.018228,"end_time":"2021-12-13T15:32:53.847052","exception":false,"start_time":"2021-12-13T15:32:53.828824","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Parameters\n","\n","IS_KAGGLE = False\n","\n","ROOT_DIR_PATH = Path('/kaggle') if IS_KAGGLE else Path('/home/jovyan/jigsaw-toxic')\n","DATA_DIR_PATH = ROOT_DIR_PATH / ('input' if IS_KAGGLE else 'data/datasets')\n","DATASET_DIR_PATH = DATA_DIR_PATH / 'external_20220207_stacking'\n","PAIR_CSV_PATH = DATASET_DIR_PATH / 'pair.csv'\n","FEATURE_CSV_PATH = DATASET_DIR_PATH / 'feature.csv'\n","TENSORBOARD_DIR_PATH = ROOT_DIR_PATH / 'working/tensorboard' if IS_KAGGLE else Path('/home/jovyan/tensorboard')\n","ARTIFACT_DIR_PATH = ROOT_DIR_PATH / 'working/artifacts' if IS_KAGGLE else ROOT_DIR_PATH / 'artifacts'\n","\n","TASK_NAME = 'stacking-ranking'\n","DATASET_NAME = 'external_20220207_stacking'\n","RUN_NAME = f'v1'\n","\n","MODEL_NAME = f'{DATASET_NAME}-{RUN_NAME}'\n","NOTEBOOK_CHECKPOINT_DIR_PATH = Path(f'.checkpoints/{TASK_NAME}')\n","MODELS_DIR_PATH = ROOT_DIR_PATH / ('working/models' if IS_KAGGLE else 'models')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if not IS_KAGGLE:\n","    os.makedirs(NOTEBOOK_CHECKPOINT_DIR_PATH, exist_ok=True)\n","    shutil.copyfile('stacking-ranking.ipynb', NOTEBOOK_CHECKPOINT_DIR_PATH / f'{RUN_NAME}.ipynb')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T21:30:44.929959Z","iopub.status.busy":"2021-12-13T21:30:44.929477Z","iopub.status.idle":"2021-12-13T21:30:45.598668Z","shell.execute_reply":"2021-12-13T21:30:45.597732Z","shell.execute_reply.started":"2021-12-13T21:30:44.929844Z"},"papermill":{"duration":0.676761,"end_time":"2021-12-13T15:32:54.534335","exception":false,"start_time":"2021-12-13T15:32:53.857574","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Create potentially missing directories\n","!mkdir -p $MODELS_DIR_PATH\n","!mkdir -p $ARTIFACT_DIR_PATH"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T21:30:45.600795Z","iopub.status.busy":"2021-12-13T21:30:45.600504Z","iopub.status.idle":"2021-12-13T21:30:45.703839Z","shell.execute_reply":"2021-12-13T21:30:45.703089Z","shell.execute_reply.started":"2021-12-13T21:30:45.600761Z"},"papermill":{"duration":2.228002,"end_time":"2021-12-13T15:32:56.773438","exception":false,"start_time":"2021-12-13T15:32:54.545436","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Read dataframes\n","\n","pair_df = t.cast(pd.DataFrame, pd.read_csv(PAIR_CSV_PATH))\n","feature_df = t.cast(pd.DataFrame, pd.read_csv(FEATURE_CSV_PATH))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pair_df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["feature_df"]},{"cell_type":"markdown","metadata":{},"source":["### Entrypoint"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T21:30:45.747655Z","iopub.status.busy":"2021-12-13T21:30:45.74677Z","iopub.status.idle":"2021-12-13T21:38:07.729693Z","shell.execute_reply":"2021-12-13T21:38:07.728387Z","shell.execute_reply.started":"2021-12-13T21:30:45.747614Z"},"papermill":{"duration":null,"end_time":null,"exception":false,"start_time":"2021-12-13T15:32:56.912308","status":"running"},"tags":[],"trusted":true},"outputs":[],"source":["# if IS_KAGGLE:\n","#     from kaggle_secrets import UserSecretsClient\n","#     user_secrets = UserSecretsClient()\n","#     wandb_api_key = user_secrets.get_secret('wandb-api-token')\n","# else:\n","#     with open(os.path.join(os.path.dirname(os.getcwd()), 'deploy/secrets/wandb_api_token.txt')) as f:\n","#         wandb_api_key = f.read()\n","\n","BATCH_SIZE = 128\n","ACCUMULATE_GRAD_STEPS = 1\n","VALID_CYCLES_PER_EPOCH = 1\n","\n","with ContextManagerList([\n","            # StdOutLogger(),\n","            TensorBoardLogger(\n","                log_dir=str(TENSORBOARD_DIR_PATH / f'jt-{MODEL_NAME}'),\n","                metric_whitelist={\n","                    'train_loss',\n","                    'train_accuracy_0.0',\n","                },\n","            ),\n","        ]) as logger_list:\n","    main(\n","        pair_df=pair_df,\n","        feature_df=feature_df,\n","        to_checkpoint=str(MODELS_DIR_PATH / f'{MODEL_NAME}.pt'),\n","        logger_list=logger_list,\n","        num_epochs=50,\n","        batch_size=BATCH_SIZE,\n","        num_workers=2,\n","        device='cuda',\n","        lr=1e-3,\n","        num_warmup_steps=len(pair_df) // (BATCH_SIZE * ACCUMULATE_GRAD_STEPS * VALID_CYCLES_PER_EPOCH),\n","        train_margin_list=[0.0],\n","        train_decision_margin=0.0,\n","        accumulate_gradient_steps=ACCUMULATE_GRAD_STEPS,\n","        validate_every_n_samples_init=len(pair_df) // VALID_CYCLES_PER_EPOCH,\n","    )"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":4}
