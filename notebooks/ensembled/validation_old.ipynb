{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import typing as t\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as torch_f\n",
    "import typing_extensions as t_ext\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers.models.auto.modeling_auto import AutoModel\n",
    "from transformers.models.auto.tokenization_auto import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Old dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _TokenizedText(t_ext.TypedDict):\n",
    "    input_ids: torch.Tensor\n",
    "    attention_mask: torch.Tensor\n",
    "\n",
    "\n",
    "def _preprocess_tokenizer_output(output: t.Dict[str, t.Any]) -> _TokenizedText:\n",
    "    return {\n",
    "        'input_ids': torch.tensor(output['input_ids']),\n",
    "        'attention_mask': torch.tensor(output['attention_mask']),\n",
    "    }\n",
    "\n",
    "\n",
    "def _split_str_to_chunk_list(s: str, chunk_size: int) -> t.List[str]:\n",
    "    chunk_list = []\n",
    "    chunk = []\n",
    "    for token in s.split(' '):\n",
    "        chunk.append(token)\n",
    "        if len(chunk) >= chunk_size:\n",
    "            chunk_list.append(' '.join(chunk))\n",
    "            chunk.clear()\n",
    "    if chunk:\n",
    "        chunk_list.append(' '.join(chunk))\n",
    "    return chunk_list\n",
    "\n",
    "\n",
    "def valid_collate_fn(\n",
    "        sample_list: t.List[t.Tuple[int, _TokenizedText, _TokenizedText]]\n",
    "        ) -> t.Tuple[t.List[int], _TokenizedText, _TokenizedText, t.List[slice], t.List[slice]]:\n",
    "    curr_pos_more, curr_pos_less = 0, 0\n",
    "\n",
    "    idx_list: t.List[int] = []\n",
    "    more_input_ids_list, less_input_ids_list = [], []\n",
    "    more_attention_mask_list, less_attention_mask_list = [], []\n",
    "    more_slice_list: t.List[slice] = []\n",
    "    less_slice_list: t.List[slice] = []\n",
    "    \n",
    "    for sample in sample_list:\n",
    "        idx_list.append(sample[0])\n",
    "        more_input_ids, more_attention_mask = sample[1]['input_ids'], sample[1]['attention_mask']\n",
    "        less_input_ids, less_attention_mask = sample[2]['input_ids'], sample[2]['attention_mask']\n",
    "        more_input_ids_list.append(more_input_ids)\n",
    "        less_input_ids_list.append(less_input_ids)\n",
    "        more_attention_mask_list.append(more_attention_mask)\n",
    "        less_attention_mask_list.append(less_attention_mask)\n",
    "        more_slice_list.append(slice(curr_pos_more, curr_pos_more + more_input_ids.shape[0]))\n",
    "        curr_pos_more += more_input_ids.shape[0]\n",
    "        less_slice_list.append(slice(curr_pos_less, curr_pos_less + less_input_ids.shape[0]))\n",
    "        curr_pos_less += less_input_ids.shape[0]\n",
    "\n",
    "    more_tokenized_collated: _TokenizedText = {\n",
    "        'input_ids': torch.cat(more_input_ids_list, dim=0),\n",
    "        'attention_mask': torch.cat(more_attention_mask_list, dim=0),\n",
    "    }\n",
    "    less_tokenized_collated: _TokenizedText = {\n",
    "        'input_ids': torch.cat(less_input_ids_list, dim=0),\n",
    "        'attention_mask': torch.cat(less_attention_mask_list, dim=0),\n",
    "    }\n",
    "\n",
    "    return idx_list, more_tokenized_collated, less_tokenized_collated, more_slice_list, less_slice_list\n",
    "\n",
    "\n",
    "class ValidDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame, tokenizer: AutoTokenizer, max_len: int) -> None:\n",
    "        super().__init__()\n",
    "        self._df = df\n",
    "        self._tokenizer = tokenizer\n",
    "        self._max_len = max_len\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._df)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> t.Tuple[int, _TokenizedText, _TokenizedText]:\n",
    "        record = self._df.iloc[idx]\n",
    "        text_more = str(record['more_toxic'])\n",
    "        text_less = str(record['less_toxic'])\n",
    "\n",
    "        more_input_ids_list, less_input_ids_list = [], []\n",
    "        more_attention_mask_list, less_attention_mask_list = [], []\n",
    "        for chunk in _split_str_to_chunk_list(text_more, chunk_size=self._max_len):\n",
    "            tokenized_chunk = _preprocess_tokenizer_output(self._tokenizer(\n",
    "                chunk,\n",
    "                add_special_tokens=True,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=self._max_len,\n",
    "                return_attention_mask=True))  # type: ignore\n",
    "            more_input_ids_list.append(tokenized_chunk['input_ids'])\n",
    "            more_attention_mask_list.append(tokenized_chunk['attention_mask'])\n",
    "        for chunk in _split_str_to_chunk_list(text_less, chunk_size=self._max_len):\n",
    "            tokenized_chunk = _preprocess_tokenizer_output(self._tokenizer(\n",
    "                chunk,\n",
    "                add_special_tokens=True,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=self._max_len,\n",
    "                return_attention_mask=True))  # type: ignore\n",
    "            less_input_ids_list.append(tokenized_chunk['input_ids'])\n",
    "            less_attention_mask_list.append(tokenized_chunk['attention_mask'])\n",
    "\n",
    "        tokenized_more: _TokenizedText = {\n",
    "            'input_ids': torch.stack(more_input_ids_list, dim=0),\n",
    "            'attention_mask': torch.stack(more_attention_mask_list, dim=0),\n",
    "        }\n",
    "        tokenized_less: _TokenizedText = {\n",
    "            'input_ids': torch.stack(less_input_ids_list, dim=0),\n",
    "            'attention_mask': torch.stack(less_attention_mask_list, dim=0),\n",
    "        }\n",
    "\n",
    "        return idx, tokenized_more, tokenized_less"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _TokenizedText(t_ext.TypedDict):\n",
    "    input_ids: torch.Tensor\n",
    "    attention_mask: torch.Tensor\n",
    "\n",
    "\n",
    "def _preprocess_tokenizer_output(output: t.Dict[str, t.Any]) -> _TokenizedText:\n",
    "    return {\n",
    "        'input_ids': torch.tensor(output['input_ids']),\n",
    "        'attention_mask': torch.tensor(output['attention_mask']),\n",
    "    }\n",
    "\n",
    "\n",
    "def _split_str_to_chunk_list(s: str, chunk_size: int) -> t.List[str]:\n",
    "    chunk_list = []\n",
    "    chunk = []\n",
    "    for token in s.split(' '):\n",
    "        chunk.append(token)\n",
    "        if len(chunk) >= chunk_size:\n",
    "            chunk_list.append(' '.join(chunk))\n",
    "            chunk.clear()\n",
    "    if chunk:\n",
    "        chunk_list.append(' '.join(chunk))\n",
    "    return chunk_list\n",
    "\n",
    "\n",
    "def valid_collate_fn(\n",
    "        sample_list: t.List[t.Tuple[int, _TokenizedText, _TokenizedText]]\n",
    "        ) -> t.Tuple[t.List[int], _TokenizedText, _TokenizedText, t.List[slice], t.List[slice]]:\n",
    "    curr_pos_more, curr_pos_less = 0, 0\n",
    "\n",
    "    idx_list: t.List[int] = []\n",
    "    more_input_ids_list, less_input_ids_list = [], []\n",
    "    more_attention_mask_list, less_attention_mask_list = [], []\n",
    "    more_slice_list: t.List[slice] = []\n",
    "    less_slice_list: t.List[slice] = []\n",
    "    \n",
    "    for sample in sample_list:\n",
    "        idx_list.append(sample[0])\n",
    "        more_input_ids, more_attention_mask = sample[1]['input_ids'], sample[1]['attention_mask']\n",
    "        less_input_ids, less_attention_mask = sample[2]['input_ids'], sample[2]['attention_mask']\n",
    "        more_input_ids_list.append(more_input_ids)\n",
    "        less_input_ids_list.append(less_input_ids)\n",
    "        more_attention_mask_list.append(more_attention_mask)\n",
    "        less_attention_mask_list.append(less_attention_mask)\n",
    "        more_slice_list.append(slice(curr_pos_more, curr_pos_more + more_input_ids.shape[0]))\n",
    "        curr_pos_more += more_input_ids.shape[0]\n",
    "        less_slice_list.append(slice(curr_pos_less, curr_pos_less + less_input_ids.shape[0]))\n",
    "        curr_pos_less += less_input_ids.shape[0]\n",
    "\n",
    "    more_tokenized_collated: _TokenizedText = {\n",
    "        'input_ids': torch.cat(more_input_ids_list, dim=0),\n",
    "        'attention_mask': torch.cat(more_attention_mask_list, dim=0),\n",
    "    }\n",
    "    less_tokenized_collated: _TokenizedText = {\n",
    "        'input_ids': torch.cat(less_input_ids_list, dim=0),\n",
    "        'attention_mask': torch.cat(less_attention_mask_list, dim=0),\n",
    "    }\n",
    "\n",
    "    return idx_list, more_tokenized_collated, less_tokenized_collated, more_slice_list, less_slice_list\n",
    "\n",
    "\n",
    "class ValidDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame, tokenizer: AutoTokenizer, max_len: int) -> None:\n",
    "        super().__init__()\n",
    "        self._df = df\n",
    "        self._tokenizer = tokenizer\n",
    "        self._max_len = max_len\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._df)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> t.Tuple[int, _TokenizedText, _TokenizedText]:\n",
    "        record = self._df.iloc[idx]\n",
    "        text_more = str(record['more_toxic'])\n",
    "        text_less = str(record['less_toxic'])\n",
    "\n",
    "        more_input_ids_list, less_input_ids_list = [], []\n",
    "        more_attention_mask_list, less_attention_mask_list = [], []\n",
    "        for chunk in _split_str_to_chunk_list(text_more, chunk_size=self._max_len):\n",
    "            tokenized_chunk = _preprocess_tokenizer_output(self._tokenizer(\n",
    "                chunk,\n",
    "                add_special_tokens=True,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=self._max_len,\n",
    "                return_attention_mask=True))  # type: ignore\n",
    "            more_input_ids_list.append(tokenized_chunk['input_ids'])\n",
    "            more_attention_mask_list.append(tokenized_chunk['attention_mask'])\n",
    "        for chunk in _split_str_to_chunk_list(text_less, chunk_size=self._max_len):\n",
    "            tokenized_chunk = _preprocess_tokenizer_output(self._tokenizer(\n",
    "                chunk,\n",
    "                add_special_tokens=True,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=self._max_len,\n",
    "                return_attention_mask=True))  # type: ignore\n",
    "            less_input_ids_list.append(tokenized_chunk['input_ids'])\n",
    "            less_attention_mask_list.append(tokenized_chunk['attention_mask'])\n",
    "\n",
    "        tokenized_more: _TokenizedText = {\n",
    "            'input_ids': torch.stack(more_input_ids_list, dim=0),\n",
    "            'attention_mask': torch.stack(more_attention_mask_list, dim=0),\n",
    "        }\n",
    "        tokenized_less: _TokenizedText = {\n",
    "            'input_ids': torch.stack(less_input_ids_list, dim=0),\n",
    "            'attention_mask': torch.stack(less_attention_mask_list, dim=0),\n",
    "        }\n",
    "\n",
    "        return idx, tokenized_more, tokenized_less"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def predict_scores(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class ModelConfig(t.NamedTuple):\n",
    "    name: str\n",
    "    model: Model\n",
    "    tokenizer: AutoTokenizer\n",
    "\n",
    "\n",
    "def import_checkpoint(model: torch.nn.Module, checkpoint: str, device: str):\n",
    "    model.load_state_dict(torch.load(checkpoint, map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CCC 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _WeightedAverageLinearRegressor(torch.nn.Linear):\n",
    "\n",
    "    def __init__(self, in_features: int, device: t.Optional[str] = None, dtype: t.Optional[str] = None):\n",
    "        super().__init__(in_features=in_features, out_features=1, bias=False, device=device, dtype=dtype)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return torch_f.linear(x, torch_f.softmax(self.weight, dim=1), self.bias)\n",
    "\n",
    "\n",
    "class _CCC2017Model(Model):\n",
    "    \"\"\"\n",
    "    ccc-2017-multilabel-harder-cls-loss_0p5-v2-valfreq_dynamic_v1\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, checkpoint: str, output_logits: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(checkpoint, return_dict=False)\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            # torch.nn.LayerNorm(output_logits),\n",
    "            torch.nn.Linear(output_logits, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, num_classes))\n",
    "        self.regressor = _WeightedAverageLinearRegressor(in_features=num_classes)\n",
    "\n",
    "    def forward_scores(self, label_preds: torch.Tensor) -> torch.Tensor:\n",
    "        return self.regressor(label_preds)\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> t.Tuple[torch.Tensor, torch.Tensor]:\n",
    "        _, pooled_output = self.encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask)\n",
    "        label_preds = self.classifier(pooled_output)\n",
    "        scores = self.forward_scores(torch.sigmoid(label_preds))\n",
    "        return label_preds, scores\n",
    "\n",
    "    def predict_scores(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        return self.forward(input_ids, attention_mask)[1]\n",
    "\n",
    "\n",
    "def load_ccc2017(device: str) -> ModelConfig:\n",
    "    model = _CCC2017Model('roberta-base', 768, 6)\n",
    "    import_checkpoint(model, '/home/jovyan/jigsaw-toxic/models/ccc-2017-multilabel-harder-cls-loss_0p5-v2-valfreq_dynamic_v1.pt', device=device)\n",
    "    return ModelConfig(\n",
    "        name='ccc-2017-multilabel-harder-cls-loss_0p5-v2-valfreq_dynamic_v1',\n",
    "        model=model,\n",
    "        tokenizer=AutoTokenizer.from_pretrained('roberta-base'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UBTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _UBTCModel(Model):\n",
    "    \"\"\"\n",
    "    ubtc-multireg-w50-cos_warmup-opt-2ep-ut_roberta-seed_42\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, checkpoint: str, output_logits: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(checkpoint, return_dict=False)\n",
    "        self.feature_regressor = torch.nn.Sequential(\n",
    "            # torch.nn.LayerNorm(output_logits),\n",
    "            torch.nn.Linear(output_logits, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, num_classes),\n",
    "            torch.nn.Sigmoid())\n",
    "        self.score_regressor = torch.nn.Sequential(\n",
    "            # torch.nn.LayerNorm(output_logits),\n",
    "            torch.nn.Linear(num_classes, 1),\n",
    "            torch.nn.Sigmoid())\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> t.Tuple[torch.Tensor, torch.Tensor]:\n",
    "        _, pooled_output = self.encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask)\n",
    "        features = self.feature_regressor(pooled_output)\n",
    "        return features, self.score_regressor(features)\n",
    "\n",
    "    def predict_scores(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        return self.forward(input_ids, attention_mask)[1]\n",
    "\n",
    "\n",
    "def load_ubtc(device: str) -> ModelConfig:\n",
    "    model = _UBTCModel('unitary/unbiased-toxic-roberta', 768, 7)\n",
    "    import_checkpoint(model, '/home/jovyan/jigsaw-toxic/models/ubtc-multireg-w50-cos_warmup-opt-2ep-ut_roberta-seed_42.pt', device=device)\n",
    "    return ModelConfig(\n",
    "        name='ubtc-multireg-w50-cos_warmup-opt-2ep-ut_roberta-seed_42',\n",
    "        model=model,\n",
    "        tokenizer=AutoTokenizer.from_pretrained('unitary/unbiased-toxic-roberta'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ruddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _RudditModel(Model):\n",
    "\n",
    "    def __init__(self, checkpoint: str, output_logits: int, dropout: float):\n",
    "        super(Model, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(checkpoint, return_dict=False)\n",
    "        self.regressor = torch.nn.Sequential(\n",
    "            # torch.nn.LayerNorm(output_logits),\n",
    "            torch.nn.Linear(output_logits, 1),\n",
    "            torch.nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        _, pooled_output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask)\n",
    "        return self.regressor(pooled_output)\n",
    "\n",
    "    def predict_scores(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        return self.forward(input_ids, attention_mask)\n",
    "\n",
    "\n",
    "def load_ruddit(device: str) -> ModelConfig:\n",
    "    model = _RudditModel('roberta-base', 768, 0.6)\n",
    "    import_checkpoint(model, '/home/jovyan/jigsaw-toxic/models/ruddit-v3-mse-2ep-pure_reg.pt', device=device)\n",
    "    return ModelConfig(\n",
    "        name='ruddit-v3-mse-2ep-pure_reg',\n",
    "        model=model,\n",
    "        tokenizer=AutoTokenizer.from_pretrained('roberta-base'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Offenseval 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _OffensevalModel(Model):\n",
    "\n",
    "    def __init__(self, checkpoint: str, output_logits: int, dropout: float):\n",
    "        super(Model, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(checkpoint, return_dict=False)\n",
    "        self.regressor = torch.nn.Sequential(\n",
    "            # torch.nn.LayerNorm(output_logits),\n",
    "            torch.nn.Linear(output_logits, 1),\n",
    "            torch.nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        _, pooled_output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask)\n",
    "        return self.regressor(pooled_output)\n",
    "\n",
    "    def predict_scores(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        return self.forward(input_ids, attention_mask)\n",
    "\n",
    "\n",
    "def load_offenseval(device: str) -> ModelConfig:\n",
    "    model = _OffensevalModel('roberta-base', 768, 0.6)\n",
    "    import_checkpoint(model, '/home/jovyan/jigsaw-toxic/models/offenseval-2020-v2-pure_reg-mse-1_ep-64_valcycles-lr_2e5-backbone_utr.pt', device=device)\n",
    "    return ModelConfig(\n",
    "        name='offenseval-2020-v2-pure_reg-mse-1_ep-64_valcycles-lr_2e5-backbone_utr',\n",
    "        model=model,\n",
    "        tokenizer=AutoTokenizer.from_pretrained('unitary/unbiased-toxic-roberta'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_valid_iteration(\n",
    "        data_loader: DataLoader,\n",
    "        model: Model,\n",
    "        model_name: str,\n",
    "        device: str) -> t.Tuple[torch.Tensor, torch.Tensor]:\n",
    "    model.eval()\n",
    "    more_score_list, less_score_list = [], []\n",
    "    with torch.no_grad():\n",
    "        it = tqdm(data_loader, desc=model_name)\n",
    "        for _, tokenized_text_more, tokenized_text_less, slice_list_more, slice_list_less in it:\n",
    "            score_more = model.predict_scores(\n",
    "                tokenized_text_more['input_ids'].to(device),\n",
    "                tokenized_text_more['attention_mask'].to(device),)\n",
    "            score_less = model.predict_scores(\n",
    "                tokenized_text_less['input_ids'].to(device),\n",
    "                tokenized_text_less['attention_mask'].to(device),)\n",
    "            score_more = torch.cat([torch.max(score_more[s], dim=0, keepdim=True)[0] for s in slice_list_more], dim=0)\n",
    "            score_less = torch.cat([torch.max(score_less[s], dim=0, keepdim=True)[0] for s in slice_list_less], dim=0)\n",
    "            more_score_list.extend(score_more.cpu().flatten().tolist())\n",
    "            less_score_list.extend(score_less.cpu().flatten().tolist())\n",
    "                \n",
    "    return torch.tensor(more_score_list), torch.tensor(less_score_list)\n",
    "\n",
    "\n",
    "def ensemble_scores(score_list: t.List[torch.Tensor]) -> torch.Tensor:\n",
    "    return sum(score_list) / len(score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_by_model(\n",
    "        valid_df: pd.DataFrame,\n",
    "        batch_size: int,\n",
    "        model_getter: t.Callable[[str], ModelConfig],\n",
    "        max_len: int,\n",
    "        num_workers: int,\n",
    "        device: str) -> t.Tuple[torch.Tensor, torch.Tensor]:\n",
    "    model_config = model_getter(device)\n",
    "    model = model_config.model.to(device)\n",
    "    # model.load_state_dict(torch.load(model_config['tokenizer_checkpoint'], map_location=device))\n",
    "    dataset = ValidDataset(\n",
    "        df=valid_df,\n",
    "        tokenizer=model_config.tokenizer,\n",
    "        max_len=max_len)\n",
    "    data_loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=valid_collate_fn,  # type: ignore\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=device.startswith('cuda'))\n",
    "    return do_valid_iteration(data_loader=data_loader, model=model, model_name=model_config.name, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df = pd.read_csv('/home/jovyan/jigsaw-toxic/data/jigsaw-toxic-severity-rating/valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a1e681621dd4a5b802d93218b230cf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ccc-2017-multilabel-harder-cls-loss_0p5-v2-valfreq_dynamic_v1:   0%|          | 0/1264 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ccc2017_more, ccc2017_less = predict_by_model(\n",
    "    valid_df=valid_df,\n",
    "    batch_size=8,\n",
    "    model_getter=load_ccc2017,\n",
    "    num_workers=8,\n",
    "    max_len=256,\n",
    "    device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at unitary/unbiased-toxic-roberta were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at unitary/unbiased-toxic-roberta and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39dcefd4b7384334a6ea63ad09e0748e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ubtc-multireg-w50-cos_warmup-opt-2ep-ut_roberta-seed_42:   0%|          | 0/1264 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ubtc_more, ubtc_less = predict_by_model(\n",
    "    valid_df=valid_df,\n",
    "    batch_size=8,\n",
    "    model_getter=load_ubtc,\n",
    "    num_workers=8,\n",
    "    max_len=256,\n",
    "    device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea5bad9d401346689c119fcaf7f092c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ruddit-v3-mse-2ep-pure_reg:   0%|          | 0/1264 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ruddit_more, ruddit_less = predict_by_model(\n",
    "    valid_df=valid_df,\n",
    "    batch_size=8,\n",
    "    model_getter=load_ruddit,\n",
    "    num_workers=8,\n",
    "    max_len=256,\n",
    "    device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8478b9bb264648a6927617dda824d769",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "offenseval-2020-v2-pure_reg-mse-1_ep-64_valcycles-lr_2e5-backbone_utr:   0%|          | 0/1264 [00:00<?, ?it/s…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "offenseval_more, offenseval_less = predict_by_model(\n",
    "    valid_df=valid_df,\n",
    "    batch_size=8,\n",
    "    model_getter=load_offenseval,\n",
    "    num_workers=8,\n",
    "    max_len=256,\n",
    "    device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7546)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "more_score = ensemble_scores([\n",
    "    ccc2017_more,\n",
    "    ubtc_more,\n",
    "    ruddit_more,\n",
    "    # offenseval_more,\n",
    "])\n",
    "less_score = ensemble_scores([\n",
    "    ccc2017_less,\n",
    "    ubtc_less,\n",
    "    ruddit_less,\n",
    "    # offenseval_less,\n",
    "])\n",
    "(more_score > less_score).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7546)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(more_score > less_score).float().mean()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
