{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import functools\n",
    "import operator\n",
    "import random\n",
    "import statistics\n",
    "import typing as t\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as torch_f\n",
    "import typing_extensions as t_ext\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers.models.auto.modeling_auto import AutoModel\n",
    "from transformers.models.auto.tokenization_auto import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _TokenizedText(t_ext.TypedDict):\n",
    "    input_ids: torch.Tensor\n",
    "    attention_mask: torch.Tensor\n",
    "\n",
    "\n",
    "def _preprocess_tokenizer_output(output: t.Dict[str, t.Any]) -> _TokenizedText:\n",
    "    return {\n",
    "        'input_ids': torch.tensor(output['input_ids']),\n",
    "        'attention_mask': torch.tensor(output['attention_mask']),\n",
    "    }\n",
    "\n",
    "\n",
    "def _split_str_to_chunk_list(s: str, chunk_size: int) -> t.List[str]:\n",
    "    chunk_list = []\n",
    "    chunk = []\n",
    "    for token in s.split(' '):\n",
    "        chunk.append(token)\n",
    "        if len(chunk) >= chunk_size:\n",
    "            chunk_list.append(' '.join(chunk))\n",
    "            chunk.clear()\n",
    "    if chunk:\n",
    "        chunk_list.append(' '.join(chunk))\n",
    "    return chunk_list\n",
    "\n",
    "\n",
    "def predict_collate_fn(\n",
    "        sample_list: t.List[t.Tuple[str, _TokenizedText]]\n",
    "        ) -> t.Tuple[t.List[str], _TokenizedText, t.List[slice]]:\n",
    "    curr_pos = 0\n",
    "\n",
    "    idx_list: t.List[str] = []\n",
    "    input_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    slice_list: t.List[slice] = []\n",
    "    \n",
    "    for sample in sample_list:\n",
    "        idx_list.append(sample[0])\n",
    "        input_ids, attention_mask = sample[1]['input_ids'], sample[1]['attention_mask']\n",
    "        input_ids_list.append(input_ids)\n",
    "        attention_mask_list.append(attention_mask)\n",
    "        slice_list.append(slice(curr_pos, curr_pos + input_ids.shape[0]))\n",
    "        curr_pos += input_ids.shape[0]\n",
    "\n",
    "    tokenized_collated: _TokenizedText = {\n",
    "        'input_ids': torch.cat(input_ids_list, dim=0),\n",
    "        'attention_mask': torch.cat(attention_mask_list, dim=0),\n",
    "    }\n",
    "\n",
    "    return idx_list, tokenized_collated, slice_list\n",
    "\n",
    "\n",
    "class PredictDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame, tokenizer: AutoTokenizer, max_len: int) -> None:\n",
    "        super().__init__()\n",
    "        self._df = df\n",
    "        self._tokenizer = tokenizer\n",
    "        self._max_len = max_len\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._df)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> t.Tuple[str, _TokenizedText]:\n",
    "        record = self._df.iloc[idx]\n",
    "        comment_id, text = str(record['comment_id']), str(record['text'])\n",
    "\n",
    "        input_ids_list, attention_mask_list = [], []\n",
    "        for chunk in _split_str_to_chunk_list(text, chunk_size=self._max_len):\n",
    "            tokenized_chunk = _preprocess_tokenizer_output(self._tokenizer(\n",
    "                chunk,\n",
    "                add_special_tokens=True,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=self._max_len,\n",
    "                return_attention_mask=True))  # type: ignore\n",
    "            input_ids_list.append(tokenized_chunk['input_ids'])\n",
    "            attention_mask_list.append(tokenized_chunk['attention_mask'])\n",
    "\n",
    "        tokenized_text: _TokenizedText = {\n",
    "            'input_ids': torch.stack(input_ids_list, dim=0),\n",
    "            'attention_mask': torch.stack(attention_mask_list, dim=0),\n",
    "        }\n",
    "\n",
    "        return comment_id, tokenized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def predict_scores(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class ModelConfig(t.NamedTuple):\n",
    "    name: str\n",
    "    model: Model\n",
    "    tokenizer: AutoTokenizer\n",
    "\n",
    "\n",
    "def import_checkpoint(model: torch.nn.Module, checkpoint: str, device: str):\n",
    "    model.load_state_dict(torch.load(checkpoint, map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CCC 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _WeightedAverageLinearRegressor(torch.nn.Linear):\n",
    "\n",
    "    def __init__(self, in_features: int, device: t.Optional[str] = None, dtype: t.Optional[str] = None):\n",
    "        super().__init__(in_features=in_features, out_features=1, bias=False, device=device, dtype=dtype)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return torch_f.linear(x, torch_f.softmax(self.weight, dim=1), self.bias)\n",
    "\n",
    "\n",
    "class _CCC2017Model(Model):\n",
    "    \"\"\"\n",
    "    ccc-2017-multilabel-harder-cls-loss_0p5-v2-valfreq_dynamic_v1\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, checkpoint: str, output_logits: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(checkpoint, return_dict=False)\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            # torch.nn.LayerNorm(output_logits),\n",
    "            torch.nn.Linear(output_logits, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, num_classes))\n",
    "        self.regressor = _WeightedAverageLinearRegressor(in_features=num_classes)\n",
    "\n",
    "    def forward_scores(self, label_preds: torch.Tensor) -> torch.Tensor:\n",
    "        return self.regressor(label_preds)\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> t.Tuple[torch.Tensor, torch.Tensor]:\n",
    "        _, pooled_output = self.encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask)\n",
    "        label_preds = self.classifier(pooled_output)\n",
    "        scores = self.forward_scores(torch.sigmoid(label_preds))\n",
    "        return label_preds, scores\n",
    "\n",
    "    def predict_scores(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        return self.forward(input_ids, attention_mask)[1]\n",
    "\n",
    "\n",
    "def load_ccc2017(device: str) -> ModelConfig:\n",
    "    model = _CCC2017Model('roberta-base', 768, 6)\n",
    "    import_checkpoint(model, '/home/jovyan/jigsaw-toxic/models/ccc-2017-multilabel-harder-cls-loss_0p5-v2-valfreq_dynamic_v1.pt', device=device)\n",
    "    return ModelConfig(\n",
    "        name='ccc-2017-multilabel-harder-cls-loss_0p5-v2-valfreq_dynamic_v1',\n",
    "        model=model,\n",
    "        tokenizer=AutoTokenizer.from_pretrained('roberta-base'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UBTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _UBTCModel(Model):\n",
    "    \"\"\"\n",
    "    ubtc-multireg-w50-cos_warmup-opt-2ep-ut_roberta-seed_42\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, checkpoint: str, output_logits: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(checkpoint, return_dict=False)\n",
    "        self.feature_regressor = torch.nn.Sequential(\n",
    "            # torch.nn.LayerNorm(output_logits),\n",
    "            torch.nn.Linear(output_logits, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, num_classes),\n",
    "            torch.nn.Sigmoid())\n",
    "        self.score_regressor = torch.nn.Sequential(\n",
    "            # torch.nn.LayerNorm(output_logits),\n",
    "            torch.nn.Linear(num_classes, 1),\n",
    "            torch.nn.Sigmoid())\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> t.Tuple[torch.Tensor, torch.Tensor]:\n",
    "        _, pooled_output = self.encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask)\n",
    "        features = self.feature_regressor(pooled_output)\n",
    "        return features, self.score_regressor(features)\n",
    "\n",
    "    def predict_scores(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        return self.forward(input_ids, attention_mask)[1]\n",
    "\n",
    "\n",
    "def load_ubtc(device: str) -> ModelConfig:\n",
    "    model = _UBTCModel('unitary/unbiased-toxic-roberta', 768, 7)\n",
    "    import_checkpoint(model, '/home/jovyan/jigsaw-toxic/models/ubtc-multireg-w50-cos_warmup-opt-2ep-ut_roberta-seed_42.pt', device=device)\n",
    "    return ModelConfig(\n",
    "        name='ubtc-multireg-w50-cos_warmup-opt-2ep-ut_roberta-seed_42',\n",
    "        model=model,\n",
    "        tokenizer=AutoTokenizer.from_pretrained('unitary/unbiased-toxic-roberta'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ruddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _RudditModel(Model):\n",
    "\n",
    "    def __init__(self, checkpoint: str, output_logits: int, dropout: float):\n",
    "        super(Model, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(checkpoint, return_dict=False)\n",
    "        self.regressor = torch.nn.Sequential(\n",
    "            # torch.nn.LayerNorm(output_logits),\n",
    "            torch.nn.Linear(output_logits, 1),\n",
    "            torch.nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        _, pooled_output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask)\n",
    "        return self.regressor(pooled_output)\n",
    "\n",
    "    def predict_scores(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        return self.forward(input_ids, attention_mask)\n",
    "\n",
    "\n",
    "def load_ruddit(device: str) -> ModelConfig:\n",
    "    model = _RudditModel('roberta-base', 768, 0.6)\n",
    "    import_checkpoint(model, '/home/jovyan/jigsaw-toxic/models/ruddit-v3-mse-2ep-pure_reg.pt', device=device)\n",
    "    return ModelConfig(\n",
    "        name='ruddit-v3-mse-2ep-pure_reg',\n",
    "        model=model,\n",
    "        tokenizer=AutoTokenizer.from_pretrained('roberta-base'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Offenseval 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _OffensevalModel(Model):\n",
    "\n",
    "    def __init__(self, checkpoint: str, output_logits: int, dropout: float):\n",
    "        super(Model, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(checkpoint, return_dict=False)\n",
    "        self.regressor = torch.nn.Sequential(\n",
    "            # torch.nn.LayerNorm(output_logits),\n",
    "            torch.nn.Linear(output_logits, 1),\n",
    "            torch.nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        _, pooled_output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask)\n",
    "        return self.regressor(pooled_output)\n",
    "\n",
    "    def predict_scores(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        return self.forward(input_ids, attention_mask)\n",
    "\n",
    "\n",
    "def load_offenseval(device: str) -> ModelConfig:\n",
    "    model = _OffensevalModel('roberta-base', 768, 0.6)\n",
    "    import_checkpoint(model, '/home/jovyan/jigsaw-toxic/models/offenseval-2020-v2-pure_reg-mse-1_ep-64_valcycles-lr_2e5-backbone_utr.pt', device=device)\n",
    "    return ModelConfig(\n",
    "        name='offenseval-2020-v2-pure_reg-mse-1_ep-64_valcycles-lr_2e5-backbone_utr',\n",
    "        model=model,\n",
    "        tokenizer=AutoTokenizer.from_pretrained('unitary/unbiased-toxic-roberta'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_predict_iteration(\n",
    "        data_loader: DataLoader,\n",
    "        model: Model,\n",
    "        model_name: str,\n",
    "        device: str) -> torch.Tensor:\n",
    "    model.eval()\n",
    "    score_list = []\n",
    "    with torch.no_grad():\n",
    "        it = tqdm(data_loader, desc=model_name)\n",
    "        for _, tokenized_text, slice_list in it:\n",
    "            score_tensor = model.predict_scores(\n",
    "                tokenized_text['input_ids'].to(device),\n",
    "                tokenized_text['attention_mask'].to(device),)\n",
    "            score_tensor = torch.cat([torch.max(score_tensor[s], dim=0, keepdim=True)[0] for s in slice_list], dim=0)\n",
    "            score_list.extend(score_tensor.cpu().flatten().tolist())\n",
    "    return torch.tensor(score_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_by_model(\n",
    "        valid_df: pd.DataFrame,\n",
    "        batch_size: int,\n",
    "        model_getter: t.Callable[[str], ModelConfig],\n",
    "        max_len: int,\n",
    "        num_workers: int,\n",
    "        device: str) -> torch.Tensor:\n",
    "    model_config = model_getter(device)\n",
    "    model = model_config.model.to(device)\n",
    "    dataset = PredictDataset(\n",
    "        df=valid_df,\n",
    "        tokenizer=model_config.tokenizer,\n",
    "        max_len=max_len)\n",
    "    data_loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=predict_collate_fn,  # type: ignore\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=device.startswith('cuda'))\n",
    "    return do_predict_iteration(data_loader=data_loader, model=model, model_name=model_config.name, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_to_score_df = t.cast(pd.DataFrame, pd.read_csv('/home/jovyan/jigsaw-toxic/data/jigsaw-toxic-severity-rating/comments_to_score.csv'))\n",
    "comments_to_score_df = t.cast(pd.DataFrame, comments_to_score_df.sample(frac=1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccc2017_score_tensor = predict_by_model(\n",
    "    valid_df=comments_to_score_df,\n",
    "    batch_size=8,\n",
    "    model_getter=load_ccc2017,\n",
    "    num_workers=8,\n",
    "    max_len=256,\n",
    "    device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ccc2017_score_tensor.tolist(), bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ubtc_score_tensor = predict_by_model(\n",
    "    valid_df=comments_to_score_df,\n",
    "    batch_size=8,\n",
    "    model_getter=load_ubtc,\n",
    "    num_workers=8,\n",
    "    max_len=256,\n",
    "    device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ubtc_score_tensor.tolist(), bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruddit_score_tensor = predict_by_model(\n",
    "    valid_df=comments_to_score_df,\n",
    "    batch_size=8,\n",
    "    model_getter=load_ruddit,\n",
    "    num_workers=8,\n",
    "    max_len=256,\n",
    "    device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ruddit_score_tensor.tolist(), bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# offenseval_score_tensor = predict_by_model(\n",
    "#     valid_df=comments_to_score_df,\n",
    "#     batch_size=8,\n",
    "#     model_getter=load_offenseval,\n",
    "#     num_workers=8,\n",
    "#     max_len=256,\n",
    "#     device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccc2017_score_arr, ubtc_score_arr, ruddit_score_arr = ccc2017_score_tensor.numpy(), ubtc_score_tensor.numpy(), ruddit_score_tensor.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_k_folds(df: pd.DataFrame, k: int) -> pd.DataFrame:\n",
    "    df = t.cast(pd.DataFrame, df.copy())\n",
    "    df['fold'] = 0\n",
    "    n_per_fold = len(df) // k\n",
    "    for i in range(k):\n",
    "        df.loc[df.index.isin(range(i * n_per_fold, (i + 1) * n_per_fold)), 'fold'] = i\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_to_score_5fold_df = split_to_k_folds(comments_to_score_df, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(comments_to_score_5fold_df['fold'], bins=5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mine_pairs_folded(df: pd.DataFrame, score_arr_list: t.List[t.Tuple[str, np.ndarray]], max_pairs_per_sample: int) -> pd.DataFrame:\n",
    "    df = t.cast(pd.DataFrame, df.copy())\n",
    "    score_col_list = []\n",
    "    for score_col, score_arr in score_arr_list:\n",
    "        df[score_col] = score_arr\n",
    "        score_col_list.append(score_col)\n",
    "    pair_row_list: t.List[t.Dict[str, t.Union[str, float]]] = []\n",
    "    it = tqdm(df.iterrows(), total=len(df))\n",
    "    for i, row in it:\n",
    "        less_text = str(row['text'])\n",
    "        fold = int(row['fold'])\n",
    "        more_mask = functools.reduce(operator.iand, [df[score_col] > row[score_col] for score_col in score_col_list], df.index != i)\n",
    "        more_candidate_mask = (df['fold'] == fold) & more_mask\n",
    "        more_candidate_df = df[more_candidate_mask]\n",
    "        if len(more_candidate_df) == 0:\n",
    "            continue\n",
    "        \n",
    "        for _, more_row in more_candidate_df.sample(n=min(len(more_candidate_df), max_pairs_per_sample)).iterrows():\n",
    "            more_text = str(more_row['text'])\n",
    "            score_diff_list = [more_row[score_col] - row[score_col] for score_col in score_col_list]\n",
    "            score_diff_mean = statistics.mean(score_diff_list)\n",
    "            pair_row_list.append({\n",
    "                'less_toxic': less_text,\n",
    "                'more_toxic': more_text,\n",
    "                'fold': fold,\n",
    "                'score_diff_mean': score_diff_mean,\n",
    "            })\n",
    "        it.set_description(f'Pairs generated: {len(pair_row_list)}.')\n",
    "    return pd.DataFrame(pair_row_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_df = mine_pairs_folded(\n",
    "    comments_to_score_5fold_df,\n",
    "    [\n",
    "        ('ccc2017', ccc2017_score_arr),\n",
    "        ('ubtc', ubtc_score_arr),\n",
    "        ('ruddit', ruddit_score_arr),\n",
    "    ],\n",
    "    max_pairs_per_sample=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(pair_df['fold'], bins=5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(pair_df['score_diff_mean'], bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_df.to_csv('/home/jovyan/jigsaw-toxic/data/datasets/jigsaw-2021-kfold/pseudo_labeled_5fold.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
