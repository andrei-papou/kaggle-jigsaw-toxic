{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T12:22:15.63678Z","iopub.status.busy":"2021-12-12T12:22:15.636453Z","iopub.status.idle":"2021-12-12T12:22:24.224455Z","shell.execute_reply":"2021-12-12T12:22:24.223461Z","shell.execute_reply.started":"2021-12-12T12:22:15.636751Z"},"trusted":true},"outputs":[],"source":["from __future__ import annotations\n","\n","import os\n","import re\n","import typing as t\n","from pathlib import Path\n","\n","import pandas as pd\n","import torch\n","import torch.nn.functional as torch_f\n","import typing_extensions as t_ext\n","import wandb\n","from bs4 import BeautifulSoup\n","from torch.optim import Optimizer, AdamW\n","from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n","from torch.utils.data import Dataset, DataLoader\n","from tqdm.notebook import tqdm\n","from transformers.models.auto.modeling_auto import AutoModel\n","from transformers.models.auto.tokenization_auto import AutoTokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T12:22:25.020653Z","iopub.status.busy":"2021-12-12T12:22:25.0203Z","iopub.status.idle":"2021-12-12T12:22:25.037511Z","shell.execute_reply":"2021-12-12T12:22:25.036417Z","shell.execute_reply.started":"2021-12-12T12:22:25.020605Z"},"trusted":true},"outputs":[],"source":["class TextCleaner:\n","    _RE_WEBSITE_LINK = re.compile(r'https?://\\S+|www\\.\\S+')\n","    _RE_EMOJI = re.compile('['\n","        u'\\U0001F600-\\U0001F64F'  # emoticons\n","        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n","        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n","        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n","        u'\\U00002702-\\U000027B0'\n","        u'\\U000024C2-\\U0001F251'\n","        ']+', flags=re.UNICODE)\n","    _RE_SPECIAL_CHARACTERS = re.compile(r'[^a-zA-Z\\d]')\n","    _RE_EXTRA_SPACES = re.compile(r' +')\n","\n","    def __init__(self):\n","        pass\n","\n","    def clean(self, text: str) -> str:\n","        \"\"\"\n","        Cleans text into a basic form for NLP. Operations include the following:-\n","        1. Remove special charecters like &, #, etc\n","        2. Removes extra spaces\n","        3. Removes embedded URL links\n","        4. Removes HTML tags\n","        5. Removes emojis\n","        \n","        text - Text piece to be cleaned.\n","        \"\"\"\n","        text = self._RE_WEBSITE_LINK.sub(r'', text)\n","        \n","        soup = BeautifulSoup(text, 'lxml')  # Removes HTML tags\n","        only_text = soup.get_text()\n","        text = only_text\n","\n","        text = self._RE_EMOJI.sub(r'', text)\n","        \n","        text = self._RE_SPECIAL_CHARACTERS.sub(\" \", text)  # Remove special Charecters\n","        text = self._RE_EXTRA_SPACES.sub(' ', text)  # Remove Extra Spaces\n","        text = text.strip()  # Remove spaces at the beginning and at the end of string\n","\n","        return text"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T12:22:25.040587Z","iopub.status.busy":"2021-12-12T12:22:25.039433Z","iopub.status.idle":"2021-12-12T12:22:25.060961Z","shell.execute_reply":"2021-12-12T12:22:25.059824Z","shell.execute_reply.started":"2021-12-12T12:22:25.040496Z"},"trusted":true},"outputs":[],"source":["class _TokenizedText(t_ext.TypedDict):\n","    input_ids: torch.Tensor\n","    attention_mask: torch.Tensor\n","    token_type_ids: torch.Tensor\n","\n","\n","def _preprocess_tokenizer_output(output: t.Dict[str, t.Any]) -> _TokenizedText:\n","    return {\n","        'input_ids': torch.tensor(output['input_ids']),\n","        'attention_mask': torch.tensor(output['attention_mask']),\n","        'token_type_ids': torch.tensor(output['token_type_ids']),\n","    }\n","\n","\n","class TrainDataset(Dataset):\n","\n","    def __init__(self, df: pd.DataFrame, tokenizer: AutoTokenizer, max_len: int):\n","        super().__init__()\n","        self._df = df\n","        self._tokenizer = tokenizer\n","        self._max_len = max_len\n","        self._cleaner = TextCleaner()\n","\n","    def __len__(self) -> int:\n","        return len(self._df)\n","\n","    def __getitem__(self, idx: int) -> t.Tuple[_TokenizedText, torch.Tensor]:\n","        record = self._df.iloc[idx]\n","        tokenized_text = _preprocess_tokenizer_output(self._tokenizer(\n","            self._cleaner.clean(str(record['text'])),\n","            add_special_tokens=True,\n","            truncation=True,\n","            padding='max_length',\n","            max_length=self._max_len,\n","            return_attention_mask=True,\n","            return_token_type_ids=True))  # type: ignore\n","        return tokenized_text, torch.tensor(float(t.cast(t.SupportsFloat, record['average'])))\n","\n","\n","class ValidDataset(Dataset):\n","\n","    def __init__(self, df: pd.DataFrame, tokenizer: AutoTokenizer, max_len: int) -> None:\n","        super().__init__()\n","        self._df = df\n","        self._tokenizer = tokenizer\n","        self._max_len = max_len\n","        self._cleaner = TextCleaner()\n","\n","    def __len__(self) -> int:\n","        return len(self._df)\n","\n","    def __getitem__(self, idx: int) -> t.Tuple[_TokenizedText, _TokenizedText]:\n","        record = self._df.iloc[idx]\n","        tokenized_text_more = _preprocess_tokenizer_output(self._tokenizer(\n","            self._cleaner.clean(str(record['more_toxic'])),\n","            add_special_tokens=True,\n","            truncation=True,\n","            padding='max_length',\n","            max_length=self._max_len,\n","            return_attention_mask=True,\n","            return_token_type_ids=True))  # type: ignore\n","        tokenized_text_less = _preprocess_tokenizer_output(self._tokenizer(\n","            self._cleaner.clean(str(record['less_toxic'])),\n","            add_special_tokens=True,\n","            truncation=True,\n","            padding='max_length',\n","            max_length=self._max_len,\n","            return_attention_mask=True,\n","            return_token_type_ids=True))  # type: ignore\n","        return tokenized_text_more, tokenized_text_less\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T12:22:25.064219Z","iopub.status.busy":"2021-12-12T12:22:25.063745Z","iopub.status.idle":"2021-12-12T12:22:25.077289Z","shell.execute_reply":"2021-12-12T12:22:25.076246Z","shell.execute_reply.started":"2021-12-12T12:22:25.064148Z"},"trusted":true},"outputs":[],"source":["class Model(torch.nn.Module):\n","\n","    def __init__(self, checkpoint: str, output_logits: int, dropout: float):\n","        super(Model, self).__init__()\n","        self.bert = AutoModel.from_pretrained(checkpoint, return_dict=False)\n","        self.layer_norm = torch.nn.LayerNorm(output_logits)\n","        self.dropout = torch.nn.Dropout(dropout)\n","        self.dense = torch.nn.Sequential(\n","            torch.nn.Linear(output_logits, 128),\n","            torch.nn.SiLU(),\n","            torch.nn.Dropout(dropout),\n","            torch.nn.Linear(128, 1)\n","        )\n","\n","    def forward(self, input_ids: torch.Tensor, token_type_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n","        _, pooled_output = self.bert(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n","        pooled_output = self.layer_norm(pooled_output)\n","        pooled_output = self.dropout(pooled_output)\n","        preds = self.dense(pooled_output)\n","        return preds"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T12:22:25.079789Z","iopub.status.busy":"2021-12-12T12:22:25.079414Z","iopub.status.idle":"2021-12-12T12:22:25.113742Z","shell.execute_reply":"2021-12-12T12:22:25.112565Z","shell.execute_reply.started":"2021-12-12T12:22:25.079744Z"},"trusted":true},"outputs":[],"source":["class Metric:\n","\n","    def compute(self) -> float:\n","        raise NotImplementedError()\n","\n","    def reset(self):\n","        raise NotImplementedError()\n","\n","    def compute_and_reset(self) -> float:\n","        value = self.compute()\n","        self.reset()\n","        return value\n","\n","\n","class Accuracy(Metric):\n","\n","    def __init__(self):\n","        self._num_correct = 0\n","        self._num_total = 0\n","\n","    def update(self, is_correct_tensor: torch.Tensor):\n","        num_correct = (is_correct_tensor == 1).int().sum().item()\n","        num_total = num_correct + (is_correct_tensor != 1).int().sum().item()\n","        self._num_correct += num_correct\n","        self._num_total += num_total\n","\n","    def compute(self) -> float:\n","        assert self._num_total > 0\n","        return self._num_correct / self._num_total\n","\n","    def reset(self):\n","        self._num_correct = 0\n","        self._num_total = 0\n","\n","\n","class Loss(Metric):\n","\n","    def __init__(self):\n","        self._value_list: t.List[float] = []\n","\n","    def update(self, loss_tensor: torch.Tensor):\n","        self._value_list.extend(loss_tensor.flatten().tolist())\n","\n","    def compute(self) -> float:\n","        assert len(self._value_list) > 0\n","        return sum(self._value_list) / len(self._value_list)\n","\n","    def reset(self):\n","        self._value_list.clear()\n","\n","\n","def do_train_iteration(\n","        train_data_loader: DataLoader,\n","        valid_data_loader: DataLoader,\n","        epoch: int,\n","        model: Model,\n","        device: str,\n","        optimizer: Optimizer,\n","        scheduler: t.Any,\n","        validate_every_n_steps: int,\n","        to_checkpoint: str,\n","        margin_list: t.Optional[t.List[float]] = None,\n","        decision_margin: float = 0.0):\n","    loss_metric = Loss()\n","    model.train()\n","    data_iter = tqdm(train_data_loader)\n","    best_accuracy = 0.0\n","    for i, (tokenized_text, y) in enumerate(data_iter, start=1):\n","        step = epoch * len(train_data_loader) + i\n","        input_ids, attention_mask, token_type_ids, y = (\n","            tokenized_text['input_ids'].to(device),\n","            tokenized_text['attention_mask'].to(device),\n","            tokenized_text['token_type_ids'].to(device),\n","            y.to(device))\n","        y_hat = model(input_ids, token_type_ids, attention_mask)\n","        loss = torch_f.mse_loss(y_hat.squeeze(1), y)\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","        optimizer.zero_grad()\n","        loss_metric.update(loss.cpu())\n","        if step % validate_every_n_steps == 0:\n","            loss_val = loss_metric.compute_and_reset()\n","            print(f'Step: {step}. Loss: {loss_val}')\n","            accuracy = do_valid_iteration(\n","                data_loader=valid_data_loader,\n","                model=model,\n","                step=step,\n","                device=device,\n","                margin_list=margin_list,\n","                decision_margin=decision_margin)\n","            wandb.log({'step': step, 'train_loss': loss_val, 'valid_accuracy': accuracy})\n","            if accuracy > best_accuracy:\n","                print(f'Best accuracy improved from {best_accuracy} to {accuracy}. Saving the model.')\n","                torch.save(model.state_dict(), to_checkpoint)\n","                best_accuracy = accuracy\n","            model.train()\n","\n","\n","def do_valid_iteration(\n","        data_loader: DataLoader,\n","        model: Model,\n","        step: int,\n","        device: str,\n","        margin_list: t.Optional[t.List[float]] = None,\n","        decision_margin: float = 0.0) -> float:\n","    if margin_list is None:\n","        margin_list = [0.0]\n","    assert decision_margin in margin_list\n","    accuracy_dict = {margin: Accuracy() for margin in margin_list}\n","    model.eval()\n","    with torch.no_grad():\n","        for tokenized_text_more, tokenized_text_less in tqdm(data_loader, desc=f'Step: {step}. Validation'):\n","            score_more = model(\n","                tokenized_text_more['input_ids'].to(device),\n","                tokenized_text_more['token_type_ids'].to(device),\n","                tokenized_text_more['attention_mask'].to(device),)\n","            score_less = model(\n","                tokenized_text_less['input_ids'].to(device),\n","                tokenized_text_less['token_type_ids'].to(device),\n","                tokenized_text_less['attention_mask'].to(device),)\n","            for margin, accuracy_metric in accuracy_dict.items():\n","                accuracy_metric.update(((score_more - score_less) >= margin).cpu())\n","    accuracy_str = ', '.join(f'{margin} = {metric.compute()}' for margin, metric in accuracy_dict.items())\n","    print(f'Step: {step}. Valid accuracy: {accuracy_str}')\n","    return accuracy_dict[decision_margin].compute()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T12:22:36.266075Z","iopub.status.busy":"2021-12-12T12:22:36.265737Z","iopub.status.idle":"2021-12-12T12:22:36.272195Z","shell.execute_reply":"2021-12-12T12:22:36.270547Z","shell.execute_reply.started":"2021-12-12T12:22:36.266044Z"},"trusted":true},"outputs":[],"source":["# Parameters\n","\n","IS_KAGGLE = True\n","\n","ROOT_DIR_PATH = Path('/kaggle') if IS_KAGGLE else Path('/home/jovyan/jigsaw-toxic')\n","DATA_DIR_PATH = ROOT_DIR_PATH / ('input' if IS_KAGGLE else 'data')\n","JIGSAW_TOXIC_20211212_DIR_PATH = DATA_DIR_PATH / 'jigsaw-toxic-20211212'\n","\n","TRAIN_CSV_PATH = JIGSAW_TOXIC_20211212_DIR_PATH / 'offenseval_2020_train.csv'\n","VALID_CSV_PATH = JIGSAW_TOXIC_20211212_DIR_PATH / 'valid.csv'\n","\n","MODELS_DIR_PATH = ROOT_DIR_PATH / ('working/models' if IS_KAGGLE else 'models')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T12:22:36.525212Z","iopub.status.busy":"2021-12-12T12:22:36.524438Z","iopub.status.idle":"2021-12-12T12:22:37.276096Z","shell.execute_reply":"2021-12-12T12:22:37.274909Z","shell.execute_reply.started":"2021-12-12T12:22:36.525179Z"},"trusted":true},"outputs":[],"source":["!mkdir -p $MODELS_DIR_PATH"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T12:22:37.281031Z","iopub.status.busy":"2021-12-12T12:22:37.280728Z","iopub.status.idle":"2021-12-12T12:22:39.686016Z","shell.execute_reply":"2021-12-12T12:22:39.685083Z","shell.execute_reply.started":"2021-12-12T12:22:37.280999Z"},"trusted":true},"outputs":[],"source":["# Read dataframes\n","\n","train_df = pd.read_csv(TRAIN_CSV_PATH)\n","valid_df = pd.read_csv(VALID_CSV_PATH)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T12:22:39.688012Z","iopub.status.busy":"2021-12-12T12:22:39.687533Z","iopub.status.idle":"2021-12-12T12:22:39.717392Z","shell.execute_reply":"2021-12-12T12:22:39.716411Z","shell.execute_reply.started":"2021-12-12T12:22:39.687969Z"},"trusted":true},"outputs":[],"source":["train_df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T12:22:39.720301Z","iopub.status.busy":"2021-12-12T12:22:39.719357Z","iopub.status.idle":"2021-12-12T12:22:39.738393Z","shell.execute_reply":"2021-12-12T12:22:39.737509Z","shell.execute_reply.started":"2021-12-12T12:22:39.720249Z"},"trusted":true},"outputs":[],"source":["valid_df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T12:22:42.408865Z","iopub.status.busy":"2021-12-12T12:22:42.408538Z","iopub.status.idle":"2021-12-12T12:22:42.42374Z","shell.execute_reply":"2021-12-12T12:22:42.422332Z","shell.execute_reply.started":"2021-12-12T12:22:42.408836Z"},"trusted":true},"outputs":[],"source":["def main(\n","        train_df: pd.DataFrame,\n","        valid_df: pd.DataFrame,\n","        from_checkpoint: str,\n","        to_checkpoint: str,\n","        num_epochs: int,\n","        batch_size: int,\n","        max_len: int,\n","        num_workers: int,\n","        device: str,\n","        output_logits: int,\n","        dropout: float,\n","        lr: float,\n","        min_lr: float,\n","        t_0: int,\n","        t_mult: int,\n","        margin_list: t.List[float],\n","        decision_margin: float,\n","        validate_every_n_steps: int,\n","        is_kaggle: bool = True):\n","    if is_kaggle:\n","        from kaggle_secrets import UserSecretsClient\n","        user_secrets = UserSecretsClient()\n","        api_key = user_secrets.get_secret('wandb-api-token')\n","    else:\n","        api_key = os.environ['WANDB_API_KEY']\n","    wandb.login(key=api_key)\n","    wandb.init(project='kaggle-jigsaw-toxic', entity='andrei-papou')\n","    \n","    wandb.config = {\n","        'from_checkpoint': from_checkpoint,\n","        'lr': lr,\n","        'batch_size': batch_size,\n","        'dropout': dropout,\n","        'output_logits': output_logits,\n","        'max_len': max_len,\n","        'num_epochs': num_epochs,\n","        'validate_every_n_steps': validate_every_n_steps,\n","        'optimizer': 'adam_w',\n","        'scheduler': 'cosine_annealing_warm_restarts',\n","        't_0': t_0,\n","    }\n","    \n","    tokenizer = AutoTokenizer.from_pretrained(from_checkpoint)\n","    train_dataset = TrainDataset(train_df, tokenizer=tokenizer, max_len=max_len)\n","    valid_dataset = ValidDataset(valid_df, tokenizer=tokenizer, max_len=max_len)\n","    train_data_loader = DataLoader(\n","        dataset=train_dataset,\n","        batch_size=batch_size,\n","        num_workers=num_workers,\n","        shuffle=True,\n","        pin_memory=True)\n","    valid_data_loader = DataLoader(\n","        dataset=valid_dataset,\n","        batch_size=batch_size * 2,\n","        num_workers=num_workers,\n","        shuffle=False,\n","        pin_memory=True)\n","    model = Model(checkpoint=from_checkpoint, output_logits=output_logits, dropout=dropout)\n","    model = model.to(device)\n","    wandb.watch(model)\n","    optimizer = AdamW(model.parameters(), lr=lr)\n","    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=t_0, T_mult=t_mult, eta_min=min_lr)\n","\n","    for epoch in range(num_epochs):\n","        do_train_iteration(\n","            train_data_loader=train_data_loader,\n","            valid_data_loader=valid_data_loader,\n","            model=model,\n","            epoch=epoch,\n","            device=device,\n","            optimizer=optimizer,\n","            scheduler=scheduler,\n","            to_checkpoint=to_checkpoint,\n","            margin_list=margin_list,\n","            decision_margin=decision_margin,\n","            validate_every_n_steps=validate_every_n_steps)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T12:22:45.495797Z","iopub.status.busy":"2021-12-12T12:22:45.495372Z","iopub.status.idle":"2021-12-12T12:25:33.064309Z","shell.execute_reply":"2021-12-12T12:25:33.062705Z","shell.execute_reply.started":"2021-12-12T12:22:45.495766Z"},"trusted":true},"outputs":[],"source":["main(\n","    train_df=train_df,\n","    valid_df=valid_df,\n","    from_checkpoint='roberta-base',\n","    to_checkpoint=str(MODELS_DIR_PATH / 'offenseval-2020-regression.pt'),\n","    num_epochs=2,\n","    batch_size=48,\n","    max_len=256,\n","    num_workers=2,\n","    device='cuda',\n","    output_logits=768,\n","    dropout=0.2,\n","    lr=1e-4,\n","    min_lr=1e-6,\n","    t_0=400,\n","    t_mult=2,\n","    margin_list=[0.0, 0.05, 0.1],\n","    decision_margin=0.0,\n","    validate_every_n_steps=400,\n","    is_kaggle=IS_KAGGLE)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":4}
