{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import typing as t\n",
    "from pathlib import Path\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import typing_extensions as t_ext\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from scipy import sparse\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_KAGGLE = False\n",
    "RUN_VALIDATION = True\n",
    "RUN_SUBMISSION = True\n",
    "\n",
    "TRAIN_DATASET_ROOT = Path('/kaggle/input/jt-combined') if IS_KAGGLE \\\n",
    "    else Path('/home/jovyan/jigsaw-toxic/data/datasets/combined')\n",
    "TEST_DATASET_ROOT = Path('/kaggle/input/jigsaw-toxic-severity-rating') if IS_KAGGLE \\\n",
    "    else Path('/home/jovyan/jigsaw-toxic/data/jigsaw-toxic-severity-rating')\n",
    "TRAIN_CSV_PATH = TRAIN_DATASET_ROOT / 'train_comment_classification_challenge_2017.csv' \n",
    "VALID_CSV_PATH = TRAIN_DATASET_ROOT / 'valid.csv'\n",
    "INFER_CSV_PATH = TEST_DATASET_ROOT / 'comments_to_score.csv'\n",
    "SUBMISSION_CSV_PATH = Path('/kaggle/working/submission.csv') if IS_KAGGLE else Path('submission.csv')\n",
    "\n",
    "WEIGHT_DICT = {\n",
    "    'obscene': 0.16,\n",
    "    'toxic': 0.32,\n",
    "    'threat': 1.5,\n",
    "    'insult': 0.64,\n",
    "    'severe_toxic': 1.5,\n",
    "    'identity_hate': 1.5\n",
    "}\n",
    "POSITIVE_FRAC = 1.0\n",
    "NEGATIVE_FRAC_SCALER = 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = t.cast(t.Callable[[str], t.List[str]], nltk.tokenize.word_tokenize)\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_y(df: pd.DataFrame, weight_dict: t.Dict[str, float]) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df['y'] = sum([df[tag] * weight for tag, weight in weight_dict.items()])\n",
    "    df['y'] /= df['y'].max()  # type: ignore\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample(df: pd.DataFrame, positive_frac: float, negative_frac_scaler: float):\n",
    "    return pd.concat([\n",
    "        df[df.y > 0].sample(frac=positive_frac) , \n",
    "        df[df.y == 0].sample(n=int(len(df[df.y > 0]) * positive_frac * negative_frac_scaler))\n",
    "    ], axis=0).sample(frac=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_url(text: str) -> str:\n",
    "    \"\"\" Replaces url address with \"url\" \"\"\"\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', 'url', text)\n",
    "    text = re.sub(r'#([^\\s]+)', r'\\1', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_abbrev(text: str) -> str:\n",
    "    text = re.sub(r\"what's\", \"what is \", text)    \n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_unicode(text: str) -> str:\n",
    "    \"\"\" Removes unicode strings like \"\\u002c\" and \"x96\" \"\"\"\n",
    "    text = re.sub(r'(\\\\u[0-9A-Fa-f]+)', r' ', text)       \n",
    "    text = re.sub(r'[^\\x00-\\x7f]', r' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_repeat_pattern(text: str) -> str:\n",
    "    text=re.sub(r'([a-zA-Z])\\1{2,}\\b', r'\\1\\1', text)\n",
    "    text=re.sub(r'([a-zA-Z])\\1\\1{2,}\\B', r'\\1\\1\\1', text)\n",
    "    text=re.sub(r'[ ]{2,}', ' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_at_user(text: str) -> str:\n",
    "    \"\"\" Replaces \"@user\" with \"atUser\" \"\"\"\n",
    "    text = re.sub('@[^\\s]+','atUser',text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_multi_toxic_words(text: str) -> str:\n",
    "    text = re.sub(r'(fuckfuck)','fuck fuck ',text)\n",
    "    text = re.sub(r'(f+)( *)([u|*]+)( *)([c|*]+)( *)(k)+','fuck',text)\n",
    "    text = re.sub(r'(haha)','ha ha ',text)\n",
    "    text = re.sub(r'(s+ *h+ *i+ *t+)','shit',text)\n",
    "    text = re.sub(r'([a|@][$|s][s|$])','ass',text)\n",
    "    text = re.sub(r'(\\bfuk\\b)','fuck',text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_numbers(text: str) -> str:\n",
    "    \"\"\" Removes integers \"\"\"\n",
    "    text = re.sub(r\"(^|\\W)\\d+\", \" \", text)    \n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_multi_punc(text: str) -> str:\n",
    "    text = re.sub(r'([!?\\'])\\1+', r' \\1\\1 ', text)\n",
    "    text = re.sub(r'([!?\\'])', r' \\1 ', text)\n",
    "    text = re.sub(r'([*_:])\\1+', r'\\1', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "class Lemmatizer(t_ext.Protocol):\n",
    "\n",
    "    def lemmatize(self, word: str, pos: str = \"n\") -> str:\n",
    "        ...\n",
    "\n",
    "\n",
    "class ReplaceTokenCleaner:\n",
    "\n",
    "    def __init__(self, token_set: t.Set[str], replace_with: str):\n",
    "        self._token_set = token_set\n",
    "        self._replace_with = replace_with\n",
    "\n",
    "    def __call__(self, text: str) -> str:\n",
    "        for token in self._token_set:\n",
    "            text = text.replace(token, self._replace_with)\n",
    "        return text\n",
    "\n",
    "\n",
    "class RemoveStopWordsCleaner:\n",
    "\n",
    "    def __init__(self, tokenizer: t.Callable[[str], t.List[str]], stop_words: t.Optional[t.List[str]] = None):\n",
    "        self._tokenizer = tokenizer\n",
    "        self._stop_words = stop_words if stop_words is not None else stopwords.words('english')\n",
    "\n",
    "    def __call__(self, text: str) -> str:\n",
    "        return ' '.join([token for token in self._tokenizer(text) if token not in self._stop_words])\n",
    "\n",
    "\n",
    "class LemmatizeCleaner:\n",
    "\n",
    "    def __init__(self, tokenizer: t.Callable[[str], t.List[str]], lemmatizer: Lemmatizer):\n",
    "        self._tokenizer = tokenizer\n",
    "        self._lemmatizer = lemmatizer\n",
    "\n",
    "    def __call__(self, text: str) -> str:\n",
    "        return ' '.join([self._lemmatizer.lemmatize(token) for token in self._tokenizer(text)])\n",
    "\n",
    "\n",
    "class TextCleanerList:\n",
    "\n",
    "    def __init__(self, cleaner_list: t.List[t.Callable[[str], str]]):\n",
    "        self._cleaner_list = cleaner_list\n",
    "\n",
    "    def __call__(self, text: str) -> str:\n",
    "        for cleaner in self._cleaner_list:\n",
    "            text = cleaner(text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _HandCraftedFeature:\n",
    "    name: str\n",
    "\n",
    "    def __call__(self, text: str) -> float:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class _TokenBasedHandCraftedFeature(_HandCraftedFeature):\n",
    "\n",
    "    def __init__(self, tokenizer: t.Callable[[str], t.List[str]]):\n",
    "        self._tokenizer = tokenizer\n",
    "\n",
    "    def _tokenize(self, text: str) -> t.List[str]:\n",
    "        return self._tokenizer(text)\n",
    "\n",
    "\n",
    "class CharLenFeature(_HandCraftedFeature):\n",
    "    name: str = 'char_len'\n",
    "\n",
    "    def __call__(self, text: str) -> float:\n",
    "        return len(text)\n",
    "\n",
    "\n",
    "class TokenLenFeature(_TokenBasedHandCraftedFeature):\n",
    "    name: str = 'token_len'\n",
    "\n",
    "    def __call__(self, text: str) -> float:\n",
    "        return len(self._tokenize(text))\n",
    "\n",
    "\n",
    "class AvgTokenLenFeature(_TokenBasedHandCraftedFeature):\n",
    "    name: str = 'avg_token_len'\n",
    "    \n",
    "    def __call__(self, text: str) -> float:\n",
    "        return np.mean([len(token) for token in self._tokenize(text)])\n",
    "\n",
    "\n",
    "class NumStopWordsFeature(_TokenBasedHandCraftedFeature):\n",
    "    name: str = 'num_stop_words'\n",
    "\n",
    "    def __init__(self, tokenizer: t.Callable[[str], t.List[str]], stop_words: t.Set[str]):\n",
    "        super().__init__(tokenizer)\n",
    "        self._stop_words = stop_words\n",
    "\n",
    "    def __call__(self, text: str) -> float:\n",
    "        return len([token for token in self._tokenize(text) if token.lower() in self._stop_words])\n",
    "\n",
    "\n",
    "class NumWebsiteLinksFeature(_HandCraftedFeature):\n",
    "    name: str = 'num_website_links'\n",
    "    _RE_WEBSITE_LINK = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "\n",
    "    def __call__(self, text: str) -> float:\n",
    "        return len(self._RE_WEBSITE_LINK.findall(text))\n",
    "\n",
    "\n",
    "class NumEmojiFeature(_HandCraftedFeature):\n",
    "    name: str = 'num_emoji'\n",
    "    _RE_EMOJI = re.compile('['\n",
    "        u'\\U0001F600-\\U0001F64F'  # emoticons\n",
    "        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n",
    "        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n",
    "        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n",
    "        u'\\U00002702-\\U000027B0'\n",
    "        u'\\U000024C2-\\U0001F251'\n",
    "        ']+', flags=re.UNICODE)\n",
    "\n",
    "    def __call__(self, text: str) -> float:\n",
    "        return len(self._RE_EMOJI.findall(text))\n",
    "\n",
    "\n",
    "class NumSpecialCharsFeature(_HandCraftedFeature):\n",
    "    name: str = 'num_special_chars'\n",
    "    _RE_SPECIAL_CHARS = re.compile(r'[^a-zA-Z\\d]')\n",
    "\n",
    "    def __call__(self, text: str) -> float:\n",
    "        return len(self._RE_SPECIAL_CHARS.findall(text))\n",
    "\n",
    "\n",
    "class NumExtraSpacesFeature(_HandCraftedFeature):\n",
    "    name: str = 'num_extra_spaces'\n",
    "    _RE_EXTRA_SPACES = re.compile(r' +')\n",
    "\n",
    "    def __call__(self, text: str) -> float:\n",
    "        return len(self._RE_EXTRA_SPACES.findall(text))\n",
    "\n",
    "\n",
    "class UpperCaseCharRatioFeature(_HandCraftedFeature):\n",
    "    name: str = 'upper_case_char_ratio'\n",
    "\n",
    "    def __call__(self, text: str) -> float:\n",
    "        return len([c for c in str(text) if c.isupper()]) / len(text)\n",
    "\n",
    "\n",
    "class LowerCaseCharRatioFeature(_HandCraftedFeature):\n",
    "    name: str = 'lower_case_char_ratio'\n",
    "\n",
    "    def __call__(self, text: str) -> float:\n",
    "        return len([c for c in str(text) if c.islower()]) / len(text)\n",
    "\n",
    "\n",
    "class UpperCaseTokenRatioFeature(_TokenBasedHandCraftedFeature):\n",
    "    name: str = 'upper_case_token_ratio'\n",
    "\n",
    "    def __call__(self, text: str) -> float:\n",
    "        token_list = self._tokenize(text)\n",
    "        return len([token for token in token_list if token.isupper()]) / len(token_list)\n",
    "\n",
    "\n",
    "class LowerCaseTokenRatioFeature(_TokenBasedHandCraftedFeature):\n",
    "    name: str = 'lower_case_token_ratio'\n",
    "\n",
    "    def __call__(self, text: str) -> float:\n",
    "        token_list = self._tokenize(text)\n",
    "        return len([token for token in token_list if token.islower()]) / len(token_list)\n",
    "\n",
    "\n",
    "class HandCraftedFeatureList:\n",
    "\n",
    "    def __init__(self, feature_list: t.List[_HandCraftedFeature]):\n",
    "        self._feature_list = feature_list\n",
    "\n",
    "    def __call__(self, text: str) -> np.ndarray:\n",
    "        return np.array([feature(text) for feature in self._feature_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_features_to_sparse(array_list: t.List[t.Union[sparse.spmatrix, np.ndarray]]) -> sparse.spmatrix:\n",
    "    assert len(array_list) > 0\n",
    "    sparse_array_list = []\n",
    "    for array in array_list:\n",
    "        if isinstance(array, np.ndarray):\n",
    "            array = sparse.csr_matrix(array)\n",
    "        sparse_array_list.append(array)\n",
    "    return sparse.hstack(sparse_array_list) if len(sparse_array_list) > 1 else sparse_array_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_F = t.TypeVar('_F')\n",
    "\n",
    "class _FeatureGenerator(t.Generic[_F]):\n",
    "\n",
    "    def __call__(self, text_list: t.List[str]) -> _F:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class TfidfFeatureGenerator(_FeatureGenerator[sparse.spmatrix]):\n",
    "\n",
    "    def __init__(self, vectorizer: TfidfVectorizer) -> None:\n",
    "        check_is_fitted(vectorizer)\n",
    "        self._vectorizer = vectorizer\n",
    "\n",
    "    def __call__(self, text_list: t.List[str]) -> sparse.spmatrix:\n",
    "        return self._vectorizer.transform(text_list)\n",
    "\n",
    "\n",
    "class HandCraftedFeatureGenerator(_FeatureGenerator[np.ndarray]):\n",
    "\n",
    "    def __init__(self, feature_list: HandCraftedFeatureList, show_progress: bool = False):\n",
    "        self._feature_list = feature_list\n",
    "        self._show_progress = show_progress\n",
    "\n",
    "    def __call__(self, text_list: t.List[str]) -> np.ndarray:\n",
    "        return np.stack([self._feature_list(text) for text in (tqdm(text_list) if self._show_progress else text_list)], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cleaner = TextCleanerList([\n",
    "    lambda text: text.lower(),\n",
    "    clean_url,\n",
    "    clean_unicode,\n",
    "    clean_numbers,\n",
    "    clean_abbrev,\n",
    "    clean_multi_toxic_words,\n",
    "    clean_multi_punc,\n",
    "    clean_repeat_pattern,\n",
    "    ReplaceTokenCleaner(\n",
    "        token_set=set('\"%&\\'()+,-./:;<=>@[\\\\]^_`{|}~'),\n",
    "        replace_with=' '),\n",
    "    LemmatizeCleaner(\n",
    "        tokenizer=tokenizer,\n",
    "        lemmatizer=WordNetLemmatizer()),\n",
    "    # RemoveStopWordsCleaner(_tokenizer),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(TRAIN_CSV_PATH)\n",
    "train_df = build_y(train_df, weight_dict=WEIGHT_DICT)\n",
    "train_df = subsample(train_df, positive_frac=POSITIVE_FRAC, negative_frac_scaler=NEGATIVE_FRAC_SCALER)\n",
    "\n",
    "train_text_list = [str(row['comment_text']) for _, row in tqdm(train_df.iterrows(), total=len(train_df))]\n",
    "train_cleaned_text_list = [text_cleaner(text) for text in tqdm(train_text_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_feature_generator = TfidfFeatureGenerator(\n",
    "    vectorizer=TfidfVectorizer(min_df=3, max_df=0.5, analyzer='char_wb', ngram_range=(3, 5)).fit(train_cleaned_text_list))\n",
    "hand_crafted_feature_generator = HandCraftedFeatureGenerator(\n",
    "    feature_list=HandCraftedFeatureList([\n",
    "        CharLenFeature(),\n",
    "        TokenLenFeature(tokenizer=tokenizer),\n",
    "        AvgTokenLenFeature(tokenizer=tokenizer),\n",
    "        NumStopWordsFeature(tokenizer=tokenizer, stop_words=stop_words),\n",
    "        NumWebsiteLinksFeature(),\n",
    "        NumEmojiFeature(),\n",
    "        NumSpecialCharsFeature(),\n",
    "        NumExtraSpacesFeature(),\n",
    "        UpperCaseCharRatioFeature(),\n",
    "        LowerCaseCharRatioFeature(),\n",
    "        UpperCaseTokenRatioFeature(tokenizer=tokenizer),\n",
    "        LowerCaseTokenRatioFeature(tokenizer=tokenizer),\n",
    "    ]),\n",
    "    show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = tfidf_feature_generator(train_cleaned_text_list)\n",
    "x = join_features_to_sparse([\n",
    "    tfidf_feature_generator(train_cleaned_text_list),\n",
    "    hand_crafted_feature_generator(train_text_list),\n",
    "])\n",
    "y = train_df['y'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Ridge(alpha=0.5)\n",
    "model.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_VALIDATION:\n",
    "    valid_df = pd.read_csv(VALID_CSV_PATH)\n",
    "\n",
    "    valid_more_text_list = [str(row['more_toxic']) for _, row in tqdm(valid_df.iterrows(), total=len(valid_df))]\n",
    "    valid_less_text_list = [str(row['less_toxic']) for _, row in tqdm(valid_df.iterrows(), total=len(valid_df))]\n",
    "    valid_cleaned_more_text_list = [text_cleaner(text) for text in valid_more_text_list]\n",
    "    valid_cleaned_less_text_list = [text_cleaner(text) for text in valid_less_text_list]\n",
    "\n",
    "    less_toxic_score_array = model.predict(join_features_to_sparse([\n",
    "        tfidf_feature_generator(valid_cleaned_less_text_list),\n",
    "        hand_crafted_feature_generator(valid_less_text_list),\n",
    "    ]))\n",
    "    more_toxic_score_array = model.predict(join_features_to_sparse([\n",
    "        tfidf_feature_generator(valid_cleaned_more_text_list),\n",
    "        hand_crafted_feature_generator(valid_more_text_list),\n",
    "    ]))\n",
    "    print(f'Validation accuracy: {np.mean(less_toxic_score_array < more_toxic_score_array)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_SUBMISSION:\n",
    "    infer_df = pd.read_csv(INFER_CSV_PATH)\n",
    "\n",
    "    comment_id_list, infer_text_list, infer_cleaned_text_list = [], [], []\n",
    "    for _, row in tqdm(infer_df.iterrows(), total=len(infer_df)):\n",
    "        comment_id, text = str(row['comment_id']), str(row['text'])\n",
    "        comment_id_list.append(comment_id)\n",
    "        infer_text_list.append(text)\n",
    "        infer_cleaned_text_list.append(text_cleaner(text))\n",
    "\n",
    "    score_array = model.predict(join_features_to_sparse([\n",
    "        tfidf_feature_generator(infer_cleaned_text_list),\n",
    "        hand_crafted_feature_generator(infer_text_list),\n",
    "    ]))\n",
    "\n",
    "    pd.DataFrame([\n",
    "        {'comment_id': comment_id, 'score': score}\n",
    "        for comment_id, score in zip(comment_id_list, score_array.tolist())\n",
    "    ]).to_csv(SUBMISSION_CSV_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 11 $SUBMISSION_CSV_PATH"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
