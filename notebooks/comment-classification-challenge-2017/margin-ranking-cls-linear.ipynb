{"cells":[{"cell_type":"markdown","metadata":{},"source":["### Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T21:30:42.290752Z","iopub.status.busy":"2021-12-13T21:30:42.290324Z","iopub.status.idle":"2021-12-13T21:30:44.813935Z","shell.execute_reply":"2021-12-13T21:30:44.813044Z","shell.execute_reply.started":"2021-12-13T21:30:42.290611Z"},"papermill":{"duration":7.63065,"end_time":"2021-12-13T15:32:53.613901","exception":false,"start_time":"2021-12-13T15:32:45.983251","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["from __future__ import annotations\n","\n","import numpy as np\n","import random\n","import re\n","import statistics\n","import typing as t\n","from pathlib import Path\n","\n","import nltk\n","import pandas as pd\n","import torch\n","import torch.nn.functional as torch_f\n","import typing_extensions as t_ext\n","import wandb\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from torch.optim import Optimizer, AdamW\n","from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n","from torch.utils.data import Dataset, DataLoader, Sampler\n","from torch.utils.tensorboard.writer import SummaryWriter\n","from tqdm.notebook import tqdm\n","from transformers.models.auto.modeling_auto import AutoModel\n","from transformers.models.auto.tokenization_auto import AutoTokenizer\n","from wandb.wandb_run import Run as WAndBRun"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tqdm.pandas()\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('stopwords')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def seed_everything(seed: int):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","\n","seed_everything(42)"]},{"cell_type":"markdown","metadata":{},"source":["### Datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T21:30:44.832059Z","iopub.status.busy":"2021-12-13T21:30:44.83171Z","iopub.status.idle":"2021-12-13T21:30:44.845976Z","shell.execute_reply":"2021-12-13T21:30:44.845075Z","shell.execute_reply.started":"2021-12-13T21:30:44.83202Z"},"papermill":{"duration":0.032712,"end_time":"2021-12-13T15:32:53.693077","exception":false,"start_time":"2021-12-13T15:32:53.660365","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["class _TokenizedText(t_ext.TypedDict):\n","    input_ids: torch.Tensor\n","    attention_mask: torch.Tensor\n","\n","\n","def _preprocess_tokenizer_output(output: t.Dict[str, t.Any]) -> _TokenizedText:\n","    return {\n","        'input_ids': torch.tensor(output['input_ids']),\n","        'attention_mask': torch.tensor(output['attention_mask']),\n","    }\n","\n","\n","class Tokenizer:\n","\n","    def tokenize(self, x: str) -> t.List[str]:\n","        return x.split(' ')\n","\n","    def invert_tokenize(self, x: t.List[str]) -> str:\n","        return ' '.join(x)\n","\n","\n","class RandomlyReduceTokenLenTo:\n","\n","    def __init__(self, token_len: int, tokenizer: t.Optional[Tokenizer] = None):\n","        self._token_len = token_len\n","        self._tokenizer = tokenizer if tokenizer is not None else Tokenizer()\n","\n","    def __call__(self, text: str) -> str:\n","        token_list = self._tokenizer.tokenize(text)\n","        if len(token_list) <= self._token_len:\n","            return text\n","        idx_set = set(random.choices(list(range(len(token_list))), k=self._token_len))\n","        return self._tokenizer.invert_tokenize([token for idx, token in enumerate(token_list) if idx in idx_set])\n","\n","\n","class RandomSubsetPerEpochSampler(Sampler[int]):\n","\n","    @staticmethod\n","    def _build_index(data_source: t.Sized) -> t.List[int]:\n","        index = list(range(len(data_source)))\n","        random.shuffle(index)\n","        return index\n","\n","    def __init__(self, data_source: t.Sized, samples_per_epoch: int):\n","        super().__init__(data_source)\n","        self._data_source = data_source\n","        self._samples_per_epoch = samples_per_epoch\n","        self._index: t.List[int] = self._build_index(data_source)\n","        self._real_epoch = 0\n","    \n","    def _sample_one(self) -> int:\n","        if not self._index:\n","            self._index = self._build_index(self._data_source)\n","            self._real_epoch += 1\n","        return self._index.pop()\n","\n","    def __iter__(self) -> t.Iterator[int]:\n","        return iter([self._sample_one() for _ in range(self._samples_per_epoch)])\n","\n","    def __len__(self) -> int:\n","        return self._samples_per_epoch\n","\n","    @property\n","    def real_epoch(self) -> int:\n","        return self._real_epoch\n","\n","    @property\n","    def frac_left(self) -> float:\n","        return len(self._index) / len(self._data_source)\n","\n","    @property\n","    def frac_consumed(self) -> float:\n","        return 1.0 - self.frac_left\n","\n","\n","class TrainDataset(Dataset):\n","\n","    def __init__(\n","            self,\n","            df: pd.DataFrame,\n","            tokenizer: AutoTokenizer,\n","            vectorizer: TfidfVectorizer,\n","            max_len: int,\n","            augmentation_list: t.Optional[t.List[t.Callable[[str], str]]] = None):\n","        super().__init__()\n","        self._df = df\n","        self._tokenizer = tokenizer\n","        self._vectorizer = vectorizer\n","        self._max_len = max_len\n","        self._augmentation_list = augmentation_list if augmentation_list is not None else []\n","\n","    def __len__(self) -> int:\n","        return len(self._df)\n","\n","    def _apply_augmentations(self, text: str) -> str:\n","        for augmentation in self._augmentation_list:\n","            text = augmentation(text)\n","        return text\n","\n","    def __getitem__(self, idx: int) -> t.Tuple[_TokenizedText, _TokenizedText, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n","        record = self._df.iloc[idx]\n","        more_comment_text = self._apply_augmentations(str(record['more_toxic']))\n","        less_comment_text = self._apply_augmentations(str(record['less_toxic']))\n","        tokenized_text_more = _preprocess_tokenizer_output(self._tokenizer(\n","            more_comment_text,\n","            add_special_tokens=True,\n","            truncation=True,\n","            padding='max_length',\n","            max_length=self._max_len,\n","            return_attention_mask=True))  # type: ignore\n","        tokenized_text_less = _preprocess_tokenizer_output(self._tokenizer(\n","            less_comment_text,\n","            add_special_tokens=True,\n","            truncation=True,\n","            padding='max_length',\n","            max_length=self._max_len,\n","            return_attention_mask=True))  # type: ignore\n","        more_labels = torch.tensor([int(l) for l in record['more_labels'].split(' ')], dtype=torch.float32)\n","        less_labels = torch.tensor([int(l) for l in record['less_labels'].split(' ')], dtype=torch.float32)\n","\n","        more_vector_tensor = torch.tensor(self._vectorizer.transform([record['more_toxic_cleaned']]).toarray()[0], dtype=torch.float32)\n","        less_vector_tensor = torch.tensor(self._vectorizer.transform([record['less_toxic_cleaned']]).toarray()[0], dtype=torch.float32)\n","\n","        # print(f'more_vector_tensor.shape = {more_vector_tensor.shape}')\n","        # print(f'less_vector_tensor.shape = {less_vector_tensor.shape}')\n","\n","        return tokenized_text_more, tokenized_text_less, more_labels, less_labels, more_vector_tensor, less_vector_tensor\n","\n","\n","def _split_str_to_chunk_list(s: str, chunk_size: int) -> t.List[str]:\n","    chunk_list = []\n","    chunk = []\n","    for token in s.split(' '):\n","        chunk.append(token)\n","        if len(chunk) >= chunk_size:\n","            chunk_list.append(' '.join(chunk))\n","            chunk.clear()\n","    if chunk:\n","        chunk_list.append(' '.join(chunk))\n","    return chunk_list\n","\n","\n","def valid_collate_fn(\n","        sample_list: t.List[t.Tuple[int, _TokenizedText, _TokenizedText, torch.Tensor, torch.Tensor]]\n","        ) -> t.Tuple[t.List[int], _TokenizedText, _TokenizedText, t.List[slice], t.List[slice], torch.Tensor, torch.Tensor]:\n","    curr_pos_more, curr_pos_less = 0, 0\n","\n","    idx_list: t.List[int] = []\n","    more_input_ids_list, less_input_ids_list = [], []\n","    more_attention_mask_list, less_attention_mask_list = [], []\n","    more_slice_list: t.List[slice] = []\n","    less_slice_list: t.List[slice] = []\n","    more_vector_tensor_list: t.List[torch.Tensor] = []\n","    less_vector_tensor_list: t.List[torch.Tensor] = []\n","    \n","    for sample in sample_list:\n","        idx_list.append(sample[0])\n","        more_input_ids, more_attention_mask = sample[1]['input_ids'], sample[1]['attention_mask']\n","        less_input_ids, less_attention_mask = sample[2]['input_ids'], sample[2]['attention_mask']\n","        more_input_ids_list.append(more_input_ids)\n","        less_input_ids_list.append(less_input_ids)\n","        more_attention_mask_list.append(more_attention_mask)\n","        less_attention_mask_list.append(less_attention_mask)\n","        more_slice_list.append(slice(curr_pos_more, curr_pos_more + more_input_ids.shape[0]))\n","        curr_pos_more += more_input_ids.shape[0]\n","        less_slice_list.append(slice(curr_pos_less, curr_pos_less + less_input_ids.shape[0]))\n","        curr_pos_less += less_input_ids.shape[0]\n","        more_vector_tensor_list.append(sample[3])\n","        less_vector_tensor_list.append(sample[4])\n","\n","    more_tokenized_collated: _TokenizedText = {\n","        'input_ids': torch.cat(more_input_ids_list, dim=0),\n","        'attention_mask': torch.cat(more_attention_mask_list, dim=0),\n","    }\n","    less_tokenized_collated: _TokenizedText = {\n","        'input_ids': torch.cat(less_input_ids_list, dim=0),\n","        'attention_mask': torch.cat(less_attention_mask_list, dim=0),\n","    }\n","\n","    return (\n","        idx_list,\n","        more_tokenized_collated,\n","        less_tokenized_collated,\n","        more_slice_list,\n","        less_slice_list,\n","        torch.cat(more_vector_tensor_list, dim=0),\n","        torch.cat(less_vector_tensor_list, dim=0),\n","    )\n","\n","\n","class ValidDataset(Dataset):\n","\n","    def __init__(\n","            self,\n","            df: pd.DataFrame,\n","            tokenizer: AutoTokenizer,\n","            vectorizer: TfidfVectorizer,\n","            max_len: int) -> None:\n","        super().__init__()\n","        self._df = df\n","        self._tokenizer = tokenizer\n","        self._vectorizer = vectorizer\n","        self._max_len = max_len\n","        self._error: np.ndarray = np.zeros(len(df))\n","\n","    def __len__(self) -> int:\n","        return len(self._df)\n","\n","    def track_error(self, idx_list: t.List[int], error: np.ndarray):\n","        self._error[idx_list] = error\n","\n","    def get_df_with_error(self) -> pd.DataFrame:\n","        df = self._df.copy()\n","        df['error'] = self._error\n","        return df\n","\n","    def __getitem__(self, idx: int) -> t.Tuple[int, _TokenizedText, _TokenizedText, torch.Tensor, torch.Tensor]:\n","        record = self._df.iloc[idx]\n","        text_more = str(record['more_toxic'])\n","        text_less = str(record['less_toxic'])\n","\n","        more_input_ids_list, less_input_ids_list = [], []\n","        more_attention_mask_list, less_attention_mask_list = [], []\n","        more_vector_tensor_list, less_vector_tensor_list = [], []\n","        for chunk in _split_str_to_chunk_list(text_more, chunk_size=self._max_len):\n","            tokenized_chunk = _preprocess_tokenizer_output(self._tokenizer(\n","                chunk,\n","                add_special_tokens=True,\n","                truncation=True,\n","                padding='max_length',\n","                max_length=self._max_len,\n","                return_attention_mask=True))  # type: ignore\n","            more_input_ids_list.append(tokenized_chunk['input_ids'])\n","            more_attention_mask_list.append(tokenized_chunk['attention_mask'])\n","            more_vector_tensor_list.append(torch.tensor(\n","                self._vectorizer.transform([record['more_toxic_cleaned']]).toarray()[0],\n","                dtype=torch.float32))\n","        for chunk in _split_str_to_chunk_list(text_less, chunk_size=self._max_len):\n","            tokenized_chunk = _preprocess_tokenizer_output(self._tokenizer(\n","                chunk,\n","                add_special_tokens=True,\n","                truncation=True,\n","                padding='max_length',\n","                max_length=self._max_len,\n","                return_attention_mask=True))  # type: ignore\n","            less_input_ids_list.append(tokenized_chunk['input_ids'])\n","            less_attention_mask_list.append(tokenized_chunk['attention_mask'])\n","            less_vector_tensor_list.append(torch.tensor(\n","                self._vectorizer.transform([record['less_toxic_cleaned']]).toarray()[0],\n","                dtype=torch.float32))\n","\n","        tokenized_more: _TokenizedText = {\n","            'input_ids': torch.stack(more_input_ids_list, dim=0),\n","            'attention_mask': torch.stack(more_attention_mask_list, dim=0),\n","        }\n","        tokenized_less: _TokenizedText = {\n","            'input_ids': torch.stack(less_input_ids_list, dim=0),\n","            'attention_mask': torch.stack(less_attention_mask_list, dim=0),\n","        }\n","\n","        return idx, tokenized_more, tokenized_less, torch.stack(more_vector_tensor_list, dim=0), torch.stack(less_vector_tensor_list, dim=0)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Text cleaners"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tokenizer = t.cast(t.Callable[[str], t.List[str]], nltk.tokenize.word_tokenize)\n","stop_words = stopwords.words('english')\n","\n","\n","class _Cleaner:\n","\n","    def __call__(self, text: str) -> str:\n","        return text\n","\n","\n","class URLCleaner(_Cleaner):\n","    _RE_URL_1 = re.compile('((www\\.[^\\s]+)|(https?://[^\\s]+))')\n","    _RE_URL_2 = re.compile(r'#([^\\s]+)')\n","\n","    def __call__(self, text: str) -> str:\n","        text = self._RE_URL_1.sub('url', text)\n","        text = self._RE_URL_2.sub(r'\\1', text)\n","        return text\n","\n","\n","class AbbrevCleaner(_Cleaner):\n","    \n","    def __call__(self, text: str) -> str:\n","        return text\\\n","            .replace('what\\'s', 'what is ')\\\n","            .replace('\\'ve', ' have ')\\\n","            .replace('can\\'t', 'cannot ')\\\n","            .replace('n\\'t', ' not ')\\\n","            .replace('i\\'m', 'i am ')\\\n","            .replace('\\'re', ' are ')\\\n","            .replace('\\'d', ' would ')\\\n","            .replace('\\'ll', ' will ')\\\n","            .replace('\\'scuse', ' excuse ')\\\n","            .replace('\\'s', ' ')\n","\n","\n","class UnicodeCleaner(_Cleaner):\n","    _RE_UNICODE_1 = re.compile(r'(\\\\u[0-9A-Fa-f]+)')\n","    _RE_UNICODE_2 = re.compile(r'[^\\x00-\\x7f]')\n","\n","    def __call__(self, text: str) -> str:\n","        text = self._RE_UNICODE_1.sub(r' ', text)\n","        text = self._RE_UNICODE_2.sub(r' ', text)\n","        return text\n","\n","\n","class RepeatPatternCleaner(_Cleaner):\n","    _RE_REPEAT_1 = re.compile(r'([a-zA-Z])\\1{2,}\\b')\n","    _RE_REPEAT_2 = re.compile(r'([a-zA-Z])\\1\\1{2,}\\B')\n","    _RE_REPEAT_3 = re.compile(r'[ ]{2,}')\n","\n","    def __call__(self, text: str) -> str:\n","        text = self._RE_REPEAT_1.sub(r'\\1\\1', text)\n","        text = self._RE_REPEAT_2.sub(r'\\1\\1\\1', text)\n","        text = self._RE_REPEAT_3.sub(' ', text)\n","        return text    \n","\n","\n","class AtUserCleaner(_Cleaner):\n","    _RE_AT_USER = re.compile('@[^\\s]+')\n","\n","    def __call__(self, text: str) -> str:\n","        text = self._RE_AT_USER.sub('atUser', text)\n","        return text\n","\n","\n","class MultiToxicWordsCleaner(_Cleaner):\n","    _RE_FUCK_1 = re.compile(r'(fuckfuck)')\n","    _RE_FUCK_2 = re.compile(r'(f+)( *)([u|*]+)( *)([c|*]+)( *)(k)+')\n","    _RE_HAHA = re.compile(r'(haha)')\n","    _RE_SHIT = re.compile(r'(s+ *h+ *i+ *t+)')\n","    _RE_ASS = re.compile(r'([a|@][$|s][s|$])')\n","    _RE_FUK = re.compile(r'(\\bfuk\\b)')\n","\n","    def __call__(self, text: str) -> str:\n","        text = self._RE_FUCK_1.sub('fuck fuck ', text)\n","        text = self._RE_FUCK_2.sub('fuck', text)\n","        text = self._RE_HAHA.sub('ha ha ', text)\n","        text = self._RE_SHIT.sub('shit', text)\n","        text = self._RE_ASS.sub('ass', text)\n","        text = self._RE_FUK.sub('fuck', text)\n","        return text\n","\n","\n","class NumbersCleaner(_Cleaner):\n","    _RE_NUMBERS = re.compile(r\"(^|\\W)\\d+\")\n","\n","    def __call__(self, text: str) -> str:\n","        return self._RE_NUMBERS.sub(' ', text)\n","\n","\n","class MultiPuncCleaner(_Cleaner):\n","    _RE_1 = re.compile(r'([!?\\'])\\1+')\n","    _RE_2 = re.compile(r'([!?\\'])')\n","    _RE_3 = re.compile(r'([*_:])\\1+')\n","\n","    def __call__(self, text: str) -> str:\n","        text = self._RE_1.sub(r' \\1\\1 ', text)\n","        text = self._RE_2.sub(r' \\1 ', text)\n","        text = self._RE_3.sub(r'\\1', text)\n","        return text\n","\n","\n","class Lemmatizer(t_ext.Protocol):\n","\n","    def lemmatize(self, word: str, pos: str = \"n\") -> str:\n","        ...\n","\n","\n","class ReplaceTokenCleaner:\n","\n","    def __init__(self, token_set: t.Set[str], replace_with: str):\n","        self._token_set = token_set\n","        self._replace_with = replace_with\n","\n","    def __call__(self, text: str) -> str:\n","        for token in self._token_set:\n","            text = text.replace(token, self._replace_with)\n","        return text\n","\n","\n","class RemoveStopWordsCleaner:\n","\n","    def __init__(self, tokenizer: t.Callable[[str], t.List[str]], stop_words: t.Optional[t.List[str]] = None):\n","        self._tokenizer = tokenizer\n","        self._stop_words = stop_words if stop_words is not None else stopwords.words('english')\n","\n","    def __call__(self, text: str) -> str:\n","        return ' '.join([token for token in self._tokenizer(text) if token not in self._stop_words])\n","\n","\n","class LemmatizeCleaner:\n","\n","    def __init__(self, tokenizer: t.Callable[[str], t.List[str]], lemmatizer: Lemmatizer):\n","        self._tokenizer = tokenizer\n","        self._lemmatizer = lemmatizer\n","\n","    def __call__(self, text: str) -> str:\n","        return ' '.join([self._lemmatizer.lemmatize(token) for token in self._tokenizer(text)])\n","\n","\n","class TextCleanerList:\n","\n","    def __init__(self, cleaner_list: t.List[t.Callable[[str], str]]):\n","        self._cleaner_list = cleaner_list\n","\n","    def __call__(self, text: str) -> str:\n","        for cleaner in self._cleaner_list:\n","            text = cleaner(text)\n","        return text\n","\n","\n","text_cleaner = TextCleanerList([\n","    lambda text: text.lower(),\n","    URLCleaner(),\n","    UnicodeCleaner(),\n","    NumbersCleaner(),\n","    AbbrevCleaner(),\n","    MultiToxicWordsCleaner(),\n","    MultiPuncCleaner(),\n","    RepeatPatternCleaner(),\n","    ReplaceTokenCleaner(\n","        token_set=set('\"%&\\'()+,-./:;<=>@[\\\\]^_`{|}~'),\n","        replace_with=' '),\n","    LemmatizeCleaner(\n","        tokenizer=tokenizer,\n","        lemmatizer=WordNetLemmatizer()),\n","    RemoveStopWordsCleaner(tokenizer),\n","])"]},{"cell_type":"markdown","metadata":{},"source":["### Model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T21:30:44.848904Z","iopub.status.busy":"2021-12-13T21:30:44.848506Z","iopub.status.idle":"2021-12-13T21:30:44.859529Z","shell.execute_reply":"2021-12-13T21:30:44.858898Z","shell.execute_reply.started":"2021-12-13T21:30:44.848854Z"},"papermill":{"duration":0.020746,"end_time":"2021-12-13T15:32:53.724218","exception":false,"start_time":"2021-12-13T15:32:53.703472","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["class _WeightedAverageLinearRegressor(torch.nn.Linear):\n","\n","    def __init__(self, in_features: int, device: t.Optional[str] = None, dtype: t.Optional[str] = None):\n","        super().__init__(in_features=in_features, out_features=1, bias=False, device=device, dtype=dtype)\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        return torch_f.linear(x, torch_f.softmax(self.weight, dim=1), self.bias)\n","\n","\n","class _TransformerClassifier(torch.nn.Module):\n","\n","    def __init__(self, checkpoint: str, output_logits: int, num_classes: int):\n","        super().__init__()\n","        self.encoder = AutoModel.from_pretrained(checkpoint, return_dict=False)\n","        self.classifier = torch.nn.Sequential(\n","            # torch.nn.LayerNorm(output_logits),\n","            torch.nn.Linear(output_logits, 128),\n","            torch.nn.ReLU(),\n","            torch.nn.Linear(128, num_classes))\n","\n","    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n","        _, pooled_output = self.encoder(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask)\n","        label_preds = self.classifier(pooled_output)\n","        return label_preds\n","\n","\n","class _LinearClassifier(torch.nn.Module):\n","\n","    def __init__(self, num_features: int, num_classes: int):\n","        super().__init__()\n","        self.inner = torch.nn.Linear(in_features=num_features, out_features=num_classes)\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        return self.inner(x)\n","\n","\n","class _ClassifierDispatcher(torch.nn.Module):\n","\n","    def __init__(self, num_classes: int):\n","        super().__init__()\n","        self.inner = torch.nn.Linear(in_features=num_classes * 2, out_features=1)\n","\n","    def forward(self, transformer_pred_tensor: torch.Tensor, linear_pred_tensor: torch.Tensor) -> torch.Tensor:\n","        return torch_f.sigmoid(self.inner(torch.concat([transformer_pred_tensor, linear_pred_tensor], dim=1)))\n","\n","\n","class Model(torch.nn.Module):\n","\n","    def __init__(\n","            self,\n","            transformer_checkpoint: str,\n","            transformer_output_logits: int,\n","            linear_num_features: int,\n","            num_classes: int):\n","        super().__init__()\n","        self.transformer_classifier = _TransformerClassifier(\n","            checkpoint=transformer_checkpoint,\n","            output_logits=transformer_output_logits,\n","            num_classes=num_classes)\n","        self.linear_classifier = _LinearClassifier(\n","            num_features=linear_num_features,\n","            num_classes=num_classes)\n","        self.classifier_dispatcher = _ClassifierDispatcher(num_classes=num_classes)\n","        self.regressor = _WeightedAverageLinearRegressor(in_features=num_classes)\n","\n","    def forward_scores(self, label_preds: torch.Tensor) -> torch.Tensor:\n","        return self.regressor(label_preds)\n","\n","    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor, tfidf_vector_repr: torch.Tensor) -> t.Tuple[torch.Tensor, torch.Tensor]:\n","        transformer_pred_tensor = self.transformer_classifier(input_ids, attention_mask)\n","        linear_pred_tensor = self.linear_classifier(tfidf_vector_repr)\n","        theta = self.classifier_dispatcher(transformer_pred_tensor, linear_pred_tensor)\n","        pred_tensor = transformer_pred_tensor * theta + linear_pred_tensor * (1.0 - theta)\n","        scores = self.forward_scores(torch.sigmoid(pred_tensor))\n","        return pred_tensor, scores\n"]},{"cell_type":"markdown","metadata":{},"source":["### Metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T21:30:44.861454Z","iopub.status.busy":"2021-12-13T21:30:44.860954Z","iopub.status.idle":"2021-12-13T21:30:44.878684Z","shell.execute_reply":"2021-12-13T21:30:44.877857Z","shell.execute_reply.started":"2021-12-13T21:30:44.861415Z"},"papermill":{"duration":0.02804,"end_time":"2021-12-13T15:32:53.762852","exception":false,"start_time":"2021-12-13T15:32:53.734812","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["class _Metric:\n","\n","    def compute(self) -> float:\n","        raise NotImplementedError()\n","\n","    def reset(self):\n","        raise NotImplementedError()\n","\n","    def compute_and_reset(self) -> float:\n","        value = self.compute()\n","        self.reset()\n","        return value\n","\n","\n","class Accuracy(_Metric):\n","\n","    def __init__(self):\n","        self._num_correct = 0\n","        self._num_total = 0\n","\n","    def update(self, is_correct_tensor: torch.Tensor):\n","        num_correct = (is_correct_tensor == 1).int().sum().item()\n","        num_total = num_correct + (is_correct_tensor != 1).int().sum().item()\n","        self._num_correct += num_correct\n","        self._num_total += num_total\n","\n","    def compute(self) -> float:\n","        assert self._num_total > 0\n","        return self._num_correct / self._num_total\n","\n","    def reset(self):\n","        self._num_correct = 0\n","        self._num_total = 0\n","\n","\n","class _FloatListMetric(_Metric):\n","\n","    def __init__(self):\n","        self._value_list: t.List[float] = []\n","\n","    def update(self, value_tensor: torch.Tensor):\n","        self._value_list.extend(value_tensor.flatten().tolist())\n","\n","    def reset(self):\n","        self._value_list.clear()\n","\n","\n","class FloatListMean(_FloatListMetric):\n","\n","    def compute(self) -> float:\n","        return statistics.mean(self._value_list)\n","\n","\n","class FloatListStd(_FloatListMetric):\n","\n","    def compute(self) -> float:\n","        return statistics.stdev(self._value_list)"]},{"cell_type":"markdown","metadata":{},"source":["### Loggers"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["_C = t.TypeVar('_C')\n","\n","\n","class ContextManagerList(t.Generic[_C]):\n","\n","    def __init__(self, cm_list: t.List[t.ContextManager[_C]]):\n","        self._cm_list = cm_list\n","\n","    def __enter__(self) -> t.List[_C]:\n","        return [cm.__enter__() for cm in self._cm_list]\n","\n","    def __exit__(self, *args, **kwargs):\n","        for cm in self._cm_list:\n","            cm.__exit__(*args, **kwargs)\n","\n","\n","class Logger:\n","\n","    def __enter__(self) -> Logger:\n","        return self\n","\n","    def __exit__(self, *args, **kwargs):\n","        pass\n","\n","    def log_params(self, params: t.Dict[str, t.Any]):\n","        raise NotImplementedError()\n","\n","    def log_metrics(self, step: int, metrics: t.Dict[str, float]):\n","        raise NotImplementedError()\n","\n","\n","class StdOutLogger(Logger):\n","\n","    def log_params(self, params: t.Dict[str, t.Any]):\n","        print('Using params:')\n","        for param in sorted(params.keys()):\n","            print(f'\\t{param} = {params[param]}')\n","\n","    def log_metrics(self, step: int, metrics: t.Dict[str, float]):\n","        print(f'Step {step} metrics:')\n","        for m in sorted(metrics.keys()):\n","            print(f'\\t{m} = {metrics[m]:.8f}')\n","\n","\n","class TensorBoardLogger(Logger):\n","\n","    def __init__(self, log_dir: str, metric_whitelist: t.Optional[t.Set[str]] = None) -> None:\n","        self._metric_whitelist = metric_whitelist\n","        self._writer = SummaryWriter(log_dir=log_dir)\n","\n","    def log_params(self, params: t.Dict[str, t.Any]):\n","        pass  # TODO: handle hyperparams properly.\n","\n","    def log_metrics(self, step: int, metrics: t.Dict[str, float]):\n","        for metric_key, metric_val in metrics.items():\n","            if self._metric_whitelist is None or metric_key in self._metric_whitelist:\n","                self._writer.add_scalar(tag=metric_key, scalar_value=metric_val, global_step=step)\n","\n","\n","class WAndBLogger(Logger):\n","\n","    def __init__(self, user_name: str, api_key: str, project: str, run_id: str):\n","        wandb.login(key=api_key)\n","        self._user_name = user_name\n","        self._project = project\n","        self._run_id = run_id\n","        self._run: t.Optional[WAndBRun] = None\n","    \n","    @property\n","    def run(self) -> WAndBRun:\n","        assert self._run is not None\n","        return self._run\n","\n","    def __enter__(self) -> WAndBLogger:\n","        self._run = wandb.init(project=self._project, entity=self._user_name, run_id=self._run_id)\n","        return self\n","    \n","    def log_params(self, params: t.Dict[str, t.Any]):\n","        self.run.config.update(params)\n","\n","    def log_metrics(self, step: int, metrics: t.Dict[str, float]):\n","        self.run.log(step=step, data=metrics)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Schedulers"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class ForwardScheduledFloat:\n","\n","    def __init__(self, start: float, end: float, step: float):\n","        self._end = end\n","        self._step = step\n","        self._val = start\n","\n","    @property\n","    def value(self) -> float:\n","        return self._val\n","\n","    def step(self):\n","        if self._val < self._end:\n","            self._val += self._step"]},{"cell_type":"markdown","metadata":{},"source":["### Loss"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def margin_ranking_loss(\n","        more_scores: torch.Tensor,\n","        less_scores: torch.Tensor,\n","        diff: torch.Tensor,\n","        margin: float,\n","        device: str,) -> torch.Tensor:\n","    return torch.maximum(torch.tensor(0.0, device=device), less_scores - more_scores + margin * diff).mean()"]},{"cell_type":"markdown","metadata":{},"source":["### Iteration functions"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T21:30:44.881104Z","iopub.status.busy":"2021-12-13T21:30:44.880154Z","iopub.status.idle":"2021-12-13T21:30:44.914119Z","shell.execute_reply":"2021-12-13T21:30:44.913266Z","shell.execute_reply.started":"2021-12-13T21:30:44.881064Z"},"papermill":{"duration":0.044539,"end_time":"2021-12-13T15:32:53.818306","exception":false,"start_time":"2021-12-13T15:32:53.773767","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def do_train_iteration(\n","        data_loader: DataLoader,\n","        model: Model,\n","        device: str,\n","        optimizer: Optimizer,\n","        scheduler: CosineAnnealingWarmRestarts,\n","        ranking_loss_part: float,\n","        train_margin_list: t.Optional[t.List[float]] = None,\n","        train_decision_margin: float = 0.3,\n","        accumulate_gradient_steps: int = 1,\n","        num_steps: t.Optional[int] = None) -> t.Dict[str, float]:\n","    sampler: RandomSubsetPerEpochSampler = t.cast(RandomSubsetPerEpochSampler, data_loader.sampler)\n","    loss_metric = FloatListMean()\n","    train_score_mean = FloatListMean()\n","    train_score_std = FloatListStd()\n","    if train_margin_list is None:\n","        train_margin_list = [train_decision_margin]\n","    assert train_decision_margin in train_margin_list\n","    train_accuracy_dict = {m: Accuracy() for m in train_margin_list}\n","\n","    model.train()\n","    data_iter = tqdm(data_loader, desc='Training', total=num_steps if num_steps is not None else len(data_loader))\n","    for step, (tokenized_text_more, tokenized_text_less, labels_more, labels_less, vector_tensor_more, vector_tensor_less) in enumerate(data_iter):\n","        (\n","            input_ids_more,\n","            attention_mask_more,\n","            input_ids_less,\n","            attention_mask_less,\n","            labels_more,\n","            labels_less,\n","            vector_tensor_more,\n","            vector_tensor_less,\n","        ) = (\n","            tokenized_text_more['input_ids'].to(device),\n","            tokenized_text_more['attention_mask'].to(device),\n","            tokenized_text_less['input_ids'].to(device),\n","            tokenized_text_less['attention_mask'].to(device),\n","            labels_more.to(device),\n","            labels_less.to(device),\n","            vector_tensor_more.to(device),\n","            vector_tensor_less.to(device),\n","        )\n","        preds_more, score_more = model(input_ids_more, attention_mask_more, vector_tensor_more)\n","        preds_less, score_less = model(input_ids_less, attention_mask_less, vector_tensor_less)\n","        score_more_approx = model.forward_scores(labels_more)\n","        score_less_approx = model.forward_scores(labels_less)\n","\n","        # loss = torch_f.margin_ranking_loss(\n","        #     score_more, score_less, torch.ones(score_more.shape[0], device=device), margin=train_decision_margin)\n","        cls_more_loss = torch_f.multilabel_soft_margin_loss(preds_more, labels_more)\n","        cls_less_loss = torch_f.multilabel_soft_margin_loss(preds_less, labels_less)\n","        ranking_loss = margin_ranking_loss(score_more, score_less, margin=train_decision_margin, diff=score_more_approx - score_less_approx, device=device)\n","        loss = ranking_loss_part * ranking_loss + (1.0 - ranking_loss_part) * (cls_more_loss + cls_less_loss) / 2\n","        loss.backward()\n","\n","        if (step + 1) % accumulate_gradient_steps == 0:\n","            optimizer.step()\n","            optimizer.zero_grad()\n","            scheduler.step()\n","\n","        with torch.no_grad():\n","            score_more_cpu = score_more.cpu()\n","            train_score_mean.update(score_more_cpu)\n","            train_score_std.update(score_more_cpu)\n","            score_less_cpu = score_less.cpu()\n","            train_score_mean.update(score_less_cpu)\n","            train_score_std.update(score_less_cpu)\n","\n","            loss_metric.update(loss.cpu())\n","            for m, a in train_accuracy_dict.items():\n","                a.update(score_more - score_less > m)\n","        epoch_str = f'epoch: {sampler.real_epoch} [{sampler.frac_consumed:.4f}]'\n","        accuracy_str = ', '.join([f'acc_{m}: {train_accuracy_dict[m].compute():.4f}' for m in sorted(train_accuracy_dict.keys())])\n","        data_iter.set_description(\n","            f'{epoch_str} th: {ranking_loss_part:.2f} loss: {loss_metric.compute():.4f}, {accuracy_str} '\n","            f'score_mean: {train_score_mean.compute():.6f}, score_std: {train_score_std.compute():.6f}')\n","\n","        if num_steps is not None and step >= num_steps - 1:\n","            break\n","\n","    train_metrics_to_track = {f'train_accuracy_{m}': a.compute() for m, a in train_accuracy_dict.items()}\n","    loss_val = loss_metric.compute_and_reset()\n","    return {\n","        'train_loss': loss_val,\n","        'train_score_mean': train_score_mean.compute(),\n","        'train_score_std': train_score_std.compute(),\n","        **train_metrics_to_track,\n","    }\n","\n","\n","@torch.no_grad()\n","def do_valid_iteration(\n","        data_loader: DataLoader,\n","        model: Model,\n","        device: str,\n","        margin_list: t.Optional[t.List[float]] = None,\n","        decision_margin: float = 0.0) -> t.Tuple[float, t.Dict[str, float]]:\n","    if margin_list is None:\n","        margin_list = [decision_margin]\n","    assert decision_margin in margin_list\n","    accuracy_dict = {margin: Accuracy() for margin in margin_list}\n","    valid_score_mean = FloatListMean()\n","    valid_score_std = FloatListStd()\n","    model.eval()\n","    it = tqdm(data_loader, desc='Validation')\n","    for idx_list, tokenized_text_more, tokenized_text_less, slice_list_more, slice_list_less, vector_tensor_more, vector_tensor_less in it:\n","        _, score_more = model(\n","            tokenized_text_more['input_ids'].to(device),\n","            tokenized_text_more['attention_mask'].to(device),\n","            vector_tensor_more.to(device))\n","        _, score_less = model(\n","            tokenized_text_less['input_ids'].to(device),\n","            tokenized_text_less['attention_mask'].to(device),\n","            vector_tensor_less.to(device))\n","        score_more = torch.cat([torch.max(score_more[s], dim=0, keepdim=True)[0] for s in slice_list_more], dim=0)\n","        score_less = torch.cat([torch.max(score_less[s], dim=0, keepdim=True)[0] for s in slice_list_less], dim=0)\n","        score_more_cpu = score_more.cpu()\n","        valid_score_mean.update(score_more_cpu)\n","        valid_score_std.update(score_more_cpu)\n","        score_less_cpu = score_less.cpu()\n","        valid_score_mean.update(score_less_cpu)\n","        valid_score_std.update(score_less_cpu)\n","        for margin, accuracy_metric in accuracy_dict.items():\n","            accuracy_metric.update(((score_more - score_less) > margin).cpu())\n","        data_loader.dataset.track_error(\n","            idx_list, torch.maximum(torch.zeros_like(score_less_cpu), score_less_cpu - score_more_cpu).squeeze(1))\n","        accuracy_str = ', '.join([f'acc_{m}: {accuracy_dict[m].compute():.4f}' for m in sorted(accuracy_dict.keys())])\n","        it.set_description(f'Validation. {accuracy_str}')\n","    score_dict_to_track = {\n","        'valid_score_mean': valid_score_mean.compute(),\n","        'valid_score_std': valid_score_std.compute(),\n","    }\n","    accuracy_dict_to_track =  {f'valid_accuracy_{m}': a.compute() for m, a in accuracy_dict.items()}\n","    return accuracy_dict[decision_margin].compute(), {**score_dict_to_track, **accuracy_dict_to_track}"]},{"cell_type":"markdown","metadata":{},"source":["### Main function"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T21:30:45.729874Z","iopub.status.busy":"2021-12-13T21:30:45.729205Z","iopub.status.idle":"2021-12-13T21:30:45.744702Z","shell.execute_reply":"2021-12-13T21:30:45.74378Z","shell.execute_reply.started":"2021-12-13T21:30:45.729823Z"},"papermill":{"duration":0.028452,"end_time":"2021-12-13T15:32:56.900841","exception":false,"start_time":"2021-12-13T15:32:56.872389","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def main(\n","        train_df: pd.DataFrame,\n","        valid_df: pd.DataFrame,\n","        num_classes: int,\n","        from_checkpoint: str,\n","        to_checkpoint: str,\n","        error_artifact_dir_path: Path,\n","        logger_list: t.List[Logger],\n","        num_epochs: int,\n","        batch_size: int,\n","        max_len: int,\n","        num_workers: int,\n","        device: str,\n","        output_logits: int,\n","        lr: float,\n","        t_0: int,\n","        eta_min: float,\n","        train_margin_list: t.List[float],\n","        train_decision_margin: float,\n","        valid_margin_list: t.List[float],\n","        valid_decision_margin: float,\n","        accumulate_gradient_steps: int = 1,\n","        validate_every_n_steps: t.Optional[int] = None):    \n","    for logger in logger_list:\n","        logger.log_params({\n","            'from_checkpoint': from_checkpoint,\n","            'lr': lr,\n","            'batch_size': batch_size,\n","            'output_logits': output_logits,\n","            'max_len': max_len,\n","            'num_epochs': num_epochs,\n","            'optimizer': 'adam_w',\n","            'scheduler': 'cosine_annealing_warm_restarts',\n","            'accumulate_gradient_steps': accumulate_gradient_steps,\n","        })\n","    \n","    tokenizer = AutoTokenizer.from_pretrained(from_checkpoint)\n","    vectorizer = TfidfVectorizer(min_df=3, max_df=0.5, max_features=65536, analyzer='char_wb', ngram_range=(3, 5))\\\n","        .fit(train_df['more_toxic_cleaned'].tolist() + train_df['less_toxic_cleaned'].tolist())\n","    train_dataset = TrainDataset(\n","        train_df,\n","        tokenizer=tokenizer,\n","        vectorizer=vectorizer,\n","        max_len=max_len,\n","        augmentation_list=[\n","            # RandomlyReduceTokenLenTo(token_len=max_len),\n","        ])\n","    valid_dataset = ValidDataset(\n","        valid_df,\n","        tokenizer=tokenizer,\n","        vectorizer=vectorizer,\n","        max_len=max_len)\n","    train_data_loader = DataLoader(\n","        dataset=train_dataset,\n","        sampler=RandomSubsetPerEpochSampler(\n","            data_source=train_dataset,\n","            samples_per_epoch=validate_every_n_steps if validate_every_n_steps is not None else len(train_dataset)),\n","        batch_size=batch_size,\n","        num_workers=num_workers,\n","        pin_memory=True)\n","    valid_data_loader = DataLoader(\n","        dataset=valid_dataset,\n","        batch_size=batch_size * 2,\n","        num_workers=num_workers,\n","        shuffle=False,\n","        pin_memory=True,\n","        collate_fn=valid_collate_fn)  # type: ignore\n","    model = Model(\n","        transformer_checkpoint=from_checkpoint,\n","        transformer_output_logits=output_logits,\n","        num_classes=num_classes,\n","        linear_num_features=len(vectorizer.vocabulary_)).to(device)\n","    optimizer = AdamW(model.parameters(), lr=lr)\n","    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=t_0, eta_min=eta_min)\n","\n","    best_accuracy = 0.0\n","    for epoch in range(num_epochs):\n","        train_metrics_to_track = do_train_iteration(\n","            data_loader=train_data_loader,\n","            model=model,\n","            device=device,\n","            optimizer=optimizer,\n","            scheduler=scheduler,\n","            train_margin_list=train_margin_list,\n","            train_decision_margin=train_decision_margin,\n","            accumulate_gradient_steps=accumulate_gradient_steps,\n","            ranking_loss_part=0.5)\n","        accuracy, valid_metrics_to_track = do_valid_iteration(\n","            data_loader=valid_data_loader,\n","            model=model,\n","            device=device,\n","            margin_list=valid_margin_list,\n","            decision_margin=valid_decision_margin)\n","        for logger in logger_list:\n","            logger.log_metrics(step=epoch, metrics={\n","                **train_metrics_to_track,\n","                **valid_metrics_to_track,\n","            })\n","        if accuracy > best_accuracy:\n","            print(f'Best accuracy improved from {best_accuracy} to {accuracy}. Saving the model.')\n","            torch.save(model.state_dict(), to_checkpoint)\n","            best_accuracy = accuracy\n","        valid_dataset.get_df_with_error().to_csv(str(error_artifact_dir_path / f'{epoch}.csv'), index=False)"]},{"cell_type":"markdown","metadata":{},"source":["### Parameter definitions"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T21:30:44.917294Z","iopub.status.busy":"2021-12-13T21:30:44.917102Z","iopub.status.idle":"2021-12-13T21:30:44.927821Z","shell.execute_reply":"2021-12-13T21:30:44.926808Z","shell.execute_reply.started":"2021-12-13T21:30:44.917271Z"},"papermill":{"duration":0.018228,"end_time":"2021-12-13T15:32:53.847052","exception":false,"start_time":"2021-12-13T15:32:53.828824","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Parameters\n","\n","IS_KAGGLE = False\n","\n","ROOT_DIR_PATH = Path('/kaggle') if IS_KAGGLE else Path('/home/jovyan/jigsaw-toxic')\n","DATA_DIR_PATH = ROOT_DIR_PATH / ('input' if IS_KAGGLE else 'data/datasets')\n","DATASET_DIR_PATH = DATA_DIR_PATH / 'ccc-2017-multilabel'\n","TRAIN_CSV_PATH = DATASET_DIR_PATH / 'train_no_leak_pair_harder_3.csv'\n","VALID_CSV_PATH = DATASET_DIR_PATH / 'valid_pair.csv'\n","TENSORBOARD_DIR_PATH = ROOT_DIR_PATH / 'working/tensorboard' if IS_KAGGLE else Path('/home/jovyan/tensorboard')\n","ARTIFACT_DIR_PATH = ROOT_DIR_PATH / 'working/artifacts' if IS_KAGGLE else ROOT_DIR_PATH / 'artifacts'\n","\n","MODELS_DIR_PATH = ROOT_DIR_PATH / ('working/models' if IS_KAGGLE else 'models')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T21:30:44.929959Z","iopub.status.busy":"2021-12-13T21:30:44.929477Z","iopub.status.idle":"2021-12-13T21:30:45.598668Z","shell.execute_reply":"2021-12-13T21:30:45.597732Z","shell.execute_reply.started":"2021-12-13T21:30:44.929844Z"},"papermill":{"duration":0.676761,"end_time":"2021-12-13T15:32:54.534335","exception":false,"start_time":"2021-12-13T15:32:53.857574","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Create potentially missing directories\n","!mkdir -p $MODELS_DIR_PATH\n","!mkdir -p $ARTIFACT_DIR_PATH"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T21:30:45.600795Z","iopub.status.busy":"2021-12-13T21:30:45.600504Z","iopub.status.idle":"2021-12-13T21:30:45.703839Z","shell.execute_reply":"2021-12-13T21:30:45.703089Z","shell.execute_reply.started":"2021-12-13T21:30:45.600761Z"},"papermill":{"duration":2.228002,"end_time":"2021-12-13T15:32:56.773438","exception":false,"start_time":"2021-12-13T15:32:54.545436","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Read dataframes\n","\n","train_df = pd.read_csv(TRAIN_CSV_PATH)\n","valid_df = pd.read_csv(VALID_CSV_PATH)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_df['more_toxic_cleaned'] = train_df['more_toxic'].progress_apply(lambda text: text_cleaner(text))\n","train_df['less_toxic_cleaned'] = train_df['less_toxic'].progress_apply(lambda text: text_cleaner(text))\n","valid_df['more_toxic_cleaned'] = valid_df['more_toxic'].progress_apply(lambda text: text_cleaner(text))\n","valid_df['less_toxic_cleaned'] = valid_df['less_toxic'].progress_apply(lambda text: text_cleaner(text))"]},{"cell_type":"markdown","metadata":{},"source":["### Entrypoint"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T21:30:45.747655Z","iopub.status.busy":"2021-12-13T21:30:45.74677Z","iopub.status.idle":"2021-12-13T21:38:07.729693Z","shell.execute_reply":"2021-12-13T21:38:07.728387Z","shell.execute_reply.started":"2021-12-13T21:30:45.747614Z"},"papermill":{"duration":null,"end_time":null,"exception":false,"start_time":"2021-12-13T15:32:56.912308","status":"running"},"tags":[],"trusted":true},"outputs":[],"source":["# if IS_KAGGLE:\n","#     from kaggle_secrets import UserSecretsClient\n","#     user_secrets = UserSecretsClient()\n","#     wandb_api_key = user_secrets.get_secret('wandb-api-token')\n","# else:\n","#     with open(os.path.join(os.path.dirname(os.getcwd()), 'deploy/secrets/wandb_api_token.txt')) as f:\n","#         wandb_api_key = f.read()\n","\n","model_name = f'ccc-2017-multilabel-linear-harder-cls-loss_0p5'\n","error_artifact_dir_path = ARTIFACT_DIR_PATH / model_name\n","error_artifact_dir_path.mkdir(exist_ok=True)\n","with ContextManagerList([\n","            # StdOutLogger(),\n","            TensorBoardLogger(\n","                log_dir=str(TENSORBOARD_DIR_PATH / f'jt-{model_name}'),\n","                metric_whitelist={\n","                    'train_loss',\n","                    'train_accuracy_0.0',\n","                    'train_accuracy_0.5',\n","                    'valid_accuracy_0.0',\n","                    'valid_accuracy_0.5',\n","                },\n","            ),\n","        ]) as logger_list:\n","    main(\n","        train_df=train_df,\n","        valid_df=valid_df,\n","        num_classes=6,\n","        from_checkpoint='roberta-base',\n","        to_checkpoint=str(MODELS_DIR_PATH / f'{model_name}.pt'),\n","        error_artifact_dir_path=error_artifact_dir_path,\n","        logger_list=logger_list,\n","        num_epochs=100,\n","        batch_size=4,\n","        max_len=256,\n","        num_workers=8,\n","        device='cuda',\n","        output_logits=768,\n","        lr=1e-4,\n","        t_0=100,\n","        eta_min=1e-6,\n","        train_margin_list=[0.0, 0.5, 1.0],\n","        train_decision_margin=1.0,\n","        valid_margin_list=[0.0, 0.5, 1.0],\n","        valid_decision_margin=0.0,\n","        accumulate_gradient_steps=16,\n","        validate_every_n_steps=4096,\n","    )"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":4}
