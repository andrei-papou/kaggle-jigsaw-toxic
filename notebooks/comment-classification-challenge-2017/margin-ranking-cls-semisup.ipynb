{"cells":[{"cell_type":"markdown","metadata":{},"source":["### Imports"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T21:30:42.290752Z","iopub.status.busy":"2021-12-13T21:30:42.290324Z","iopub.status.idle":"2021-12-13T21:30:44.813935Z","shell.execute_reply":"2021-12-13T21:30:44.813044Z","shell.execute_reply.started":"2021-12-13T21:30:42.290611Z"},"papermill":{"duration":7.63065,"end_time":"2021-12-13T15:32:53.613901","exception":false,"start_time":"2021-12-13T15:32:45.983251","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["from __future__ import annotations\n","\n","import numpy as np\n","import os\n","import random\n","import shutil\n","import statistics\n","import typing as t\n","from pathlib import Path\n","\n","import pandas as pd\n","import torch\n","import torch.nn.functional as torch_f\n","import typing_extensions as t_ext\n","import wandb\n","from torch.optim import Optimizer, AdamW\n","from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n","from torch.utils.data import Dataset, DataLoader, Sampler\n","from torch.utils.tensorboard.writer import SummaryWriter\n","from tqdm.notebook import tqdm\n","from transformers.models.auto.configuration_auto import AutoConfig\n","from transformers.models.auto.modeling_auto import AutoModel\n","from transformers.models.auto.tokenization_auto import AutoTokenizer\n","from wandb.wandb_run import Run as WAndBRun"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["def seed_everything(seed: int):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","\n","seed_everything(42)"]},{"cell_type":"markdown","metadata":{},"source":["### Datasets"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T21:30:44.832059Z","iopub.status.busy":"2021-12-13T21:30:44.83171Z","iopub.status.idle":"2021-12-13T21:30:44.845976Z","shell.execute_reply":"2021-12-13T21:30:44.845075Z","shell.execute_reply.started":"2021-12-13T21:30:44.83202Z"},"papermill":{"duration":0.032712,"end_time":"2021-12-13T15:32:53.693077","exception":false,"start_time":"2021-12-13T15:32:53.660365","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["class _TokenizedText(t_ext.TypedDict):\n","    input_ids: torch.Tensor\n","    attention_mask: torch.Tensor\n","\n","\n","def _preprocess_tokenizer_output(output: t.Dict[str, t.Any]) -> _TokenizedText:\n","    return {\n","        'input_ids': torch.tensor(output['input_ids']),\n","        'attention_mask': torch.tensor(output['attention_mask']),\n","    }\n","\n","\n","class Tokenizer:\n","\n","    def tokenize(self, x: str) -> t.List[str]:\n","        return x.split(' ')\n","\n","    def invert_tokenize(self, x: t.List[str]) -> str:\n","        return ' '.join(x)\n","\n","\n","class RandomlyReduceTokenLenTo:\n","\n","    def __init__(self, token_len: int, tokenizer: t.Optional[Tokenizer] = None):\n","        self._token_len = token_len\n","        self._tokenizer = tokenizer if tokenizer is not None else Tokenizer()\n","\n","    def __call__(self, text: str) -> str:\n","        token_list = self._tokenizer.tokenize(text)\n","        if len(token_list) <= self._token_len:\n","            return text\n","        idx_set = set(random.choices(list(range(len(token_list))), k=self._token_len))\n","        return self._tokenizer.invert_tokenize([token for idx, token in enumerate(token_list) if idx in idx_set])\n","\n","\n","class RandomSubsetPerEpochSampler(Sampler[int]):\n","\n","    @staticmethod\n","    def _build_index(data_source: t.Sized) -> t.List[int]:\n","        index = list(range(len(data_source)))\n","        random.shuffle(index)\n","        return index\n","\n","    def __init__(self, data_source: t.Sized, samples_per_epoch: int):\n","        super().__init__(data_source)\n","        self._data_source = data_source\n","        self._samples_per_epoch = samples_per_epoch\n","        self._index: t.List[int] = self._build_index(data_source)\n","        self._real_epoch = 0\n","    \n","    def _sample_one(self) -> int:\n","        if not self._index:\n","            self._index = self._build_index(self._data_source)\n","            self._real_epoch += 1\n","        return self._index.pop()\n","\n","    def __iter__(self) -> t.Iterator[int]:\n","        return iter([self._sample_one() for _ in range(self._samples_per_epoch)])\n","\n","    def __len__(self) -> int:\n","        return self._samples_per_epoch\n","\n","    @property\n","    def real_epoch(self) -> int:\n","        return self._real_epoch\n","\n","    @property\n","    def frac_left(self) -> float:\n","        return len(self._index) / len(self._data_source)\n","\n","    @property\n","    def frac_consumed(self) -> float:\n","        return 1.0 - self.frac_left\n","\n","\n","class TrainDataset(Dataset):\n","\n","    def __init__(\n","            self,\n","            df: pd.DataFrame,\n","            tokenizer: AutoTokenizer,\n","            max_len: int,\n","            augmentation_list: t.Optional[t.List[t.Callable[[str], str]]] = None):\n","        super().__init__()\n","        self._df = df\n","        self._tokenizer = tokenizer\n","        self._max_len = max_len\n","        self._augmentation_list = augmentation_list if augmentation_list is not None else []\n","\n","    def __len__(self) -> int:\n","        return len(self._df)\n","\n","    def _apply_augmentations(self, text: str) -> str:\n","        for augmentation in self._augmentation_list:\n","            text = augmentation(text)\n","        return text\n","\n","    def __getitem__(self, idx: int) -> t.Tuple[_TokenizedText, _TokenizedText, torch.Tensor, torch.Tensor]:\n","        record_left = self._df.iloc[idx]\n","        record_right = self._df[self._df.index != idx].sample(n=1).iloc[0]\n","        left_comment_text = self._apply_augmentations(str(record_left['comment_text']))\n","        right_comment_text = self._apply_augmentations(str(record_right['comment_text']))\n","        tokenized_text_left = _preprocess_tokenizer_output(self._tokenizer(\n","            left_comment_text,\n","            add_special_tokens=True,\n","            truncation=True,\n","            padding='max_length',\n","            max_length=self._max_len,\n","            return_attention_mask=True))  # type: ignore\n","        tokenized_text_right = _preprocess_tokenizer_output(self._tokenizer(\n","            right_comment_text,\n","            add_special_tokens=True,\n","            truncation=True,\n","            padding='max_length',\n","            max_length=self._max_len,\n","            return_attention_mask=True))  # type: ignore\n","        left_labels = torch.tensor([int(l) for l in record_left['labels'].split(' ')], dtype=torch.float32)\n","        right_labels = torch.tensor([int(l) for l in record_right['labels'].split(' ')], dtype=torch.float32)\n","        return tokenized_text_left, tokenized_text_right, left_labels, right_labels\n","\n","\n","def _split_str_to_chunk_list(s: str, chunk_size: int) -> t.List[str]:\n","    chunk_list = []\n","    chunk = []\n","    for token in s.split(' '):\n","        chunk.append(token)\n","        if len(chunk) >= chunk_size:\n","            chunk_list.append(' '.join(chunk))\n","            chunk.clear()\n","    if chunk:\n","        chunk_list.append(' '.join(chunk))\n","    return chunk_list\n","\n","\n","def valid_collate_fn(\n","        sample_list: t.List[t.Tuple[int, _TokenizedText, _TokenizedText]]\n","        ) -> t.Tuple[t.List[int], _TokenizedText, _TokenizedText, t.List[slice], t.List[slice]]:\n","    curr_pos_more, curr_pos_less = 0, 0\n","\n","    idx_list: t.List[int] = []\n","    more_input_ids_list, less_input_ids_list = [], []\n","    more_attention_mask_list, less_attention_mask_list = [], []\n","    more_slice_list: t.List[slice] = []\n","    less_slice_list: t.List[slice] = []\n","    \n","    for sample in sample_list:\n","        idx_list.append(sample[0])\n","        more_input_ids, more_attention_mask = sample[1]['input_ids'], sample[1]['attention_mask']\n","        less_input_ids, less_attention_mask = sample[2]['input_ids'], sample[2]['attention_mask']\n","        more_input_ids_list.append(more_input_ids)\n","        less_input_ids_list.append(less_input_ids)\n","        more_attention_mask_list.append(more_attention_mask)\n","        less_attention_mask_list.append(less_attention_mask)\n","        more_slice_list.append(slice(curr_pos_more, curr_pos_more + more_input_ids.shape[0]))\n","        curr_pos_more += more_input_ids.shape[0]\n","        less_slice_list.append(slice(curr_pos_less, curr_pos_less + less_input_ids.shape[0]))\n","        curr_pos_less += less_input_ids.shape[0]\n","\n","    more_tokenized_collated: _TokenizedText = {\n","        'input_ids': torch.cat(more_input_ids_list, dim=0),\n","        'attention_mask': torch.cat(more_attention_mask_list, dim=0),\n","    }\n","    less_tokenized_collated: _TokenizedText = {\n","        'input_ids': torch.cat(less_input_ids_list, dim=0),\n","        'attention_mask': torch.cat(less_attention_mask_list, dim=0),\n","    }\n","\n","    return idx_list, more_tokenized_collated, less_tokenized_collated, more_slice_list, less_slice_list\n","\n","\n","class ValidDataset(Dataset):\n","\n","    def __init__(self, df: pd.DataFrame, tokenizer: AutoTokenizer, max_len: int) -> None:\n","        super().__init__()\n","        self._df = df\n","        self._tokenizer = tokenizer\n","        self._max_len = max_len\n","        self._error: np.ndarray = np.zeros(len(df))\n","\n","    def __len__(self) -> int:\n","        return len(self._df)\n","\n","    def track_error(self, idx_list: t.List[int], error: np.ndarray):\n","        self._error[idx_list] = error\n","\n","    def get_df_with_error(self) -> pd.DataFrame:\n","        df = self._df.copy()\n","        df['error'] = self._error\n","        return df\n","\n","    def __getitem__(self, idx: int) -> t.Tuple[int, _TokenizedText, _TokenizedText]:\n","        record = self._df.iloc[idx]\n","        text_more = str(record['more_toxic'])\n","        text_less = str(record['less_toxic'])\n","\n","        more_input_ids_list, less_input_ids_list = [], []\n","        more_attention_mask_list, less_attention_mask_list = [], []\n","        for chunk in _split_str_to_chunk_list(text_more, chunk_size=self._max_len):\n","            tokenized_chunk = _preprocess_tokenizer_output(self._tokenizer(\n","                chunk,\n","                add_special_tokens=True,\n","                truncation=True,\n","                padding='max_length',\n","                max_length=self._max_len,\n","                return_attention_mask=True))  # type: ignore\n","            more_input_ids_list.append(tokenized_chunk['input_ids'])\n","            more_attention_mask_list.append(tokenized_chunk['attention_mask'])\n","        for chunk in _split_str_to_chunk_list(text_less, chunk_size=self._max_len):\n","            tokenized_chunk = _preprocess_tokenizer_output(self._tokenizer(\n","                chunk,\n","                add_special_tokens=True,\n","                truncation=True,\n","                padding='max_length',\n","                max_length=self._max_len,\n","                return_attention_mask=True))  # type: ignore\n","            less_input_ids_list.append(tokenized_chunk['input_ids'])\n","            less_attention_mask_list.append(tokenized_chunk['attention_mask'])\n","\n","        tokenized_more: _TokenizedText = {\n","            'input_ids': torch.stack(more_input_ids_list, dim=0),\n","            'attention_mask': torch.stack(more_attention_mask_list, dim=0),\n","        }\n","        tokenized_less: _TokenizedText = {\n","            'input_ids': torch.stack(less_input_ids_list, dim=0),\n","            'attention_mask': torch.stack(less_attention_mask_list, dim=0),\n","        }\n","\n","        return idx, tokenized_more, tokenized_less\n"]},{"cell_type":"markdown","metadata":{},"source":["### Model"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T21:30:44.848904Z","iopub.status.busy":"2021-12-13T21:30:44.848506Z","iopub.status.idle":"2021-12-13T21:30:44.859529Z","shell.execute_reply":"2021-12-13T21:30:44.858898Z","shell.execute_reply.started":"2021-12-13T21:30:44.848854Z"},"papermill":{"duration":0.020746,"end_time":"2021-12-13T15:32:53.724218","exception":false,"start_time":"2021-12-13T15:32:53.703472","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["class WeightedAverageLinearRegressor(torch.nn.Linear):\n","\n","    def __init__(self, in_features: int, device: t.Optional[str] = None, dtype: t.Optional[str] = None):\n","        super().__init__(in_features=in_features, out_features=1, bias=False, device=device, dtype=dtype)\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        return torch_f.linear(x, torch_f.softmax(self.weight, dim=1), self.bias)\n","\n","\n","class AttentionRegressor(torch.nn.Module):\n","\n","    def __init__(self, in_features: int) -> None:\n","        super().__init__()\n","        self.attention = torch.nn.Linear(in_features=in_features, out_features=in_features, bias=False)\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        weight = self.attention(x)\n","        return (x * torch_f.softmax(weight, dim=1)).sum(dim=1)\n","\n","\n","class Model(torch.nn.Module):\n","\n","    def __init__(self, checkpoint: str, output_logits: int, num_classes: int):\n","        super(Model, self).__init__()\n","        self.encoder = AutoModel.from_pretrained(checkpoint, return_dict=False)\n","        self.classifier = torch.nn.Sequential(\n","            # torch.nn.LayerNorm(output_logits),\n","            torch.nn.Linear(output_logits, 128),\n","            torch.nn.ReLU(),\n","            torch.nn.Linear(128, num_classes))\n","        self.regressor = WeightedAverageLinearRegressor(in_features=num_classes)\n","\n","    def forward_scores(self, label_preds: torch.Tensor) -> torch.Tensor:\n","        return self.regressor(label_preds)\n","\n","    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> t.Tuple[torch.Tensor, torch.Tensor]:\n","        _, pooled_output = self.encoder(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask)\n","        label_preds = self.classifier(pooled_output)\n","        scores = self.forward_scores(torch.sigmoid(label_preds))\n","        return label_preds, scores\n","\n","\n","# class Model(torch.nn.Module):\n","\n","#     def __init__(self, checkpoint: str, output_logits: int, num_classes: int):\n","#         super(Model, self).__init__()\n","#         self.encoder = AutoModel.from_pretrained(checkpoint, return_dict=False)\n","#         self.blind_regressor = torch.nn.Sequential(\n","#             torch.nn.Linear(output_logits, 1),\n","#             torch.nn.Sigmoid())\n","#         self.classifier = torch.nn.Sequential(\n","#             # torch.nn.LayerNorm(output_logits),\n","#             torch.nn.Linear(output_logits, 128),\n","#             torch.nn.ReLU(),\n","#             torch.nn.Linear(128, num_classes))\n","#         self.regressor = AttentionRegressor(in_features=num_classes + 1)\n","\n","#     def forward_scores(self, label_preds: torch.Tensor) -> torch.Tensor:\n","#         return self.regressor(label_preds)\n","\n","#     def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> t.Tuple[torch.Tensor, torch.Tensor]:\n","#         _, pooled_output = self.encoder(\n","#             input_ids=input_ids,\n","#             attention_mask=attention_mask)\n","#         label_preds = self.classifier(pooled_output)\n","#         scores = self.forward_scores(torch.cat([torch.sigmoid(label_preds), self.blind_regressor(pooled_output)], dim=1))\n","#         return label_preds, scores"]},{"cell_type":"markdown","metadata":{},"source":["### Metrics"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T21:30:44.861454Z","iopub.status.busy":"2021-12-13T21:30:44.860954Z","iopub.status.idle":"2021-12-13T21:30:44.878684Z","shell.execute_reply":"2021-12-13T21:30:44.877857Z","shell.execute_reply.started":"2021-12-13T21:30:44.861415Z"},"papermill":{"duration":0.02804,"end_time":"2021-12-13T15:32:53.762852","exception":false,"start_time":"2021-12-13T15:32:53.734812","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["class _Metric:\n","\n","    def compute(self) -> float:\n","        raise NotImplementedError()\n","\n","    def reset(self):\n","        raise NotImplementedError()\n","\n","    def compute_and_reset(self) -> float:\n","        value = self.compute()\n","        self.reset()\n","        return value\n","\n","\n","class Accuracy(_Metric):\n","\n","    def __init__(self):\n","        self._num_correct = 0\n","        self._num_total = 0\n","\n","    def update(self, is_correct_tensor: torch.Tensor):\n","        num_correct = (is_correct_tensor == 1).int().sum().item()\n","        num_total = num_correct + (is_correct_tensor != 1).int().sum().item()\n","        self._num_correct += num_correct\n","        self._num_total += num_total\n","\n","    def compute(self) -> float:\n","        assert self._num_total > 0\n","        return self._num_correct / self._num_total\n","\n","    def reset(self):\n","        self._num_correct = 0\n","        self._num_total = 0\n","\n","\n","class _FloatListMetric(_Metric):\n","\n","    def __init__(self):\n","        self._value_list: t.List[float] = []\n","\n","    def update(self, value_tensor: torch.Tensor):\n","        self._value_list.extend(value_tensor.flatten().tolist())\n","\n","    def reset(self):\n","        self._value_list.clear()\n","\n","\n","class FloatListMean(_FloatListMetric):\n","\n","    def compute(self) -> float:\n","        return statistics.mean(self._value_list)\n","\n","\n","class FloatListStd(_FloatListMetric):\n","\n","    def compute(self) -> float:\n","        return statistics.stdev(self._value_list)"]},{"cell_type":"markdown","metadata":{},"source":["### Loggers"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["_C = t.TypeVar('_C')\n","\n","\n","class ContextManagerList(t.Generic[_C]):\n","\n","    def __init__(self, cm_list: t.List[t.ContextManager[_C]]):\n","        self._cm_list = cm_list\n","\n","    def __enter__(self) -> t.List[_C]:\n","        return [cm.__enter__() for cm in self._cm_list]\n","\n","    def __exit__(self, *args, **kwargs):\n","        for cm in self._cm_list:\n","            cm.__exit__(*args, **kwargs)\n","\n","\n","class Logger:\n","\n","    def __enter__(self) -> Logger:\n","        return self\n","\n","    def __exit__(self, *args, **kwargs):\n","        pass\n","\n","    def log_params(self, params: t.Dict[str, t.Any]):\n","        raise NotImplementedError()\n","\n","    def log_metrics(self, step: int, metrics: t.Dict[str, float]):\n","        raise NotImplementedError()\n","\n","\n","class StdOutLogger(Logger):\n","\n","    def log_params(self, params: t.Dict[str, t.Any]):\n","        print('Using params:')\n","        for param in sorted(params.keys()):\n","            print(f'\\t{param} = {params[param]}')\n","\n","    def log_metrics(self, step: int, metrics: t.Dict[str, float]):\n","        print(f'Step {step} metrics:')\n","        for m in sorted(metrics.keys()):\n","            print(f'\\t{m} = {metrics[m]:.8f}')\n","\n","\n","class TensorBoardLogger(Logger):\n","\n","    def __init__(self, log_dir: str, metric_whitelist: t.Optional[t.Set[str]] = None) -> None:\n","        self._metric_whitelist = metric_whitelist\n","        self._writer = SummaryWriter(log_dir=log_dir)\n","\n","    def log_params(self, params: t.Dict[str, t.Any]):\n","        pass  # TODO: handle hyperparams properly.\n","\n","    def log_metrics(self, step: int, metrics: t.Dict[str, float]):\n","        for metric_key, metric_val in metrics.items():\n","            if self._metric_whitelist is None or metric_key in self._metric_whitelist:\n","                self._writer.add_scalar(tag=metric_key, scalar_value=metric_val, global_step=step)\n","\n","\n","class WAndBLogger(Logger):\n","\n","    def __init__(self, user_name: str, api_key: str, project: str, run_id: str):\n","        wandb.login(key=api_key)\n","        self._user_name = user_name\n","        self._project = project\n","        self._run_id = run_id\n","        self._run: t.Optional[WAndBRun] = None\n","    \n","    @property\n","    def run(self) -> WAndBRun:\n","        assert self._run is not None\n","        return self._run\n","\n","    def __enter__(self) -> WAndBLogger:\n","        self._run = wandb.init(project=self._project, entity=self._user_name, run_id=self._run_id)\n","        return self\n","    \n","    def log_params(self, params: t.Dict[str, t.Any]):\n","        self.run.config.update(params)\n","\n","    def log_metrics(self, step: int, metrics: t.Dict[str, float]):\n","        self.run.log(step=step, data=metrics)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Schedulers"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["class ForwardScheduledFloat:\n","\n","    def __init__(self, start: float, end: float, step: float):\n","        self._end = end\n","        self._step = step\n","        self._val = start\n","\n","    @property\n","    def value(self) -> float:\n","        return self._val\n","\n","    def step(self):\n","        if self._val < self._end:\n","            self._val += self._step"]},{"cell_type":"markdown","metadata":{},"source":["### Loss"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["def margin_ranking_loss(\n","        more_scores: torch.Tensor,\n","        less_scores: torch.Tensor,\n","        diff: torch.Tensor,\n","        margin: float,\n","        device: str,) -> torch.Tensor:\n","    return torch.maximum(torch.tensor(0.0, device=device), less_scores - more_scores + margin * diff).mean()"]},{"cell_type":"markdown","metadata":{},"source":["### Iteration functions"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T21:30:44.881104Z","iopub.status.busy":"2021-12-13T21:30:44.880154Z","iopub.status.idle":"2021-12-13T21:30:44.914119Z","shell.execute_reply":"2021-12-13T21:30:44.913266Z","shell.execute_reply.started":"2021-12-13T21:30:44.881064Z"},"papermill":{"duration":0.044539,"end_time":"2021-12-13T15:32:53.818306","exception":false,"start_time":"2021-12-13T15:32:53.773767","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def do_train_iteration(\n","        data_loader: DataLoader,\n","        model: Model,\n","        device: str,\n","        optimizer: Optimizer,\n","        scheduler: CosineAnnealingWarmRestarts,\n","        ranking_loss_part: float,\n","        train_margin_list: t.Optional[t.List[float]] = None,\n","        train_decision_margin: float = 0.3,\n","        accumulate_gradient_steps: int = 1,\n","        num_steps: t.Optional[int] = None) -> t.Dict[str, float]:\n","    sampler: RandomSubsetPerEpochSampler = t.cast(RandomSubsetPerEpochSampler, data_loader.sampler)\n","    loss_metric = FloatListMean()\n","    train_score_mean = FloatListMean()\n","    train_score_std = FloatListStd()\n","    if train_margin_list is None:\n","        train_margin_list = [train_decision_margin]\n","    assert train_decision_margin in train_margin_list\n","    train_accuracy_dict = {m: Accuracy() for m in train_margin_list}\n","\n","    model.train()\n","    data_iter = tqdm(data_loader, desc='Training', total=num_steps if num_steps is not None else len(data_loader))\n","    for step, (tokenized_text_left, tokenized_text_right, labels_left, labels_right) in enumerate(data_iter):\n","        (\n","            input_ids_left,\n","            attention_mask_left,\n","            input_ids_right,\n","            attention_mask_right,\n","            labels_left,\n","            labels_right,\n","        ) = (\n","            tokenized_text_left['input_ids'].to(device),\n","            tokenized_text_left['attention_mask'].to(device),\n","            tokenized_text_right['input_ids'].to(device),\n","            tokenized_text_right['attention_mask'].to(device),\n","            labels_left.to(device),\n","            labels_right.to(device),\n","        )\n","        preds_left, score_left = model(input_ids_left, attention_mask_left)\n","        preds_right, score_right = model(input_ids_right, attention_mask_right)\n","        score_left_approx = model.forward_scores(labels_left)\n","        score_right_approx = model.forward_scores(labels_right)\n","        \n","        more_left_mask = (score_left_approx > score_right_approx).float()\n","        more_right_mask = (score_right_approx > score_left_approx).float()\n","        less_left_mask = (score_left_approx <= score_right_approx).float()\n","        less_right_mask = (score_right_approx <= score_left_approx).float()\n","        score_more_approx = more_left_mask * score_left_approx + more_right_mask * score_right_approx\n","        score_less_approx = less_left_mask * score_left_approx + less_right_mask * score_right_approx\n","        score_more = more_left_mask * score_left + more_right_mask * score_right\n","        score_less = less_left_mask * score_left + less_right_mask * score_right\n","\n","        cls_left_loss = torch_f.multilabel_soft_margin_loss(preds_left, labels_left)\n","        cls_right_loss = torch_f.multilabel_soft_margin_loss(preds_right, labels_right)\n","        ranking_loss = margin_ranking_loss(score_more, score_less, margin=train_decision_margin, diff=score_more_approx - score_less_approx, device=device)\n","        loss = ranking_loss_part * ranking_loss + (1.0 - ranking_loss_part) * (cls_left_loss + cls_right_loss) / 2\n","        loss.backward()\n","\n","        if (step + 1) % accumulate_gradient_steps == 0:\n","            optimizer.step()\n","            optimizer.zero_grad()\n","            scheduler.step()\n","\n","        with torch.no_grad():\n","            score_more_cpu = score_more.cpu()\n","            train_score_mean.update(score_more_cpu)\n","            train_score_std.update(score_more_cpu)\n","            score_less_cpu = score_less.cpu()\n","            train_score_mean.update(score_less_cpu)\n","            train_score_std.update(score_less_cpu)\n","\n","            loss_metric.update(loss.cpu())\n","            for m, a in train_accuracy_dict.items():\n","                a.update(score_more - score_less > m)\n","        epoch_str = f'epoch: {sampler.real_epoch} [{sampler.frac_consumed:.4f}]'\n","        accuracy_str = ', '.join([f'acc_{m}: {train_accuracy_dict[m].compute():.4f}' for m in sorted(train_accuracy_dict.keys())])\n","        data_iter.set_description(\n","            f'{epoch_str} th: {ranking_loss_part:.2f} loss: {loss_metric.compute():.4f}, {accuracy_str} '\n","            f'score_mean: {train_score_mean.compute():.6f}, score_std: {train_score_std.compute():.6f}')\n","\n","        if num_steps is not None and step >= num_steps - 1:\n","            break\n","\n","    train_metrics_to_track = {f'train_accuracy_{m}': a.compute() for m, a in train_accuracy_dict.items()}\n","    loss_val = loss_metric.compute_and_reset()\n","    return {\n","        'train_loss': loss_val,\n","        'train_score_mean': train_score_mean.compute(),\n","        'train_score_std': train_score_std.compute(),\n","        **train_metrics_to_track,\n","    }\n","\n","\n","@torch.no_grad()\n","def do_valid_iteration(\n","        data_loader: DataLoader,\n","        model: Model,\n","        device: str,\n","        margin_list: t.Optional[t.List[float]] = None,\n","        decision_margin: float = 0.0) -> t.Tuple[float, t.Dict[str, float]]:\n","    if margin_list is None:\n","        margin_list = [decision_margin]\n","    assert decision_margin in margin_list\n","    accuracy_dict = {margin: Accuracy() for margin in margin_list}\n","    valid_score_mean = FloatListMean()\n","    valid_score_std = FloatListStd()\n","    model.eval()\n","    it = tqdm(data_loader, desc='Validation')\n","    for idx_list, tokenized_text_more, tokenized_text_less, slice_list_more, slice_list_less in it:\n","        _, score_more = model(\n","            tokenized_text_more['input_ids'].to(device),\n","            tokenized_text_more['attention_mask'].to(device),)\n","        _, score_less = model(\n","            tokenized_text_less['input_ids'].to(device),\n","            tokenized_text_less['attention_mask'].to(device),)\n","        score_more = torch.cat([torch.max(score_more[s], dim=0, keepdim=True)[0] for s in slice_list_more], dim=0)\n","        score_less = torch.cat([torch.max(score_less[s], dim=0, keepdim=True)[0] for s in slice_list_less], dim=0)\n","        score_more_cpu = score_more.cpu()\n","        valid_score_mean.update(score_more_cpu)\n","        valid_score_std.update(score_more_cpu)\n","        score_less_cpu = score_less.cpu()\n","        valid_score_mean.update(score_less_cpu)\n","        valid_score_std.update(score_less_cpu)\n","        for margin, accuracy_metric in accuracy_dict.items():\n","            accuracy_metric.update(((score_more - score_less) > margin).cpu())\n","        data_loader.dataset.track_error(\n","            idx_list, torch.maximum(torch.zeros_like(score_less_cpu), score_less_cpu - score_more_cpu).squeeze(1))\n","        accuracy_str = ', '.join([f'acc_{m}: {accuracy_dict[m].compute():.4f}' for m in sorted(accuracy_dict.keys())])\n","        it.set_description(f'Validation. {accuracy_str}')\n","    score_dict_to_track = {\n","        'valid_score_mean': valid_score_mean.compute(),\n","        'valid_score_std': valid_score_std.compute(),\n","    }\n","    accuracy_dict_to_track =  {f'valid_accuracy_{m}': a.compute() for m, a in accuracy_dict.items()}\n","    return accuracy_dict[decision_margin].compute(), {**score_dict_to_track, **accuracy_dict_to_track}"]},{"cell_type":"markdown","metadata":{},"source":["### Main function"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T21:30:45.729874Z","iopub.status.busy":"2021-12-13T21:30:45.729205Z","iopub.status.idle":"2021-12-13T21:30:45.744702Z","shell.execute_reply":"2021-12-13T21:30:45.74378Z","shell.execute_reply.started":"2021-12-13T21:30:45.729823Z"},"papermill":{"duration":0.028452,"end_time":"2021-12-13T15:32:56.900841","exception":false,"start_time":"2021-12-13T15:32:56.872389","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def main(\n","        train_df: pd.DataFrame,\n","        valid_df: pd.DataFrame,\n","        num_classes: int,\n","        from_checkpoint: str,\n","        to_checkpoint: str,\n","        error_artifact_dir_path: Path,\n","        logger_list: t.List[Logger],\n","        num_epochs: int,\n","        batch_size: int,\n","        max_len: int,\n","        num_workers: int,\n","        device: str,\n","        output_logits: int,\n","        lr: float,\n","        t_0: int,\n","        eta_min: float,\n","        train_margin_list: t.List[float],\n","        train_decision_margin: float,\n","        valid_margin_list: t.List[float],\n","        valid_decision_margin: float,\n","        accumulate_gradient_steps: int = 1,\n","        validate_every_n_steps: t.Optional[int] = None):    \n","    for logger in logger_list:\n","        logger.log_params({\n","            'from_checkpoint': from_checkpoint,\n","            'lr': lr,\n","            'batch_size': batch_size,\n","            'output_logits': output_logits,\n","            'max_len': max_len,\n","            'num_epochs': num_epochs,\n","            'optimizer': 'adam_w',\n","            'scheduler': 'cosine_annealing_warm_restarts',\n","            'accumulate_gradient_steps': accumulate_gradient_steps,\n","        })\n","    \n","    tokenizer = AutoTokenizer.from_pretrained(from_checkpoint)\n","    train_dataset = TrainDataset(\n","        train_df,\n","        tokenizer=tokenizer,\n","        max_len=max_len,\n","        augmentation_list=[\n","            # RandomlyReduceTokenLenTo(token_len=max_len),\n","        ])\n","    valid_dataset = ValidDataset(\n","        valid_df,\n","        tokenizer=tokenizer,\n","        max_len=max_len)\n","    train_data_loader = DataLoader(\n","        dataset=train_dataset,\n","        sampler=RandomSubsetPerEpochSampler(\n","            data_source=train_dataset,\n","            samples_per_epoch=validate_every_n_steps if validate_every_n_steps is not None else len(train_dataset)),\n","        batch_size=batch_size,\n","        num_workers=num_workers,\n","        pin_memory=True)\n","    valid_data_loader = DataLoader(\n","        dataset=valid_dataset,\n","        batch_size=batch_size * 2,\n","        num_workers=num_workers,\n","        shuffle=False,\n","        pin_memory=True,\n","        collate_fn=valid_collate_fn)  # type: ignore\n","    model = Model(\n","        checkpoint=from_checkpoint,\n","        output_logits=output_logits,\n","        num_classes=num_classes).to(device)\n","    optimizer = AdamW(model.parameters(), lr=lr)\n","    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=t_0, eta_min=eta_min)\n","\n","    best_accuracy = 0.0\n","    for epoch in range(num_epochs):\n","        train_metrics_to_track = do_train_iteration(\n","            data_loader=train_data_loader,\n","            model=model,\n","            device=device,\n","            optimizer=optimizer,\n","            scheduler=scheduler,\n","            train_margin_list=train_margin_list,\n","            train_decision_margin=train_decision_margin,\n","            accumulate_gradient_steps=accumulate_gradient_steps,\n","            ranking_loss_part=0.5)\n","        accuracy, valid_metrics_to_track = do_valid_iteration(\n","            data_loader=valid_data_loader,\n","            model=model,\n","            device=device,\n","            margin_list=valid_margin_list,\n","            decision_margin=valid_decision_margin)\n","        for logger in logger_list:\n","            logger.log_metrics(step=epoch, metrics={\n","                **train_metrics_to_track,\n","                **valid_metrics_to_track,\n","            })\n","        if accuracy > best_accuracy:\n","            print(f'Best accuracy improved from {best_accuracy} to {accuracy}. Saving the model.')\n","            torch.save(model.state_dict(), to_checkpoint)\n","            best_accuracy = accuracy\n","        valid_dataset.get_df_with_error().to_csv(str(error_artifact_dir_path / f'{epoch}.csv'), index=False)"]},{"cell_type":"markdown","metadata":{},"source":["### Parameter definitions"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T21:30:44.917294Z","iopub.status.busy":"2021-12-13T21:30:44.917102Z","iopub.status.idle":"2021-12-13T21:30:44.927821Z","shell.execute_reply":"2021-12-13T21:30:44.926808Z","shell.execute_reply.started":"2021-12-13T21:30:44.917271Z"},"papermill":{"duration":0.018228,"end_time":"2021-12-13T15:32:53.847052","exception":false,"start_time":"2021-12-13T15:32:53.828824","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Parameters\n","\n","IS_KAGGLE = False\n","\n","ROOT_DIR_PATH = Path('/kaggle') if IS_KAGGLE else Path('/home/jovyan/jigsaw-toxic')\n","DATA_DIR_PATH = ROOT_DIR_PATH / ('input' if IS_KAGGLE else 'data/datasets')\n","DATASET_DIR_PATH = DATA_DIR_PATH / 'ccc-2017-multilabel'\n","TRAIN_CSV_PATH = DATASET_DIR_PATH / 'train_no_leak_toxic.csv'\n","VALID_CSV_PATH = DATASET_DIR_PATH / 'valid_pair.csv'\n","TENSORBOARD_DIR_PATH = ROOT_DIR_PATH / 'working/tensorboard' if IS_KAGGLE else Path('/home/jovyan/tensorboard')\n","ARTIFACT_DIR_PATH = ROOT_DIR_PATH / 'working/artifacts' if IS_KAGGLE else ROOT_DIR_PATH / 'artifacts'\n","\n","TASK_NAME = 'margin-ranking'\n","DATASET_NAME = 'ccc-2017-multilabel'\n","RUN_NAME = 'semisum-cls-loss_0p5'\n","\n","MODEL_NAME = f'{DATASET_NAME}-{RUN_NAME}'\n","NOTEBOOK_CHECKPOINT_DIR_PATH = Path(f'.checkpoints/{TASK_NAME}')\n","MODELS_DIR_PATH = ROOT_DIR_PATH / ('working/models' if IS_KAGGLE else 'models')"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["if not IS_KAGGLE:\n","    os.makedirs(NOTEBOOK_CHECKPOINT_DIR_PATH, exist_ok=True)\n","    shutil.copyfile('margin-ranking-cls-semisup.ipynb', NOTEBOOK_CHECKPOINT_DIR_PATH / f'{RUN_NAME}.ipynb')"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T21:30:44.929959Z","iopub.status.busy":"2021-12-13T21:30:44.929477Z","iopub.status.idle":"2021-12-13T21:30:45.598668Z","shell.execute_reply":"2021-12-13T21:30:45.597732Z","shell.execute_reply.started":"2021-12-13T21:30:44.929844Z"},"papermill":{"duration":0.676761,"end_time":"2021-12-13T15:32:54.534335","exception":false,"start_time":"2021-12-13T15:32:53.857574","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Create potentially missing directories\n","!mkdir -p $MODELS_DIR_PATH\n","!mkdir -p $ARTIFACT_DIR_PATH"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T21:30:45.600795Z","iopub.status.busy":"2021-12-13T21:30:45.600504Z","iopub.status.idle":"2021-12-13T21:30:45.703839Z","shell.execute_reply":"2021-12-13T21:30:45.703089Z","shell.execute_reply.started":"2021-12-13T21:30:45.600761Z"},"papermill":{"duration":2.228002,"end_time":"2021-12-13T15:32:56.773438","exception":false,"start_time":"2021-12-13T15:32:54.545436","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Read dataframes\n","\n","train_df = pd.read_csv(TRAIN_CSV_PATH)\n","valid_df = pd.read_csv(VALID_CSV_PATH)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>comment_text</th>\n","      <th>toxic</th>\n","      <th>severe_toxic</th>\n","      <th>obscene</th>\n","      <th>threat</th>\n","      <th>insult</th>\n","      <th>identity_hate</th>\n","      <th>score</th>\n","      <th>n_flags</th>\n","      <th>labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0005c987bdfc9d4b</td>\n","      <td>Hey... what is it..\\n@ | talk .\\nWhat is it......</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.133333</td>\n","      <td>1</td>\n","      <td>1 0 0 0 0 0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0007e25b2121310b</td>\n","      <td>Bye! \\n\\nDon't look, come or think of comming ...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.133333</td>\n","      <td>1</td>\n","      <td>1 0 0 0 0 0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>001810bf8c45bf5f</td>\n","      <td>You are gay or antisemmitian? \\n\\nArchangel WH...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0.533333</td>\n","      <td>4</td>\n","      <td>1 0 1 0 1 1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>001956c382006abd</td>\n","      <td>I'm Sorry \\n\\nI'm sorry I screwed around with ...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.133333</td>\n","      <td>1</td>\n","      <td>1 0 0 0 0 0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>001dc38a83d420cf</td>\n","      <td>GET FUCKED UP. GET FUCKEEED UP.  GOT A DRINK T...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.266667</td>\n","      <td>2</td>\n","      <td>1 0 1 0 0 0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>12730</th>\n","      <td>fef4cf7ba0012866</td>\n","      <td>\"\\n\\n our previous conversation \\n\\nyou fuckin...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0.533333</td>\n","      <td>4</td>\n","      <td>1 0 1 0 1 1</td>\n","    </tr>\n","    <tr>\n","      <th>12731</th>\n","      <td>ff39a2895fc3b40e</td>\n","      <td>YOU ARE A MISCHIEVIOUS PUBIC HAIR</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.266667</td>\n","      <td>2</td>\n","      <td>1 0 0 0 1 0</td>\n","    </tr>\n","    <tr>\n","      <th>12732</th>\n","      <td>ffa33d3122b599d6</td>\n","      <td>Your absurd edits \\n\\nYour absurd edits on gre...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.400000</td>\n","      <td>3</td>\n","      <td>1 0 1 0 1 0</td>\n","    </tr>\n","    <tr>\n","      <th>12733</th>\n","      <td>ffb47123b2d82762</td>\n","      <td>\"\\n\\nHey listen don't you ever!!!! Delete my e...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.266667</td>\n","      <td>2</td>\n","      <td>1 0 0 0 1 0</td>\n","    </tr>\n","    <tr>\n","      <th>12734</th>\n","      <td>ffbdbb0483ed0841</td>\n","      <td>and i'm going to keep posting the stuff u dele...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0.400000</td>\n","      <td>3</td>\n","      <td>1 0 1 0 1 0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>12735 rows Ã— 11 columns</p>\n","</div>"],"text/plain":["                     id                                       comment_text  \\\n","0      0005c987bdfc9d4b  Hey... what is it..\\n@ | talk .\\nWhat is it......   \n","1      0007e25b2121310b  Bye! \\n\\nDon't look, come or think of comming ...   \n","2      001810bf8c45bf5f  You are gay or antisemmitian? \\n\\nArchangel WH...   \n","3      001956c382006abd  I'm Sorry \\n\\nI'm sorry I screwed around with ...   \n","4      001dc38a83d420cf  GET FUCKED UP. GET FUCKEEED UP.  GOT A DRINK T...   \n","...                 ...                                                ...   \n","12730  fef4cf7ba0012866  \"\\n\\n our previous conversation \\n\\nyou fuckin...   \n","12731  ff39a2895fc3b40e                  YOU ARE A MISCHIEVIOUS PUBIC HAIR   \n","12732  ffa33d3122b599d6  Your absurd edits \\n\\nYour absurd edits on gre...   \n","12733  ffb47123b2d82762  \"\\n\\nHey listen don't you ever!!!! Delete my e...   \n","12734  ffbdbb0483ed0841  and i'm going to keep posting the stuff u dele...   \n","\n","       toxic  severe_toxic  obscene  threat  insult  identity_hate     score  \\\n","0          1             0        0       0       0              0  0.133333   \n","1          1             0        0       0       0              0  0.133333   \n","2          1             0        1       0       1              1  0.533333   \n","3          1             0        0       0       0              0  0.133333   \n","4          1             0        1       0       0              0  0.266667   \n","...      ...           ...      ...     ...     ...            ...       ...   \n","12730      1             0        1       0       1              1  0.533333   \n","12731      1             0        0       0       1              0  0.266667   \n","12732      1             0        1       0       1              0  0.400000   \n","12733      1             0        0       0       1              0  0.266667   \n","12734      1             0        1       0       1              0  0.400000   \n","\n","       n_flags       labels  \n","0            1  1 0 0 0 0 0  \n","1            1  1 0 0 0 0 0  \n","2            4  1 0 1 0 1 1  \n","3            1  1 0 0 0 0 0  \n","4            2  1 0 1 0 0 0  \n","...        ...          ...  \n","12730        4  1 0 1 0 1 1  \n","12731        2  1 0 0 0 1 0  \n","12732        3  1 0 1 0 1 0  \n","12733        2  1 0 0 0 1 0  \n","12734        3  1 0 1 0 1 0  \n","\n","[12735 rows x 11 columns]"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["train_df"]},{"cell_type":"markdown","metadata":{},"source":["### Entrypoint"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T21:30:45.747655Z","iopub.status.busy":"2021-12-13T21:30:45.74677Z","iopub.status.idle":"2021-12-13T21:38:07.729693Z","shell.execute_reply":"2021-12-13T21:38:07.728387Z","shell.execute_reply.started":"2021-12-13T21:30:45.747614Z"},"papermill":{"duration":null,"end_time":null,"exception":false,"start_time":"2021-12-13T15:32:56.912308","status":"running"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"45518dc4832b4105a8ef0f75b5b6a080","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/1024 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"80748615b6ee4fb58263d585583c919a","version_major":2,"version_minor":0},"text/plain":["Validation:   0%|          | 0/1264 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Best accuracy improved from 0.0 to 0.7096359319351009. Saving the model.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f1d9cf007cdb4d0e83d2cf059705e275","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/1024 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"07841a1113b54dc288b62226144d96cf","version_major":2,"version_minor":0},"text/plain":["Validation:   0%|          | 0/1264 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f66fb54c39c743ca9efda87b7af650ac","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/1024 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"248602b2cf074b68b0a04557507a7191","version_major":2,"version_minor":0},"text/plain":["Validation:   0%|          | 0/1264 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Best accuracy improved from 0.7096359319351009 to 0.712801741195093. Saving the model.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"451a03fb4bb44a22ad56c46f7ab30f9d","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/1024 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"55197414050d4aa0bfd098db3447617d","version_major":2,"version_minor":0},"text/plain":["Validation:   0%|          | 0/1264 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Best accuracy improved from 0.712801741195093 to 0.7169568658488326. Saving the model.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5d519bcf3cfd45428921526de29d0ad3","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/1024 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"be8d2f825ddd4060abf537a8fad6e487","version_major":2,"version_minor":0},"text/plain":["Validation:   0%|          | 0/1264 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Best accuracy improved from 0.7169568658488326 to 0.7230906212900673. Saving the model.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f4ce516616af438b803f64ca1c2bc633","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/1024 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"61a7899f3e634d07b0ba09a017a8a18e","version_major":2,"version_minor":0},"text/plain":["Validation:   0%|          | 0/1264 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Best accuracy improved from 0.7230906212900673 to 0.723189552829442. Saving the model.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ab241d4ee12442659a771ff4d28137d6","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/1024 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7d00c29796c64e9da47f37754d81df8a","version_major":2,"version_minor":0},"text/plain":["Validation:   0%|          | 0/1264 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ea6728566d2244bf8d8f3272193ce05d","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/1024 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b8a0aab80b1a452195fef50576501bf8","version_major":2,"version_minor":0},"text/plain":["Validation:   0%|          | 0/1264 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"605f7e95bdef4496817fa7a37776cbb9","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/1024 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"769a700724824707afee65488d9fd801","version_major":2,"version_minor":0},"text/plain":["Validation:   0%|          | 0/1264 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4244c190a8dd4ea9a3162afddc4ea83b","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/1024 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3ed0c68f8836461aa23aafb3fe91bc58","version_major":2,"version_minor":0},"text/plain":["Validation:   0%|          | 0/1264 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4ea911ca229f41758d72acfdee3c4cd3","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/1024 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b0736ecf1fe44f85b5bc564b3621443e","version_major":2,"version_minor":0},"text/plain":["Validation:   0%|          | 0/1264 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1d61500498b94caa8173d5800b78783d","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/1024 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1d496d2a87ea42639d05491889f63883","version_major":2,"version_minor":0},"text/plain":["Validation:   0%|          | 0/1264 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"85c35219b3264626933bc00c7b7c57a7","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/1024 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"66c44ee70e1043f294f0bdcd0792924b","version_major":2,"version_minor":0},"text/plain":["Validation:   0%|          | 0/1264 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b6034b04d2dc47ca8518671cacd8b0a9","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/1024 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bd11e422d4444f6f9ed4f6d8c65bdd41","version_major":2,"version_minor":0},"text/plain":["Validation:   0%|          | 0/1264 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31670/1920417193.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m             ),\n\u001b[1;32m     23\u001b[0m         ]) as logger_list:\n\u001b[0;32m---> 24\u001b[0;31m     main(\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mtrain_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mvalid_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31670/3651525585.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(train_df, valid_df, num_classes, from_checkpoint, to_checkpoint, error_artifact_dir_path, logger_list, num_epochs, batch_size, max_len, num_workers, device, output_logits, lr, t_0, eta_min, train_margin_list, train_decision_margin, valid_margin_list, valid_decision_margin, accumulate_gradient_steps, validate_every_n_steps)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0maccumulate_gradient_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccumulate_gradient_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             ranking_loss_part=0.5)\n\u001b[0;32m---> 83\u001b[0;31m         accuracy, valid_metrics_to_track = do_valid_iteration(\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0mdata_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_data_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31670/3181833668.py\u001b[0m in \u001b[0;36mdo_valid_iteration\u001b[0;34m(data_loader, model, device, margin_list, decision_margin)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mscore_more\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_more\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslice_list_more\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mscore_less\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_less\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslice_list_less\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mscore_more_cpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore_more\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0mvalid_score_mean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_more_cpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mvalid_score_std\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_more_cpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# if IS_KAGGLE:\n","#     from kaggle_secrets import UserSecretsClient\n","#     user_secrets = UserSecretsClient()\n","#     wandb_api_key = user_secrets.get_secret('wandb-api-token')\n","# else:\n","#     with open(os.path.join(os.path.dirname(os.getcwd()), 'deploy/secrets/wandb_api_token.txt')) as f:\n","#         wandb_api_key = f.read()\n","\n","error_artifact_dir_path = ARTIFACT_DIR_PATH / MODEL_NAME\n","error_artifact_dir_path.mkdir(exist_ok=True)\n","with ContextManagerList([\n","            # StdOutLogger(),\n","            TensorBoardLogger(\n","                log_dir=str(TENSORBOARD_DIR_PATH / f'jt-{MODEL_NAME}'),\n","                metric_whitelist={\n","                    'train_loss',\n","                    'train_accuracy_0.0',\n","                    'train_accuracy_0.5',\n","                    'valid_accuracy_0.0',\n","                    'valid_accuracy_0.5',\n","                },\n","            ),\n","        ]) as logger_list:\n","    main(\n","        train_df=train_df,\n","        valid_df=valid_df,\n","        num_classes=6,\n","        from_checkpoint='roberta-base',\n","        to_checkpoint=str(MODELS_DIR_PATH / f'{MODEL_NAME}.pt'),\n","        error_artifact_dir_path=error_artifact_dir_path,\n","        logger_list=logger_list,\n","        num_epochs=100,\n","        batch_size=4,\n","        max_len=256,\n","        num_workers=8,\n","        device='cuda',\n","        output_logits=768,\n","        lr=1e-4,\n","        t_0=100,\n","        eta_min=1e-6,\n","        train_margin_list=[0.0, 0.5, 1.0],\n","        train_decision_margin=1.0,\n","        valid_margin_list=[0.0, 0.5, 1.0],\n","        valid_decision_margin=0.0,\n","        accumulate_gradient_steps=16,\n","        validate_every_n_steps=4096,\n","    )"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":4}
