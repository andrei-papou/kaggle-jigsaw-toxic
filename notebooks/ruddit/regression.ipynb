{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import typing as t\n",
    "from pathlib import Path\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import typing_extensions as t_ext\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from scipy import sparse\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_VALIDATION = True\n",
    "RUN_SUBMISSION = True\n",
    "\n",
    "RUDDIT_DIR = Path('/home/jovyan/jigsaw-toxic/data/datasets/ruddit')\n",
    "JIGSAW_DIR = Path('/home/jovyan/jigsaw-toxic/data/jigsaw-toxic-severity-rating')\n",
    "TRAIN_CSV_PATH = RUDDIT_DIR / 'train_no_quote.csv' \n",
    "VALID_CSV_PATH = RUDDIT_DIR / 'valid.csv'\n",
    "INFER_CSV_PATH = JIGSAW_DIR / 'comments_to_score.csv'\n",
    "SUBMISSION_CSV_PATH = Path('submission.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = t.cast(t.Callable[[str], t.List[str]], nltk.tokenize.word_tokenize)\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_y(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df['y'] = df['score']\n",
    "    df['y'] /= df['y'].max()  # type: ignore\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample(df: pd.DataFrame):\n",
    "    return df.sample(frac=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_url(text: str) -> str:\n",
    "    \"\"\" Replaces url address with \"url\" \"\"\"\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', 'url', text)\n",
    "    text = re.sub(r'#([^\\s]+)', r'\\1', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_abbrev(text: str) -> str:\n",
    "    text = re.sub(r\"what's\", \"what is \", text)    \n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_unicode(text: str) -> str:\n",
    "    \"\"\" Removes unicode strings like \"\\u002c\" and \"x96\" \"\"\"\n",
    "    text = re.sub(r'(\\\\u[0-9A-Fa-f]+)', r' ', text)       \n",
    "    text = re.sub(r'[^\\x00-\\x7f]', r' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_repeat_pattern(text: str) -> str:\n",
    "    text=re.sub(r'([a-zA-Z])\\1{2,}\\b', r'\\1\\1', text)\n",
    "    text=re.sub(r'([a-zA-Z])\\1\\1{2,}\\B', r'\\1\\1\\1', text)\n",
    "    text=re.sub(r'[ ]{2,}', ' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_at_user(text: str) -> str:\n",
    "    \"\"\" Replaces \"@user\" with \"atUser\" \"\"\"\n",
    "    text = re.sub('@[^\\s]+','atUser',text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_multi_toxic_words(text: str) -> str:\n",
    "    text = re.sub(r'(fuckfuck)','fuck fuck ',text)\n",
    "    text = re.sub(r'(f+)( *)([u|*]+)( *)([c|*]+)( *)(k)+','fuck',text)\n",
    "    text = re.sub(r'(haha)','ha ha ',text)\n",
    "    text = re.sub(r'(s+ *h+ *i+ *t+)','shit',text)\n",
    "    text = re.sub(r'([a|@][$|s][s|$])','ass',text)\n",
    "    text = re.sub(r'(\\bfuk\\b)','fuck',text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_numbers(text: str) -> str:\n",
    "    \"\"\" Removes integers \"\"\"\n",
    "    text = re.sub(r\"(^|\\W)\\d+\", \" \", text)    \n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_multi_punc(text: str) -> str:\n",
    "    text = re.sub(r'([!?\\'])\\1+', r' \\1\\1 ', text)\n",
    "    text = re.sub(r'([!?\\'])', r' \\1 ', text)\n",
    "    text = re.sub(r'([*_:])\\1+', r'\\1', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "class Lemmatizer(t_ext.Protocol):\n",
    "\n",
    "    def lemmatize(self, word: str, pos: str = \"n\") -> str:\n",
    "        ...\n",
    "\n",
    "\n",
    "class ReplaceTokenCleaner:\n",
    "\n",
    "    def __init__(self, token_set: t.Set[str], replace_with: str):\n",
    "        self._token_set = token_set\n",
    "        self._replace_with = replace_with\n",
    "\n",
    "    def __call__(self, text: str) -> str:\n",
    "        for token in self._token_set:\n",
    "            text = text.replace(token, self._replace_with)\n",
    "        return text\n",
    "\n",
    "\n",
    "class RemoveStopWordsCleaner:\n",
    "\n",
    "    def __init__(self, tokenizer: t.Callable[[str], t.List[str]], stop_words: t.Optional[t.List[str]] = None):\n",
    "        self._tokenizer = tokenizer\n",
    "        self._stop_words = stop_words if stop_words is not None else stopwords.words('english')\n",
    "\n",
    "    def __call__(self, text: str) -> str:\n",
    "        return ' '.join([token for token in self._tokenizer(text) if token not in self._stop_words])\n",
    "\n",
    "\n",
    "class LemmatizeCleaner:\n",
    "\n",
    "    def __init__(self, tokenizer: t.Callable[[str], t.List[str]], lemmatizer: Lemmatizer):\n",
    "        self._tokenizer = tokenizer\n",
    "        self._lemmatizer = lemmatizer\n",
    "\n",
    "    def __call__(self, text: str) -> str:\n",
    "        return ' '.join([self._lemmatizer.lemmatize(token) for token in self._tokenizer(text)])\n",
    "\n",
    "\n",
    "class TextCleanerList:\n",
    "\n",
    "    def __init__(self, cleaner_list: t.List[t.Callable[[str], str]]):\n",
    "        self._cleaner_list = cleaner_list\n",
    "\n",
    "    def __call__(self, text: str) -> str:\n",
    "        for cleaner in self._cleaner_list:\n",
    "            text = cleaner(text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _HandCraftedFeature:\n",
    "    name: str\n",
    "\n",
    "    def __call__(self, text: str) -> float:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class _TokenBasedHandCraftedFeature(_HandCraftedFeature):\n",
    "\n",
    "    def __init__(self, tokenizer: t.Callable[[str], t.List[str]]):\n",
    "        self._tokenizer = tokenizer\n",
    "\n",
    "    def _tokenize(self, text: str) -> t.List[str]:\n",
    "        return self._tokenizer(text)\n",
    "\n",
    "\n",
    "class CharLenFeature(_HandCraftedFeature):\n",
    "    name: str = 'char_len'\n",
    "\n",
    "    def __call__(self, text: str) -> float:\n",
    "        return len(text)\n",
    "\n",
    "\n",
    "class TokenLenFeature(_TokenBasedHandCraftedFeature):\n",
    "    name: str = 'token_len'\n",
    "\n",
    "    def __call__(self, text: str) -> float:\n",
    "        return len(self._tokenize(text))\n",
    "\n",
    "\n",
    "class AvgTokenLenFeature(_TokenBasedHandCraftedFeature):\n",
    "    name: str = 'avg_token_len'\n",
    "    \n",
    "    def __call__(self, text: str) -> float:\n",
    "        val = np.mean([len(token) for token in self._tokenize(text)])\n",
    "        return val if np.isfinite(val) else 0\n",
    "\n",
    "\n",
    "class NumStopWordsFeature(_TokenBasedHandCraftedFeature):\n",
    "    name: str = 'num_stop_words'\n",
    "\n",
    "    def __init__(self, tokenizer: t.Callable[[str], t.List[str]], stop_words: t.Set[str]):\n",
    "        super().__init__(tokenizer)\n",
    "        self._stop_words = stop_words\n",
    "\n",
    "    def __call__(self, text: str) -> float:\n",
    "        return len([token for token in self._tokenize(text) if token.lower() in self._stop_words])\n",
    "\n",
    "\n",
    "class NumWebsiteLinksFeature(_HandCraftedFeature):\n",
    "    name: str = 'num_website_links'\n",
    "    _RE_WEBSITE_LINK = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "\n",
    "    def __call__(self, text: str) -> float:\n",
    "        return len(self._RE_WEBSITE_LINK.findall(text))\n",
    "\n",
    "\n",
    "class NumEmojiFeature(_HandCraftedFeature):\n",
    "    name: str = 'num_emoji'\n",
    "    _RE_EMOJI = re.compile('['\n",
    "        u'\\U0001F600-\\U0001F64F'  # emoticons\n",
    "        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n",
    "        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n",
    "        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n",
    "        u'\\U00002702-\\U000027B0'\n",
    "        u'\\U000024C2-\\U0001F251'\n",
    "        ']+', flags=re.UNICODE)\n",
    "\n",
    "    def __call__(self, text: str) -> float:\n",
    "        return len(self._RE_EMOJI.findall(text))\n",
    "\n",
    "\n",
    "class NumSpecialCharsFeature(_HandCraftedFeature):\n",
    "    name: str = 'num_special_chars'\n",
    "    _RE_SPECIAL_CHARS = re.compile(r'[^a-zA-Z\\d]')\n",
    "\n",
    "    def __call__(self, text: str) -> float:\n",
    "        return len(self._RE_SPECIAL_CHARS.findall(text))\n",
    "\n",
    "\n",
    "class NumExtraSpacesFeature(_HandCraftedFeature):\n",
    "    name: str = 'num_extra_spaces'\n",
    "    _RE_EXTRA_SPACES = re.compile(r' +')\n",
    "\n",
    "    def __call__(self, text: str) -> float:\n",
    "        return len(self._RE_EXTRA_SPACES.findall(text))\n",
    "\n",
    "\n",
    "class UpperCaseCharRatioFeature(_HandCraftedFeature):\n",
    "    name: str = 'upper_case_char_ratio'\n",
    "\n",
    "    def __call__(self, text: str) -> float:\n",
    "        return len([c for c in str(text) if c.isupper()]) / len(text)\n",
    "\n",
    "\n",
    "class LowerCaseCharRatioFeature(_HandCraftedFeature):\n",
    "    name: str = 'lower_case_char_ratio'\n",
    "\n",
    "    def __call__(self, text: str) -> float:\n",
    "        return len([c for c in str(text) if c.islower()]) / len(text)\n",
    "\n",
    "\n",
    "class UpperCaseTokenRatioFeature(_TokenBasedHandCraftedFeature):\n",
    "    name: str = 'upper_case_token_ratio'\n",
    "\n",
    "    def __call__(self, text: str) -> float:\n",
    "        token_list = self._tokenize(text)\n",
    "        if not token_list:\n",
    "            return 0\n",
    "        return len([token for token in token_list if token.isupper()]) / len(token_list)\n",
    "\n",
    "\n",
    "class LowerCaseTokenRatioFeature(_TokenBasedHandCraftedFeature):\n",
    "    name: str = 'lower_case_token_ratio'\n",
    "\n",
    "    def __call__(self, text: str) -> float:\n",
    "        token_list = self._tokenize(text)\n",
    "        if not token_list:\n",
    "            return 0\n",
    "        return len([token for token in token_list if token.islower()]) / len(token_list)\n",
    "\n",
    "\n",
    "class HandCraftedFeatureList:\n",
    "\n",
    "    def __init__(self, feature_list: t.List[_HandCraftedFeature]):\n",
    "        self._feature_list = feature_list\n",
    "\n",
    "    def __call__(self, text: str) -> np.ndarray:\n",
    "        feature_val_list = []\n",
    "        for feature in self._feature_list:\n",
    "            feature_val = feature(text)\n",
    "            if not np.isfinite(feature_val):\n",
    "                raise ValueError(f'feature: {feature}, val: {feature_val}, text: {text}')\n",
    "            feature_val_list.append(feature_val)\n",
    "        return np.array(feature_val_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_features_to_sparse(array_list: t.List[t.Union[sparse.spmatrix, np.ndarray]]) -> sparse.spmatrix:\n",
    "    assert len(array_list) > 0\n",
    "    sparse_array_list = []\n",
    "    for array in array_list:\n",
    "        if isinstance(array, np.ndarray):\n",
    "            array = sparse.csr_matrix(array)\n",
    "        sparse_array_list.append(array)\n",
    "    return sparse.hstack(sparse_array_list) if len(sparse_array_list) > 1 else sparse_array_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "_F = t.TypeVar('_F')\n",
    "\n",
    "class _FeatureGenerator(t.Generic[_F]):\n",
    "\n",
    "    def __call__(self, text_list: t.List[str]) -> _F:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class TfidfFeatureGenerator(_FeatureGenerator[sparse.spmatrix]):\n",
    "\n",
    "    def __init__(self, vectorizer: TfidfVectorizer) -> None:\n",
    "        check_is_fitted(vectorizer)\n",
    "        self._vectorizer = vectorizer\n",
    "\n",
    "    def __call__(self, text_list: t.List[str]) -> sparse.spmatrix:\n",
    "        return self._vectorizer.transform(text_list)\n",
    "\n",
    "\n",
    "class HandCraftedFeatureGenerator(_FeatureGenerator[np.ndarray]):\n",
    "\n",
    "    def __init__(self, feature_list: HandCraftedFeatureList, show_progress: bool = False):\n",
    "        self._feature_list = feature_list\n",
    "        self._show_progress = show_progress\n",
    "\n",
    "    def __call__(self, text_list: t.List[str]) -> np.ndarray:\n",
    "        return np.stack([self._feature_list(text) for text in (tqdm(text_list) if self._show_progress else text_list)], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cleaner = TextCleanerList([\n",
    "    lambda text: text.lower(),\n",
    "    clean_url,\n",
    "    clean_unicode,\n",
    "    clean_numbers,\n",
    "    clean_abbrev,\n",
    "    clean_multi_toxic_words,\n",
    "    clean_multi_punc,\n",
    "    clean_repeat_pattern,\n",
    "    ReplaceTokenCleaner(\n",
    "        token_set=set('\"%&\\'()+,-./:;<=>@[\\\\]^_`{|}~'),\n",
    "        replace_with=' '),\n",
    "    LemmatizeCleaner(\n",
    "        tokenizer=tokenizer,\n",
    "        lemmatizer=WordNetLemmatizer()),\n",
    "    RemoveStopWordsCleaner(tokenizer),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5835b23309f046468baf2ec95452bbfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5722 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b074af954b64f459b0c74e3c0f835b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5722 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df = pd.read_csv(TRAIN_CSV_PATH)\n",
    "train_df = build_y(train_df)\n",
    "train_df = subsample(train_df)\n",
    "\n",
    "train_text_list = [str(row['comment_text']) for _, row in tqdm(train_df.iterrows(), total=len(train_df))]\n",
    "train_cleaned_text_list = [text_cleaner(text) for text in tqdm(train_text_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>score</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1600</th>\n",
       "      <td>I'd feel confused why my girlfriend has her re...</td>\n",
       "      <td>0.208244</td>\n",
       "      <td>0.208244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1121</th>\n",
       "      <td>SNRIs don’t make you feel high. I’ve been on t...</td>\n",
       "      <td>0.277837</td>\n",
       "      <td>0.277837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4328</th>\n",
       "      <td>At what point is it legal to shoot them on sig...</td>\n",
       "      <td>0.708779</td>\n",
       "      <td>0.708779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3827</th>\n",
       "      <td>The Youth Risk Survey provided by the CDC. Not...</td>\n",
       "      <td>0.263919</td>\n",
       "      <td>0.263919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2131</th>\n",
       "      <td>Oh, *I* was killing them, I promise.</td>\n",
       "      <td>0.520343</td>\n",
       "      <td>0.520343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3772</th>\n",
       "      <td>You literally just hand the condo to a propert...</td>\n",
       "      <td>0.364561</td>\n",
       "      <td>0.364561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5191</th>\n",
       "      <td>Makes me sad. They have an awesome culture asi...</td>\n",
       "      <td>0.420236</td>\n",
       "      <td>0.420236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5226</th>\n",
       "      <td>[removed]</td>\n",
       "      <td>0.832976</td>\n",
       "      <td>0.832976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5390</th>\n",
       "      <td>That's why there we no protests over Net Neutr...</td>\n",
       "      <td>0.509101</td>\n",
       "      <td>0.509101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>It is one way to financial success, though it ...</td>\n",
       "      <td>0.308887</td>\n",
       "      <td>0.308887</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5722 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           comment_text     score         y\n",
       "1600  I'd feel confused why my girlfriend has her re...  0.208244  0.208244\n",
       "1121  SNRIs don’t make you feel high. I’ve been on t...  0.277837  0.277837\n",
       "4328  At what point is it legal to shoot them on sig...  0.708779  0.708779\n",
       "3827  The Youth Risk Survey provided by the CDC. Not...  0.263919  0.263919\n",
       "2131              Oh, *I* was killing them, I promise.   0.520343  0.520343\n",
       "...                                                 ...       ...       ...\n",
       "3772  You literally just hand the condo to a propert...  0.364561  0.364561\n",
       "5191  Makes me sad. They have an awesome culture asi...  0.420236  0.420236\n",
       "5226                                          [removed]  0.832976  0.832976\n",
       "5390  That's why there we no protests over Net Neutr...  0.509101  0.509101\n",
       "860   It is one way to financial success, though it ...  0.308887  0.308887\n",
       "\n",
       "[5722 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_feature_generator = TfidfFeatureGenerator(\n",
    "    vectorizer=TfidfVectorizer(min_df=3, max_df=0.5, analyzer='char_wb', ngram_range=(3, 5)).fit(train_cleaned_text_list))\n",
    "hand_crafted_feature_generator = HandCraftedFeatureGenerator(\n",
    "    feature_list=HandCraftedFeatureList([\n",
    "        CharLenFeature(),\n",
    "        TokenLenFeature(tokenizer=tokenizer),\n",
    "        AvgTokenLenFeature(tokenizer=tokenizer),\n",
    "        NumStopWordsFeature(tokenizer=tokenizer, stop_words=stop_words),\n",
    "        NumWebsiteLinksFeature(),\n",
    "        NumEmojiFeature(),\n",
    "        NumSpecialCharsFeature(),\n",
    "        NumExtraSpacesFeature(),\n",
    "        UpperCaseCharRatioFeature(),\n",
    "        LowerCaseCharRatioFeature(),\n",
    "        UpperCaseTokenRatioFeature(tokenizer=tokenizer),\n",
    "        LowerCaseTokenRatioFeature(tokenizer=tokenizer),\n",
    "    ]),\n",
    "    show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "968db450cd4844edb3c58614be7119dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5722 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "# x = tfidf_feature_generator(train_cleaned_text_list)\n",
    "x = join_features_to_sparse([\n",
    "    tfidf_feature_generator(train_cleaned_text_list),\n",
    "    hand_crafted_feature_generator(train_text_list),\n",
    "])\n",
    "y = train_df['y'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=0.5)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Ridge(alpha=0.5)\n",
    "model.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3678df9e67e341078577439796238d86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a58dbff558dd485f92fe8dff8beebaae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e62d7efaeb094f87b78ba48ad7f0d3af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c957eacdc9947b3bbb057b25b2f0d00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.6616541353383458\n"
     ]
    }
   ],
   "source": [
    "if RUN_VALIDATION:\n",
    "    valid_df = pd.read_csv(VALID_CSV_PATH)\n",
    "\n",
    "    valid_more_text_list = [str(row['more_toxic']) for _, row in tqdm(valid_df.iterrows(), total=len(valid_df))]\n",
    "    valid_less_text_list = [str(row['less_toxic']) for _, row in tqdm(valid_df.iterrows(), total=len(valid_df))]\n",
    "    valid_cleaned_more_text_list = [text_cleaner(text) for text in valid_more_text_list]\n",
    "    valid_cleaned_less_text_list = [text_cleaner(text) for text in valid_less_text_list]\n",
    "\n",
    "    less_toxic_score_array = model.predict(join_features_to_sparse([\n",
    "        tfidf_feature_generator(valid_cleaned_less_text_list),\n",
    "        hand_crafted_feature_generator(valid_less_text_list),\n",
    "    ]))\n",
    "    more_toxic_score_array = model.predict(join_features_to_sparse([\n",
    "        tfidf_feature_generator(valid_cleaned_more_text_list),\n",
    "        hand_crafted_feature_generator(valid_more_text_list),\n",
    "    ]))\n",
    "    print(f'Validation accuracy: {np.mean(less_toxic_score_array < more_toxic_score_array)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "028866b177a34f5faa4ad4b8a33aa666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7537 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8390ee3da4bf4d6b84ded8a3a7348588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7537 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if RUN_SUBMISSION:\n",
    "    infer_df = pd.read_csv(INFER_CSV_PATH)\n",
    "\n",
    "    comment_id_list, infer_text_list, infer_cleaned_text_list = [], [], []\n",
    "    for _, row in tqdm(infer_df.iterrows(), total=len(infer_df)):\n",
    "        comment_id, text = str(row['comment_id']), str(row['text'])\n",
    "        comment_id_list.append(comment_id)\n",
    "        infer_text_list.append(text)\n",
    "        infer_cleaned_text_list.append(text_cleaner(text))\n",
    "\n",
    "    score_array = model.predict(join_features_to_sparse([\n",
    "        tfidf_feature_generator(infer_cleaned_text_list),\n",
    "        hand_crafted_feature_generator(infer_text_list),\n",
    "    ]))\n",
    "\n",
    "    pd.DataFrame([\n",
    "        {'comment_id': comment_id, 'score': score}\n",
    "        for comment_id, score in zip(comment_id_list, score_array.tolist())\n",
    "    ]).to_csv(SUBMISSION_CSV_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comment_id,score\n",
      "114890,0.5802728622557065\n",
      "732895,0.31857674205932207\n",
      "1139051,0.49695251339636964\n",
      "1434512,0.547778426990235\n",
      "2084821,0.5255721309939012\n",
      "2452675,0.5670362356261693\n",
      "3206615,0.7252069317415191\n",
      "3665348,0.46170659695861826\n",
      "4502494,0.43391978535830006\n",
      "4804786,0.39014954410299346\n"
     ]
    }
   ],
   "source": [
    "!head -n 11 $SUBMISSION_CSV_PATH"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
