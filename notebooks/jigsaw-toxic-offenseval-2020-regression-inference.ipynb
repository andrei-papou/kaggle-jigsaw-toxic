{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "import typing as t\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import typing_extensions as t_ext\n",
    "from bs4 import BeautifulSoup\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers.models.auto.modeling_auto import AutoModel\n",
    "from transformers.models.auto.tokenization_auto import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCleaner:\n",
    "    _RE_WEBSITE_LINK = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    _RE_EMOJI = re.compile('['\n",
    "        u'\\U0001F600-\\U0001F64F'  # emoticons\n",
    "        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n",
    "        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n",
    "        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n",
    "        u'\\U00002702-\\U000027B0'\n",
    "        u'\\U000024C2-\\U0001F251'\n",
    "        ']+', flags=re.UNICODE)\n",
    "    _RE_SPECIAL_CHARACTERS = re.compile(r'[^a-zA-Z\\d]')\n",
    "    _RE_EXTRA_SPACES = re.compile(r' +')\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def clean(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Cleans text into a basic form for NLP. Operations include the following:-\n",
    "        1. Remove special charecters like &, #, etc\n",
    "        2. Removes extra spaces\n",
    "        3. Removes embedded URL links\n",
    "        4. Removes HTML tags\n",
    "        5. Removes emojis\n",
    "        \n",
    "        text - Text piece to be cleaned.\n",
    "        \"\"\"\n",
    "        text = self._RE_WEBSITE_LINK.sub(r'', text)\n",
    "        \n",
    "        soup = BeautifulSoup(text, 'lxml')  # Removes HTML tags\n",
    "        only_text = soup.get_text()\n",
    "        text = only_text\n",
    "\n",
    "        text = self._RE_EMOJI.sub(r'', text)\n",
    "        \n",
    "        text = self._RE_SPECIAL_CHARACTERS.sub(\" \", text)  # Remove special Charecters\n",
    "        text = self._RE_EXTRA_SPACES.sub(' ', text)  # Remove Extra Spaces\n",
    "        text = text.strip()  # Remove spaces at the beginning and at the end of string\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _TokenizedText(t_ext.TypedDict):\n",
    "    input_ids: torch.Tensor\n",
    "    attention_mask: torch.Tensor\n",
    "    token_type_ids: torch.Tensor\n",
    "\n",
    "\n",
    "def _preprocess_tokenizer_output(output: t.Dict[str, t.Any]) -> _TokenizedText:\n",
    "    return {\n",
    "        'input_ids': torch.tensor(output['input_ids']),\n",
    "        'attention_mask': torch.tensor(output['attention_mask']),\n",
    "        'token_type_ids': torch.tensor(output['token_type_ids']),\n",
    "    }\n",
    "\n",
    "\n",
    "class InferenceDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame, tokenizer: AutoTokenizer, max_len: int) -> None:\n",
    "        super().__init__()\n",
    "        self._df = df\n",
    "        self._tokenizer = tokenizer\n",
    "        self._max_len = max_len\n",
    "        self._cleaner = TextCleaner()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._df)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> _TokenizedText:\n",
    "        record = self._df.iloc[idx]\n",
    "        tokenized_text = _preprocess_tokenizer_output(self._tokenizer(\n",
    "            self._cleaner.clean(str(record['text'])),\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self._max_len,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=True))  # type: ignore\n",
    "        return tokenized_text\n",
    "\n",
    "    def get_comment_id_iter(self) -> t.Iterable[str]:\n",
    "        return [str(row['comment_id']) for _, row in self._df.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, checkpoint: str, output_logits: int, dropout: float):\n",
    "        super(Model, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(checkpoint, return_dict=False)\n",
    "        self.layer_norm = torch.nn.LayerNorm(output_logits)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.dense = torch.nn.Sequential(\n",
    "            torch.nn.Linear(output_logits, 128),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, token_type_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        _, pooled_output = self.bert(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
    "        pooled_output = self.layer_norm(pooled_output)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        preds = self.dense(pooled_output)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "        df: pd.DataFrame,\n",
    "        dst_path: str,\n",
    "        transformer_dir_path: str,\n",
    "        model_checkpoint: str,\n",
    "        output_logits: int,\n",
    "        dropout: float,\n",
    "        max_len: int,\n",
    "        batch_size: int,\n",
    "        num_workers: int,\n",
    "        device: str):\n",
    "    model = Model(checkpoint=transformer_dir_path, output_logits=output_logits, dropout=dropout)\n",
    "    model.load_state_dict(torch.load(model_checkpoint, map_location=device))\n",
    "    model.eval()\n",
    "    dataset = InferenceDataset(df=df, tokenizer=AutoTokenizer.from_pretrained(transformer_dir_path), max_len=max_len)\n",
    "    data_loader = DataLoader(dataset=dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False, pin_memory=device.startswith('cuda'))\n",
    "    score_list = []\n",
    "    with torch.no_grad():\n",
    "        for tokenized_text in data_loader:\n",
    "            score_tensor = model(\n",
    "                tokenized_text['input_ids'].to(device),\n",
    "                tokenized_text['token_type_ids'].to(device),\n",
    "                tokenized_text['attention_mask'].to(device),)\n",
    "            score_list.extend(score_tensor.flatten().tolist())\n",
    "    pd.DataFrame([\n",
    "        {'comment_id': comment_id, 'score': score}\n",
    "        for comment_id, score in zip(dataset.get_comment_id_iter(), score_list)\n",
    "    ]).to_csv(dst_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_CSV_PATH = '/kaggle/input/jigsaw-toxic-severity-rating/comments_to_score.csv'\n",
    "OUTPUT_CSV_PATH = '/kaggle/working/submission.csv'\n",
    "MODEL_PATH = '/kaggle/input/jigsaw-toxic-offenseval-2020-regression-output/models/offenseval-2020-regression.pt'\n",
    "TRANSFORMER_DIR_PATH = '/kaggle/input/roberta-base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(\n",
    "    df=pd.read_csv(INPUT_CSV_PATH),\n",
    "    dst_path=OUTPUT_CSV_PATH,\n",
    "    transformer_dir_path=TRANSFORMER_DIR_PATH,\n",
    "    model_checkpoint=MODEL_PATH,\n",
    "    output_logits=768,\n",
    "    dropout=0.2,\n",
    "    max_len=256,\n",
    "    batch_size=64,\n",
    "    num_workers=2,\n",
    "    device='cpu')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
