{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import typing as t\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import typing_extensions as t_ext\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers.models.auto.modeling_auto import AutoModel\n",
    "from transformers.models.auto.tokenization_auto import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _TokenizedText(t_ext.TypedDict):\n",
    "    input_ids: torch.Tensor\n",
    "    attention_mask: torch.Tensor\n",
    "\n",
    "\n",
    "def _preprocess_tokenizer_output(output: t.Dict[str, t.Any]) -> _TokenizedText:\n",
    "    return {\n",
    "        'input_ids': torch.tensor(output['input_ids']),\n",
    "        'attention_mask': torch.tensor(output['attention_mask']),\n",
    "    }\n",
    "\n",
    "\n",
    "def _split_str_to_chunk_list(s: str, chunk_size: int) -> t.List[str]:\n",
    "    chunk_list = []\n",
    "    chunk = []\n",
    "    for token in s.split(' '):\n",
    "        chunk.append(token)\n",
    "        if len(chunk) >= chunk_size:\n",
    "            chunk_list.append(' '.join(chunk))\n",
    "            chunk.clear()\n",
    "    if chunk:\n",
    "        chunk_list.append(' '.join(chunk))\n",
    "    return chunk_list\n",
    "\n",
    "\n",
    "def predict_collate_fn(\n",
    "        sample_list: t.List[t.Tuple[str, _TokenizedText]]\n",
    "        ) -> t.Tuple[t.List[str], _TokenizedText, t.List[slice]]:\n",
    "    curr_pos = 0\n",
    "\n",
    "    idx_list: t.List[str] = []\n",
    "    input_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    slice_list: t.List[slice] = []\n",
    "    \n",
    "    for sample in sample_list:\n",
    "        idx_list.append(sample[0])\n",
    "        input_ids, attention_mask = sample[1]['input_ids'], sample[1]['attention_mask']\n",
    "        input_ids_list.append(input_ids)\n",
    "        attention_mask_list.append(attention_mask)\n",
    "        slice_list.append(slice(curr_pos, curr_pos + input_ids.shape[0]))\n",
    "        curr_pos += input_ids.shape[0]\n",
    "\n",
    "    tokenized_collated: _TokenizedText = {\n",
    "        'input_ids': torch.cat(input_ids_list, dim=0),\n",
    "        'attention_mask': torch.cat(attention_mask_list, dim=0),\n",
    "    }\n",
    "\n",
    "    return idx_list, tokenized_collated, slice_list\n",
    "\n",
    "\n",
    "class PredictDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame, tokenizer: AutoTokenizer, max_len: int) -> None:\n",
    "        super().__init__()\n",
    "        self._df = df\n",
    "        self._tokenizer = tokenizer\n",
    "        self._max_len = max_len\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._df)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> t.Tuple[str, _TokenizedText]:\n",
    "        record = self._df.iloc[idx]\n",
    "        comment_id, text = str(record['comment_id']), str(record['text'])\n",
    "\n",
    "        input_ids_list, attention_mask_list = [], []\n",
    "        for chunk in _split_str_to_chunk_list(text, chunk_size=self._max_len):\n",
    "            tokenized_chunk = _preprocess_tokenizer_output(self._tokenizer(\n",
    "                chunk,\n",
    "                add_special_tokens=True,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=self._max_len,\n",
    "                return_attention_mask=True))  # type: ignore\n",
    "            input_ids_list.append(tokenized_chunk['input_ids'])\n",
    "            attention_mask_list.append(tokenized_chunk['attention_mask'])\n",
    "\n",
    "        tokenized_text: _TokenizedText = {\n",
    "            'input_ids': torch.stack(input_ids_list, dim=0),\n",
    "            'attention_mask': torch.stack(attention_mask_list, dim=0),\n",
    "        }\n",
    "\n",
    "        return comment_id, tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, checkpoint: str, output_logits: int):\n",
    "        super(Model, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(checkpoint, return_dict=False)\n",
    "        self.regressor = torch.nn.Sequential(\n",
    "            torch.nn.Linear(output_logits, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 1),\n",
    "            torch.nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        _, pooled_output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask)\n",
    "        return self.regressor(pooled_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_prediction_iteration(\n",
    "        data_loader: DataLoader,\n",
    "        model: Model,\n",
    "        device: str) -> pd.DataFrame:\n",
    "    model.eval()\n",
    "    prediction_dict_list = []\n",
    "    with torch.no_grad():\n",
    "        for comment_id_list, tokenized_text, slice_list in tqdm(data_loader, desc='Prediction'):\n",
    "            scores_tensor = model(\n",
    "                tokenized_text['input_ids'].to(device),\n",
    "                tokenized_text['attention_mask'].to(device),)\n",
    "            scores_tensor = torch.cat([torch.max(scores_tensor[s], dim=0, keepdim=True)[0] for s in slice_list], dim=0)\n",
    "            for comment_id, score in zip(comment_id_list, scores_tensor.flatten().tolist()):\n",
    "                prediction_dict_list.append({\n",
    "                    'comment_id': comment_id,\n",
    "                    'score': score,\n",
    "                })\n",
    "    return pd.DataFrame(prediction_dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    input_csv_path: str,\n",
    "    output_csv_path: str,\n",
    "    batch_size: int,\n",
    "    model_checkpoint: str,\n",
    "    tokenizer_checkpoint: str,\n",
    "    max_len: int,\n",
    "    output_logits: int,\n",
    "    num_workers: int,\n",
    "    device: str,\n",
    "):\n",
    "    model = Model(checkpoint=tokenizer_checkpoint, output_logits=output_logits).to(device)\n",
    "    model.load_state_dict(torch.load(model_checkpoint, map_location=device))\n",
    "    in_df = pd.read_csv(input_csv_path)\n",
    "    dataset = PredictDataset(\n",
    "        df=in_df,\n",
    "        tokenizer=AutoTokenizer.from_pretrained(tokenizer_checkpoint),\n",
    "        max_len=max_len)\n",
    "    data_loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=predict_collate_fn,  # type: ignore\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=device.startswith('cuda'))\n",
    "    out_df = do_prediction_iteration(data_loader=data_loader, model=model, device=device)\n",
    "    out_df.to_csv(output_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_KAGGLE = False\n",
    "MAX_LEN = 256\n",
    "OUTPUT_LOGITS = 768\n",
    "\n",
    "INPUT_CSV_PATH = '/kaggle/input/jigsaw-toxic-severity-rating/comments_to_score.csv' if IS_KAGGLE \\\n",
    "    else '/home/jovyan/jigsaw-toxic/data/jigsaw-toxic-severity-rating/comments_to_score.csv'\n",
    "OUTPUT_CSV_PATH = '/kaggle/working/submission.csv' if IS_KAGGLE else '/home/jovyan/jigsaw-toxic/output/ruddit.csv'\n",
    "MODEL_PATH = '/kaggle/input/jt-models-unintended-bias-in-toxicity/margin-ranking-unintended-bias-in-toxicity-classification-v1-unbiased-toxic-roberta.pt' if IS_KAGGLE else \\\n",
    "    '/home/jovyan/jigsaw-toxic/models/margin-ranking-unintended-bias-in-toxicity-classification-v1-unbiased-toxic-roberta.pt'\n",
    "TOKENIZER_PATH = '/kaggle/input/unbiased-toxic-roberta/unbiased-toxic-roberta_update/unbiased-toxic-roberta' if IS_KAGGLE else 'unitary/unbiased-toxic-roberta'\n",
    "BATCH_SIZE = 16 if IS_KAGGLE else 8\n",
    "NUM_WORKERS = 2 if IS_KAGGLE else 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(\n",
    "    input_csv_path=INPUT_CSV_PATH,\n",
    "    output_csv_path=OUTPUT_CSV_PATH,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    model_checkpoint=MODEL_PATH,\n",
    "    tokenizer_checkpoint=TOKENIZER_PATH,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    output_logits=OUTPUT_LOGITS,\n",
    "    max_len=MAX_LEN,\n",
    "    device='cuda')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
