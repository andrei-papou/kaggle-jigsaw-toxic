{"cells":[{"cell_type":"markdown","metadata":{},"source":["### Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T21:30:42.290752Z","iopub.status.busy":"2021-12-13T21:30:42.290324Z","iopub.status.idle":"2021-12-13T21:30:44.813935Z","shell.execute_reply":"2021-12-13T21:30:44.813044Z","shell.execute_reply.started":"2021-12-13T21:30:42.290611Z"},"papermill":{"duration":7.63065,"end_time":"2021-12-13T15:32:53.613901","exception":false,"start_time":"2021-12-13T15:32:45.983251","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["from __future__ import annotations\n","\n","import numpy as np\n","import os\n","import random\n","import statistics\n","import shutil\n","import typing as t\n","from pathlib import Path\n","\n","import pandas as pd\n","import torch\n","import torch.nn.functional as torch_f\n","import typing_extensions as t_ext\n","import wandb\n","from torch.optim import Optimizer, AdamW\n","from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n","from torch.utils.data import Dataset, DataLoader, Sampler\n","from torch.utils.tensorboard.writer import SummaryWriter\n","from tqdm.notebook import tqdm\n","from transformers.models.auto.configuration_auto import AutoConfig\n","from transformers.models.auto.modeling_auto import AutoModel\n","from transformers.models.auto.tokenization_auto import AutoTokenizer\n","from transformers.optimization import get_cosine_schedule_with_warmup\n","from wandb.wandb_run import Run as WAndBRun"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def seed_everything(seed: int):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","\n","seed_everything(42)"]},{"cell_type":"markdown","metadata":{},"source":["### Datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T21:30:44.832059Z","iopub.status.busy":"2021-12-13T21:30:44.83171Z","iopub.status.idle":"2021-12-13T21:30:44.845976Z","shell.execute_reply":"2021-12-13T21:30:44.845075Z","shell.execute_reply.started":"2021-12-13T21:30:44.83202Z"},"papermill":{"duration":0.032712,"end_time":"2021-12-13T15:32:53.693077","exception":false,"start_time":"2021-12-13T15:32:53.660365","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["class _TokenizedText(t_ext.TypedDict):\n","    input_ids: torch.Tensor\n","    attention_mask: torch.Tensor\n","\n","\n","def _preprocess_tokenizer_output(output: t.Dict[str, t.Any]) -> _TokenizedText:\n","    return {\n","        'input_ids': torch.tensor(output['input_ids']),\n","        'attention_mask': torch.tensor(output['attention_mask']),\n","    }\n","\n","\n","class Tokenizer:\n","\n","    def tokenize(self, x: str) -> t.List[str]:\n","        return x.split(' ')\n","\n","    def invert_tokenize(self, x: t.List[str]) -> str:\n","        return ' '.join(x)\n","\n","\n","class RandomlyReduceTokenLenTo:\n","\n","    def __init__(self, token_len: int, tokenizer: t.Optional[Tokenizer] = None):\n","        self._token_len = token_len\n","        self._tokenizer = tokenizer if tokenizer is not None else Tokenizer()\n","\n","    def __call__(self, text: str) -> str:\n","        token_list = self._tokenizer.tokenize(text)\n","        if len(token_list) <= self._token_len:\n","            return text\n","        idx_set = set(random.choices(list(range(len(token_list))), k=self._token_len))\n","        return self._tokenizer.invert_tokenize([token for idx, token in enumerate(token_list) if idx in idx_set])\n","\n","\n","class RandomSubsetPerEpochSampler(Sampler[int]):\n","\n","    @staticmethod\n","    def _build_index(data_source: t.Sized) -> t.List[int]:\n","        index = list(range(len(data_source)))\n","        random.shuffle(index)\n","        return index\n","\n","    def __init__(self, data_source: t.Sized, samples_per_epoch: int):\n","        super().__init__(data_source)\n","        self._data_source = data_source\n","        self._samples_per_epoch = samples_per_epoch\n","        self._index: t.List[int] = self._build_index(data_source)\n","        self._real_epoch = 0\n","        self._step = -1\n","    \n","    def _sample_one(self) -> int:\n","        if not self._index:\n","            self._index = self._build_index(self._data_source)\n","            self._real_epoch += 1\n","        return self._index.pop()\n","\n","    def __iter__(self) -> t.Iterator[int]:\n","        self._step += 1\n","        return iter([self._sample_one() for _ in range(self._samples_per_epoch)])\n","\n","    def __len__(self) -> int:\n","        return self._samples_per_epoch\n","\n","    @property\n","    def step(self) -> int:\n","        return self._step\n","\n","    @property\n","    def real_epoch(self) -> int:\n","        return self._real_epoch\n","\n","    @property\n","    def frac_left(self) -> float:\n","        return len(self._index) / len(self._data_source)\n","\n","    @property\n","    def frac_consumed(self) -> float:\n","        return 1.0 - self.frac_left\n","\n","\n","class MetricBasedRandomSubsetPerEpochSampler(RandomSubsetPerEpochSampler):\n","\n","    def __init__(\n","            self,\n","            data_source: t.Sized,\n","            samples_per_epoch: int,\n","            thresholded_samples_per_epoch_list: t.List[t.Tuple[float, int]],):\n","        super().__init__(data_source, samples_per_epoch)\n","        self._samples_per_epoch_init = samples_per_epoch\n","        self._thresholded_samples_per_epoch_list = thresholded_samples_per_epoch_list\n","\n","    def adjust_samples_per_epoch(self, metric_val: float):\n","        self._samples_per_epoch = self._samples_per_epoch_init\n","        for threshold, samples_per_epoch in self._thresholded_samples_per_epoch_list:\n","            if metric_val >= threshold:\n","                self._samples_per_epoch = samples_per_epoch\n","\n","\n","class TrainDataset(Dataset):\n","\n","    def __init__(\n","            self,\n","            df: pd.DataFrame,\n","            tokenizer: AutoTokenizer,\n","            cls_list: t.List[str],\n","            max_len: int,\n","            score_weight_pow: int = 2,\n","            score_weight_eps: float = 1e-4,\n","            score_cls: str = 'target',\n","            augmentation_list: t.Optional[t.List[t.Callable[[str], str]]] = None):\n","        super().__init__()\n","        self._df_all = df\n","        self._df_non_zero_score = df[df[score_cls] > 0.0]\n","        self._cls_list = cls_list\n","        self._score_cls = score_cls\n","        self._tokenizer = tokenizer\n","        self._max_len = max_len\n","        self._score_weight_pow = score_weight_pow\n","        self._score_weight_eps = score_weight_eps\n","        self._use_score_as_weights = False\n","        self._augmentation_list = augmentation_list if augmentation_list is not None else []\n","\n","    def start_using_score_as_weights(self):\n","        self._use_score_as_weights = True\n","\n","    def __len__(self) -> int:\n","        return len(self._df_non_zero_score)\n","\n","    def _apply_augmentations(self, text: str) -> str:\n","        for augmentation in self._augmentation_list:\n","            text = augmentation(text)\n","        return text\n","\n","    def __getitem__(self, idx: int) -> t.Tuple[_TokenizedText, _TokenizedText, torch.Tensor, torch.Tensor, torch.Tensor]:\n","        more_record = self._df_non_zero_score.iloc[idx]\n","        less_population = self._df_all[self._df_all[self._score_cls] < more_record[self._score_cls]]\n","        if self._use_score_as_weights:\n","            less_record = less_population.sample(\n","                n=1,\n","                weights=np.power(less_population[self._score_cls].to_numpy(), self._score_weight_pow) + self._score_weight_eps).iloc[0]\n","        else:\n","            less_record = less_population.sample(n=1).iloc[0]\n","        more_comment_text = self._apply_augmentations(str(more_record['comment_text']))\n","        less_comment_text = self._apply_augmentations(str(less_record['comment_text']))\n","        tokenized_text_more = _preprocess_tokenizer_output(self._tokenizer(\n","            more_comment_text,\n","            add_special_tokens=True,\n","            truncation=True,\n","            padding='max_length',\n","            max_length=self._max_len,\n","            return_attention_mask=True))  # type: ignore\n","        tokenized_text_less = _preprocess_tokenizer_output(self._tokenizer(\n","            less_comment_text,\n","            add_special_tokens=True,\n","            truncation=True,\n","            padding='max_length',\n","            max_length=self._max_len,\n","            return_attention_mask=True))  # type: ignore\n","        features_more = torch.tensor([float(more_record[cls]) for cls in self._cls_list], dtype=torch.float32)\n","        features_less = torch.tensor([float(less_record[cls]) for cls in self._cls_list], dtype=torch.float32)\n","        return \\\n","            tokenized_text_more, \\\n","            tokenized_text_less, \\\n","            features_more, \\\n","            features_less, \\\n","            torch.tensor(float(more_record[self._score_cls]) - float(less_record[self._score_cls]))\n","\n","\n","def _split_str_to_chunk_list(s: str, chunk_size: int) -> t.List[str]:\n","    chunk_list = []\n","    chunk = []\n","    for token in s.split(' '):\n","        chunk.append(token)\n","        if len(chunk) >= chunk_size:\n","            chunk_list.append(' '.join(chunk))\n","            chunk.clear()\n","    if chunk:\n","        chunk_list.append(' '.join(chunk))\n","    return chunk_list\n","\n","\n","def valid_collate_fn(\n","        sample_list: t.List[t.Tuple[int, _TokenizedText, _TokenizedText]]\n","        ) -> t.Tuple[t.List[int], _TokenizedText, _TokenizedText, t.List[slice], t.List[slice]]:\n","    curr_pos_more, curr_pos_less = 0, 0\n","\n","    idx_list: t.List[int] = []\n","    more_input_ids_list, less_input_ids_list = [], []\n","    more_attention_mask_list, less_attention_mask_list = [], []\n","    more_slice_list: t.List[slice] = []\n","    less_slice_list: t.List[slice] = []\n","    \n","    for sample in sample_list:\n","        idx_list.append(sample[0])\n","        more_input_ids, more_attention_mask = sample[1]['input_ids'], sample[1]['attention_mask']\n","        less_input_ids, less_attention_mask = sample[2]['input_ids'], sample[2]['attention_mask']\n","        more_input_ids_list.append(more_input_ids)\n","        less_input_ids_list.append(less_input_ids)\n","        more_attention_mask_list.append(more_attention_mask)\n","        less_attention_mask_list.append(less_attention_mask)\n","        more_slice_list.append(slice(curr_pos_more, curr_pos_more + more_input_ids.shape[0]))\n","        curr_pos_more += more_input_ids.shape[0]\n","        less_slice_list.append(slice(curr_pos_less, curr_pos_less + less_input_ids.shape[0]))\n","        curr_pos_less += less_input_ids.shape[0]\n","\n","    more_tokenized_collated: _TokenizedText = {\n","        'input_ids': torch.cat(more_input_ids_list, dim=0),\n","        'attention_mask': torch.cat(more_attention_mask_list, dim=0),\n","    }\n","    less_tokenized_collated: _TokenizedText = {\n","        'input_ids': torch.cat(less_input_ids_list, dim=0),\n","        'attention_mask': torch.cat(less_attention_mask_list, dim=0),\n","    }\n","\n","    return idx_list, more_tokenized_collated, less_tokenized_collated, more_slice_list, less_slice_list\n","\n","\n","class ValidDataset(Dataset):\n","\n","    def __init__(self, df: pd.DataFrame, tokenizer: AutoTokenizer, max_len: int) -> None:\n","        super().__init__()\n","        self._df = df\n","        self._tokenizer = tokenizer\n","        self._max_len = max_len\n","        self._error: np.ndarray = np.zeros(len(df))\n","\n","    def __len__(self) -> int:\n","        return len(self._df)\n","\n","    def track_error(self, idx_list: t.List[int], error: np.ndarray):\n","        self._error[idx_list] = error\n","\n","    def get_df_with_error(self) -> pd.DataFrame:\n","        df = self._df.copy()\n","        df['error'] = self._error\n","        return df\n","\n","    def __getitem__(self, idx: int) -> t.Tuple[int, _TokenizedText, _TokenizedText]:\n","        record = self._df.iloc[idx]\n","        text_more = str(record['more_toxic'])\n","        text_less = str(record['less_toxic'])\n","\n","        more_input_ids_list, less_input_ids_list = [], []\n","        more_attention_mask_list, less_attention_mask_list = [], []\n","        for chunk in _split_str_to_chunk_list(text_more, chunk_size=self._max_len):\n","            tokenized_chunk = _preprocess_tokenizer_output(self._tokenizer(\n","                chunk,\n","                add_special_tokens=True,\n","                truncation=True,\n","                padding='max_length',\n","                max_length=self._max_len,\n","                return_attention_mask=True))  # type: ignore\n","            more_input_ids_list.append(tokenized_chunk['input_ids'])\n","            more_attention_mask_list.append(tokenized_chunk['attention_mask'])\n","        for chunk in _split_str_to_chunk_list(text_less, chunk_size=self._max_len):\n","            tokenized_chunk = _preprocess_tokenizer_output(self._tokenizer(\n","                chunk,\n","                add_special_tokens=True,\n","                truncation=True,\n","                padding='max_length',\n","                max_length=self._max_len,\n","                return_attention_mask=True))  # type: ignore\n","            less_input_ids_list.append(tokenized_chunk['input_ids'])\n","            less_attention_mask_list.append(tokenized_chunk['attention_mask'])\n","\n","        tokenized_more: _TokenizedText = {\n","            'input_ids': torch.stack(more_input_ids_list, dim=0),\n","            'attention_mask': torch.stack(more_attention_mask_list, dim=0),\n","        }\n","        tokenized_less: _TokenizedText = {\n","            'input_ids': torch.stack(less_input_ids_list, dim=0),\n","            'attention_mask': torch.stack(less_attention_mask_list, dim=0),\n","        }\n","\n","        return idx, tokenized_more, tokenized_less\n"]},{"cell_type":"markdown","metadata":{},"source":["### Model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T21:30:44.848904Z","iopub.status.busy":"2021-12-13T21:30:44.848506Z","iopub.status.idle":"2021-12-13T21:30:44.859529Z","shell.execute_reply":"2021-12-13T21:30:44.858898Z","shell.execute_reply.started":"2021-12-13T21:30:44.848854Z"},"papermill":{"duration":0.020746,"end_time":"2021-12-13T15:32:53.724218","exception":false,"start_time":"2021-12-13T15:32:53.703472","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["class WeightedAverageLinearRegressor(torch.nn.Linear):\n","\n","    def __init__(self, in_features: int, device: t.Optional[str] = None, dtype: t.Optional[str] = None):\n","        super().__init__(in_features=in_features, out_features=1, bias=False, device=device, dtype=dtype)\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        return torch_f.linear(x, torch_f.softmax(self.weight, dim=1), self.bias)\n","\n","\n","class AttentionRegressor(torch.nn.Module):\n","\n","    def __init__(self, in_features: int) -> None:\n","        super().__init__()\n","        self.attention = torch.nn.Linear(in_features=in_features, out_features=in_features, bias=False)\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        weight = self.attention(x)\n","        return (x * torch_f.softmax(weight, dim=1)).sum(dim=1)\n","\n","\n","class Model(torch.nn.Module):\n","\n","    def __init__(self, checkpoint: str, output_logits: int, num_classes: int):\n","        super(Model, self).__init__()\n","        self.encoder = AutoModel.from_pretrained(checkpoint, return_dict=False)\n","        self.feature_regressor = torch.nn.Sequential(\n","            torch.nn.Linear(output_logits, 128),\n","            torch.nn.ReLU(),\n","            torch.nn.Linear(128, num_classes),\n","            torch.nn.Sigmoid())\n","        self.score_regressor = torch.nn.Sequential(\n","            torch.nn.Linear(num_classes, 1),\n","            torch.nn.Sigmoid())\n","\n","    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> t.Tuple[torch.Tensor, torch.Tensor]:\n","        _, pooled_output = self.encoder(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask)\n","        features = self.feature_regressor(pooled_output)\n","        return features, self.score_regressor(features)"]},{"cell_type":"markdown","metadata":{},"source":["### Optimizer"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def create_optimizer(model: Model, base_lr: float) -> AdamW:\n","    named_parameters = list(model.named_parameters())    \n","\n","    roberta_parameters = named_parameters[:197]\n","    regressor_parameters = named_parameters[199:]\n","\n","    regressor_group = [params for (name, params) in regressor_parameters]\n","\n","    parameters = []\n","    parameters.append({\"params\": regressor_group})\n","\n","    for layer_num, (name, params) in enumerate(roberta_parameters):\n","        weight_decay = 0.0 if \"bias\" in name else 0.01\n","        lr = base_lr\n","        if layer_num >= 69:        \n","            lr = lr * 2.5\n","        if layer_num >= 133:\n","            lr = lr * 5\n","        parameters.append({\n","            \"params\": params,\n","            \"weight_decay\": weight_decay,\n","            \"lr\": lr\n","        })\n","\n","    return AdamW(parameters)"]},{"cell_type":"markdown","metadata":{},"source":["### Metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T21:30:44.861454Z","iopub.status.busy":"2021-12-13T21:30:44.860954Z","iopub.status.idle":"2021-12-13T21:30:44.878684Z","shell.execute_reply":"2021-12-13T21:30:44.877857Z","shell.execute_reply.started":"2021-12-13T21:30:44.861415Z"},"papermill":{"duration":0.02804,"end_time":"2021-12-13T15:32:53.762852","exception":false,"start_time":"2021-12-13T15:32:53.734812","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["class _Metric:\n","\n","    def compute(self) -> float:\n","        raise NotImplementedError()\n","\n","    def reset(self):\n","        raise NotImplementedError()\n","\n","    def compute_and_reset(self) -> float:\n","        value = self.compute()\n","        self.reset()\n","        return value\n","\n","\n","class Accuracy(_Metric):\n","\n","    def __init__(self):\n","        self._num_correct = 0\n","        self._num_total = 0\n","\n","    def update(self, is_correct_tensor: torch.Tensor):\n","        num_correct = (is_correct_tensor == 1).int().sum().item()\n","        num_total = num_correct + (is_correct_tensor != 1).int().sum().item()\n","        self._num_correct += num_correct\n","        self._num_total += num_total\n","\n","    def compute(self) -> float:\n","        assert self._num_total > 0\n","        return self._num_correct / self._num_total\n","\n","    def reset(self):\n","        self._num_correct = 0\n","        self._num_total = 0\n","\n","\n","class _FloatListMetric(_Metric):\n","\n","    def __init__(self):\n","        self._value_list: t.List[float] = []\n","\n","    def update(self, value_tensor: torch.Tensor):\n","        self._value_list.extend(value_tensor.flatten().tolist())\n","\n","    def reset(self):\n","        self._value_list.clear()\n","\n","\n","class FloatListMean(_FloatListMetric):\n","\n","    def compute(self) -> float:\n","        return statistics.mean(self._value_list)\n","\n","\n","class FloatListStd(_FloatListMetric):\n","\n","    def compute(self) -> float:\n","        return statistics.stdev(self._value_list)"]},{"cell_type":"markdown","metadata":{},"source":["### Loggers"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["_C = t.TypeVar('_C')\n","\n","\n","class ContextManagerList(t.Generic[_C]):\n","\n","    def __init__(self, cm_list: t.List[t.ContextManager[_C]]):\n","        self._cm_list = cm_list\n","\n","    def __enter__(self) -> t.List[_C]:\n","        return [cm.__enter__() for cm in self._cm_list]\n","\n","    def __exit__(self, *args, **kwargs):\n","        for cm in self._cm_list:\n","            cm.__exit__(*args, **kwargs)\n","\n","\n","class Logger:\n","\n","    def __enter__(self) -> Logger:\n","        return self\n","\n","    def __exit__(self, *args, **kwargs):\n","        pass\n","\n","    def log_params(self, params: t.Dict[str, t.Any]):\n","        raise NotImplementedError()\n","\n","    def log_metrics(self, step: int, metrics: t.Dict[str, float]):\n","        raise NotImplementedError()\n","\n","\n","class StdOutLogger(Logger):\n","\n","    def log_params(self, params: t.Dict[str, t.Any]):\n","        print('Using params:')\n","        for param in sorted(params.keys()):\n","            print(f'\\t{param} = {params[param]}')\n","\n","    def log_metrics(self, step: int, metrics: t.Dict[str, float]):\n","        print(f'Step {step} metrics:')\n","        for m in sorted(metrics.keys()):\n","            print(f'\\t{m} = {metrics[m]:.8f}')\n","\n","\n","class TensorBoardLogger(Logger):\n","\n","    def __init__(self, log_dir: str, metric_whitelist: t.Optional[t.Set[str]] = None) -> None:\n","        self._metric_whitelist = metric_whitelist\n","        self._writer = SummaryWriter(log_dir=log_dir)\n","\n","    def log_params(self, params: t.Dict[str, t.Any]):\n","        pass  # TODO: handle hyperparams properly.\n","\n","    def log_metrics(self, step: int, metrics: t.Dict[str, float]):\n","        for metric_key, metric_val in metrics.items():\n","            if self._metric_whitelist is None or metric_key in self._metric_whitelist:\n","                self._writer.add_scalar(tag=metric_key, scalar_value=metric_val, global_step=step)\n","\n","\n","class WAndBLogger(Logger):\n","\n","    def __init__(self, user_name: str, api_key: str, project: str, run_id: str):\n","        wandb.login(key=api_key)\n","        self._user_name = user_name\n","        self._project = project\n","        self._run_id = run_id\n","        self._run: t.Optional[WAndBRun] = None\n","    \n","    @property\n","    def run(self) -> WAndBRun:\n","        assert self._run is not None\n","        return self._run\n","\n","    def __enter__(self) -> WAndBLogger:\n","        self._run = wandb.init(project=self._project, entity=self._user_name, run_id=self._run_id)\n","        return self\n","    \n","    def log_params(self, params: t.Dict[str, t.Any]):\n","        self.run.config.update(params)\n","\n","    def log_metrics(self, step: int, metrics: t.Dict[str, float]):\n","        self.run.log(step=step, data=metrics)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def margin_ranking_and_mse_loss(\n","        features_pred_more: torch.Tensor,\n","        features_target_more: torch.Tensor,\n","        features_pred_less: torch.Tensor,\n","        features_target_less: torch.Tensor,\n","        scores_more: torch.Tensor,\n","        scores_less: torch.Tensor,\n","        margin: torch.Tensor,\n","        mse_frac: float,\n","        device: str) -> torch.Tensor:\n","    more_mse_loss = torch_f.mse_loss(features_pred_more, features_target_more)\n","    less_mse_loss = torch_f.mse_loss(features_pred_less, features_target_less)\n","    ranking_loss = torch.maximum(torch.tensor(0.0, device=device), scores_less - scores_more + margin).mean()\n","    return (more_mse_loss + less_mse_loss) / 2 * mse_frac + ranking_loss * (1.0 - mse_frac)"]},{"cell_type":"markdown","metadata":{},"source":["### Iteration functions"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T21:30:44.881104Z","iopub.status.busy":"2021-12-13T21:30:44.880154Z","iopub.status.idle":"2021-12-13T21:30:44.914119Z","shell.execute_reply":"2021-12-13T21:30:44.913266Z","shell.execute_reply.started":"2021-12-13T21:30:44.881064Z"},"papermill":{"duration":0.044539,"end_time":"2021-12-13T15:32:53.818306","exception":false,"start_time":"2021-12-13T15:32:53.773767","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def do_train_iteration(\n","        data_loader: DataLoader,\n","        model: Model,\n","        device: str,\n","        optimizer: Optimizer,\n","        scheduler: CosineAnnealingWarmRestarts,\n","        loss_mse_frac: float,\n","        train_margin_list: t.Optional[t.List[float]] = None,\n","        train_decision_margin: float = 0.3,\n","        accumulate_gradient_steps: int = 1,\n","        num_steps: t.Optional[int] = None) -> t.Dict[str, float]:\n","    sampler: RandomSubsetPerEpochSampler = t.cast(RandomSubsetPerEpochSampler, data_loader.sampler)\n","    loss_metric = FloatListMean()\n","    train_score_mean = FloatListMean()\n","    train_score_std = FloatListStd()\n","    if train_margin_list is None:\n","        train_margin_list = [train_decision_margin]\n","    assert train_decision_margin in train_margin_list\n","    train_accuracy_dict = {m: Accuracy() for m in train_margin_list}\n","\n","    model.train()\n","    data_iter = tqdm(data_loader, desc='Training', total=len(data_loader))\n","    for step, (tokenized_text_more, tokenized_text_less, features_target_more, features_target_less, diff) in enumerate(data_iter):\n","        (\n","            input_ids_more,\n","            attention_mask_more,\n","            input_ids_less,\n","            attention_mask_less,\n","            features_target_more,\n","            features_target_less,\n","            diff,\n","        ) = (\n","            tokenized_text_more['input_ids'].to(device),\n","            tokenized_text_more['attention_mask'].to(device),\n","            tokenized_text_less['input_ids'].to(device),\n","            tokenized_text_less['attention_mask'].to(device),\n","            features_target_more.to(device),\n","            features_target_less.to(device),\n","            diff.to(device),\n","        )\n","        features_pred_more, score_more = model(input_ids_more, attention_mask_more)\n","        features_pred_less, score_less = model(input_ids_less, attention_mask_less)\n","\n","        loss = margin_ranking_and_mse_loss(\n","            features_pred_more=features_pred_more,\n","            features_target_more=features_target_more,\n","            features_pred_less=features_pred_less,\n","            features_target_less=features_target_less,\n","            scores_more=score_more,\n","            scores_less=score_less,\n","            margin=train_decision_margin * diff,\n","            mse_frac=loss_mse_frac,\n","            device=device)\n","        loss.backward()\n","\n","        if (step + 1) % accumulate_gradient_steps == 0:\n","            optimizer.step()\n","            optimizer.zero_grad()\n","            scheduler.step()\n","\n","        with torch.no_grad():\n","            score_more_cpu = score_more.cpu()\n","            train_score_mean.update(score_more_cpu)\n","            train_score_std.update(score_more_cpu)\n","            score_less_cpu = score_less.cpu()\n","            train_score_mean.update(score_less_cpu)\n","            train_score_std.update(score_less_cpu)\n","\n","            loss_metric.update(loss.cpu())\n","            for m, a in train_accuracy_dict.items():\n","                a.update(score_more - score_less > m)\n","        accuracy_str = ', '.join([f'acc_{m}: {train_accuracy_dict[m].compute():.4f}' for m in sorted(train_accuracy_dict.keys())])\n","        data_iter.set_description(f'Training, epoch: {sampler.real_epoch} [{sampler.frac_consumed:.4f}], loss: {loss_metric.compute():.4f}, {accuracy_str}')\n","\n","        if num_steps is not None and step >= num_steps - 1:\n","            break\n","\n","    train_metrics_to_track = {f'train_accuracy_{m}': a.compute() for m, a in train_accuracy_dict.items()}\n","    loss_val = loss_metric.compute_and_reset()\n","    return {\n","        'train_loss': loss_val,\n","        'train_score_mean': train_score_mean.compute(),\n","        'train_score_std': train_score_std.compute(),\n","        **train_metrics_to_track,\n","    }\n","\n","\n","def do_valid_iteration(\n","        data_loader: DataLoader,\n","        model: Model,\n","        device: str,\n","        margin_list: t.Optional[t.List[float]] = None,\n","        decision_margin: float = 0.0) -> t.Tuple[float, t.Dict[str, float]]:\n","    if margin_list is None:\n","        margin_list = [decision_margin]\n","    assert decision_margin in margin_list\n","    accuracy_dict = {margin: Accuracy() for margin in margin_list}\n","    valid_score_mean = FloatListMean()\n","    valid_score_std = FloatListStd()\n","    model.eval()\n","    with torch.no_grad():\n","        it = tqdm(data_loader, desc='Validation.')\n","        for idx_list, tokenized_text_more, tokenized_text_less, slice_list_more, slice_list_less in it:\n","            _, score_more = model(\n","                tokenized_text_more['input_ids'].to(device),\n","                tokenized_text_more['attention_mask'].to(device),)\n","            _, score_less = model(\n","                tokenized_text_less['input_ids'].to(device),\n","                tokenized_text_less['attention_mask'].to(device),)\n","            score_more = torch.cat([torch.max(score_more[s], dim=0, keepdim=True)[0] for s in slice_list_more], dim=0)\n","            score_less = torch.cat([torch.max(score_less[s], dim=0, keepdim=True)[0] for s in slice_list_less], dim=0)\n","            score_more_cpu = score_more.cpu()\n","            valid_score_mean.update(score_more_cpu)\n","            valid_score_std.update(score_more_cpu)\n","            score_less_cpu = score_less.cpu()\n","            valid_score_mean.update(score_less_cpu)\n","            valid_score_std.update(score_less_cpu)\n","            for margin, accuracy_metric in accuracy_dict.items():\n","                accuracy_metric.update(((score_more - score_less) > margin).cpu())\n","            error_tensor = torch.maximum(torch.zeros_like(score_less_cpu), score_less_cpu - score_more_cpu)\n","            if len(error_tensor.shape) > 1:\n","                error_tensor = error_tensor.squeeze(1)\n","            data_loader.dataset.track_error(idx_list, error_tensor.numpy())\n","            accuracy_str = ', '.join([f'acc_{m}: {accuracy_dict[m].compute():.4f}' for m in sorted(accuracy_dict.keys())])\n","            it.set_description(f'Validation. {accuracy_str}')\n","    score_dict_to_track = {\n","        'valid_score_mean': valid_score_mean.compute(),\n","        'valid_score_std': valid_score_std.compute(),\n","    }\n","    accuracy_dict_to_track =  {f'valid_accuracy_{m}': a.compute() for m, a in accuracy_dict.items()}\n","    return accuracy_dict[decision_margin].compute(), {**score_dict_to_track, **accuracy_dict_to_track}"]},{"cell_type":"markdown","metadata":{},"source":["### Main function"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T21:30:45.729874Z","iopub.status.busy":"2021-12-13T21:30:45.729205Z","iopub.status.idle":"2021-12-13T21:30:45.744702Z","shell.execute_reply":"2021-12-13T21:30:45.74378Z","shell.execute_reply.started":"2021-12-13T21:30:45.729823Z"},"papermill":{"duration":0.028452,"end_time":"2021-12-13T15:32:56.900841","exception":false,"start_time":"2021-12-13T15:32:56.872389","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["def main(\n","        train_df: pd.DataFrame,\n","        valid_df: pd.DataFrame,\n","        cls_list: t.List[str],\n","        from_checkpoint: str,\n","        to_checkpoint: str,\n","        error_artifact_dir_path: Path,\n","        logger_list: t.List[Logger],\n","        num_epochs: int,\n","        batch_size: int,\n","        max_len: int,\n","        num_workers: int,\n","        device: str,\n","        output_logits: int,\n","        dropout: float,\n","        lr: float,\n","        num_warmup_steps: int,\n","        # t_0: int,\n","        loss_mse_frac: float,\n","        train_margin_list: t.List[float],\n","        train_decision_margin: float,\n","        valid_margin_list: t.List[float],\n","        valid_decision_margin: float,\n","        validate_every_n_samples_init: int,\n","        validate_every_n_samples_thresholded_list: t.List[t.Tuple[float, int]],\n","        accumulate_gradient_steps: int = 1,\n","        score_weight_pow: int = 2,\n","        score_weight_eps: float = 1e-4,\n","        use_score_as_weights_from_epoch: int = 1):    \n","    for logger in logger_list:\n","        logger.log_params({\n","            'from_checkpoint': from_checkpoint,\n","            'lr': lr,\n","            'batch_size': batch_size,\n","            'dropout': dropout,\n","            'output_logits': output_logits,\n","            'max_len': max_len,\n","            'num_epochs': num_epochs,\n","            'optimizer': 'adam_w',\n","            'scheduler': 'cosine_annealing_warm_restarts',\n","            'accumulate_gradient_steps': accumulate_gradient_steps,\n","        })\n","    \n","    tokenizer = AutoTokenizer.from_pretrained(from_checkpoint)\n","    train_dataset = TrainDataset(\n","        train_df,\n","        tokenizer=tokenizer,\n","        max_len=max_len,\n","        cls_list=cls_list,\n","        score_weight_pow=score_weight_pow,\n","        score_weight_eps=score_weight_eps,\n","        augmentation_list=[\n","        ])\n","    valid_dataset = ValidDataset(\n","        valid_df,\n","        tokenizer=tokenizer,\n","        max_len=max_len)\n","    sampler = MetricBasedRandomSubsetPerEpochSampler(\n","        data_source=train_dataset,\n","        samples_per_epoch=validate_every_n_samples_init,\n","        thresholded_samples_per_epoch_list=validate_every_n_samples_thresholded_list)\n","    train_data_loader = DataLoader(\n","        dataset=train_dataset,\n","        sampler=sampler,\n","        batch_size=batch_size,\n","        num_workers=num_workers,\n","        pin_memory=True)\n","    valid_data_loader = DataLoader(\n","        dataset=valid_dataset,\n","        batch_size=batch_size * 2,\n","        num_workers=num_workers,\n","        shuffle=False,\n","        pin_memory=True,\n","        collate_fn=valid_collate_fn)  # type: ignore\n","    model = Model(checkpoint=from_checkpoint, output_logits=output_logits, num_classes=len(cls_list)).to(device)\n","    optimizer = create_optimizer(model=model, base_lr=lr)\n","    scheduler = get_cosine_schedule_with_warmup(\n","        optimizer=optimizer,\n","        num_warmup_steps=num_warmup_steps,\n","        num_training_steps=len(train_data_loader) // accumulate_gradient_steps * num_epochs)\n","\n","    best_accuracy = 0.0\n","    while sampler.real_epoch < num_epochs:\n","        if use_score_as_weights_from_epoch is not None and sampler.step >= use_score_as_weights_from_epoch:\n","            train_dataset.start_using_score_as_weights()\n","        train_metrics_to_track = do_train_iteration(\n","            data_loader=train_data_loader,\n","            model=model,\n","            device=device,\n","            optimizer=optimizer,\n","            scheduler=scheduler,\n","            train_margin_list=train_margin_list,\n","            train_decision_margin=train_decision_margin,\n","            loss_mse_frac=loss_mse_frac,\n","            accumulate_gradient_steps=accumulate_gradient_steps)\n","        accuracy, valid_metrics_to_track = do_valid_iteration(\n","            data_loader=valid_data_loader,\n","            model=model,\n","            device=device,\n","            margin_list=valid_margin_list,\n","            decision_margin=valid_decision_margin)\n","        sampler.adjust_samples_per_epoch(accuracy)\n","        for logger in logger_list:\n","            logger.log_metrics(step=sampler.step, metrics={\n","                **train_metrics_to_track,\n","                **valid_metrics_to_track,\n","            })\n","        if accuracy > best_accuracy:\n","            print(f'Best accuracy improved from {best_accuracy} to {accuracy}. Saving the model.')\n","            torch.save(model.state_dict(), to_checkpoint)\n","            best_accuracy = accuracy\n","        valid_dataset.get_df_with_error().to_csv(str(error_artifact_dir_path / f'{sampler.step}.csv'), index=False)"]},{"cell_type":"markdown","metadata":{},"source":["### Parameter definitions"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T21:30:44.917294Z","iopub.status.busy":"2021-12-13T21:30:44.917102Z","iopub.status.idle":"2021-12-13T21:30:44.927821Z","shell.execute_reply":"2021-12-13T21:30:44.926808Z","shell.execute_reply.started":"2021-12-13T21:30:44.917271Z"},"papermill":{"duration":0.018228,"end_time":"2021-12-13T15:32:53.847052","exception":false,"start_time":"2021-12-13T15:32:53.828824","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Parameters\n","\n","IS_KAGGLE = False\n","MAX_LEN = 256\n","CLS_LIST = [\n","    'target', 'severe_toxicity', 'obscene',\n","    'identity_attack', 'insult', 'threat', 'sexual_explicit'\n","]\n","\n","ROOT_DIR_PATH = Path('/kaggle') if IS_KAGGLE else Path('/home/jovyan/jigsaw-toxic')\n","DATA_DIR_PATH = ROOT_DIR_PATH / ('input' if IS_KAGGLE else 'data/datasets')\n","TENSORBOARD_DIR_PATH = ROOT_DIR_PATH / 'working/tensorboard' if IS_KAGGLE else Path('/home/jovyan/tensorboard')\n","ARTIFACT_DIR_PATH = ROOT_DIR_PATH / 'working/artifacts' if IS_KAGGLE else ROOT_DIR_PATH / 'artifacts'\n","\n","TASK_NAME = 'margin-ranking'\n","DATASET_NAME = 'ubtc'\n","RUN_NAME = 'multireg-w50-cos_warmup-opt-2ep-ut_roberta-valfreq_dynamic_v1'\n","SEED = 42\n","\n","DATASET_DIR_PATH = DATA_DIR_PATH / DATASET_NAME\n","TRAIN_CSV_PATH = DATASET_DIR_PATH / 'train_multireg_w50.csv'\n","VALID_CSV_PATH = DATASET_DIR_PATH / 'valid.csv'\n","MODEL_NAME = f'{DATASET_NAME}-{RUN_NAME}-seed_{SEED}'\n","NOTEBOOK_CHECKPOINT_DIR_PATH = Path(f'.checkpoints/{TASK_NAME}')\n","\n","MODELS_DIR_PATH = ROOT_DIR_PATH / ('working/models' if IS_KAGGLE else 'models')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if not IS_KAGGLE:\n","    os.makedirs(NOTEBOOK_CHECKPOINT_DIR_PATH, exist_ok=True)\n","    shutil.copyfile('margin-ranking.ipynb', NOTEBOOK_CHECKPOINT_DIR_PATH / f'{RUN_NAME}.ipynb')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T21:30:44.929959Z","iopub.status.busy":"2021-12-13T21:30:44.929477Z","iopub.status.idle":"2021-12-13T21:30:45.598668Z","shell.execute_reply":"2021-12-13T21:30:45.597732Z","shell.execute_reply.started":"2021-12-13T21:30:44.929844Z"},"papermill":{"duration":0.676761,"end_time":"2021-12-13T15:32:54.534335","exception":false,"start_time":"2021-12-13T15:32:53.857574","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Create potentially missing directories\n","!mkdir -p $MODELS_DIR_PATH\n","!mkdir -p $ARTIFACT_DIR_PATH"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T21:30:45.600795Z","iopub.status.busy":"2021-12-13T21:30:45.600504Z","iopub.status.idle":"2021-12-13T21:30:45.703839Z","shell.execute_reply":"2021-12-13T21:30:45.703089Z","shell.execute_reply.started":"2021-12-13T21:30:45.600761Z"},"papermill":{"duration":2.228002,"end_time":"2021-12-13T15:32:56.773438","exception":false,"start_time":"2021-12-13T15:32:54.545436","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Read dataframes\n","\n","train_df = pd.read_csv(TRAIN_CSV_PATH)\n","valid_df = pd.read_csv(VALID_CSV_PATH)"]},{"cell_type":"markdown","metadata":{},"source":["### Entrypoint"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-13T21:30:45.747655Z","iopub.status.busy":"2021-12-13T21:30:45.74677Z","iopub.status.idle":"2021-12-13T21:38:07.729693Z","shell.execute_reply":"2021-12-13T21:38:07.728387Z","shell.execute_reply.started":"2021-12-13T21:30:45.747614Z"},"papermill":{"duration":null,"end_time":null,"exception":false,"start_time":"2021-12-13T15:32:56.912308","status":"running"},"tags":[],"trusted":true},"outputs":[],"source":["error_artifact_dir_path = ARTIFACT_DIR_PATH / MODEL_NAME\n","error_artifact_dir_path.mkdir(exist_ok=True)\n","with ContextManagerList([\n","            TensorBoardLogger(\n","                log_dir=str(TENSORBOARD_DIR_PATH / f'jt-{MODEL_NAME}'),\n","                metric_whitelist={\n","                    'train_loss',\n","                    'train_accuracy_0.0',\n","                    'train_accuracy_0.5',\n","                    'valid_accuracy_0.0',\n","                    'valid_accuracy_0.5',\n","                },\n","            ),\n","        ]) as logger_list:\n","    main(\n","        train_df=train_df,\n","        valid_df=valid_df,\n","        cls_list=CLS_LIST,\n","        from_checkpoint='unitary/unbiased-toxic-roberta',\n","        to_checkpoint=str(MODELS_DIR_PATH / f'{MODEL_NAME}.pt'),\n","        error_artifact_dir_path=error_artifact_dir_path,\n","        logger_list=logger_list,\n","        num_epochs=2,\n","        batch_size=4,\n","        max_len=MAX_LEN,\n","        num_workers=8,\n","        device='cuda',\n","        output_logits=768,\n","        dropout=0.6,\n","        lr=2e-5,\n","        num_warmup_steps=1 * 128,\n","        loss_mse_frac=0.5,\n","        train_margin_list=[0.0, 0.5, 1.0],\n","        train_decision_margin=1.0,\n","        valid_margin_list=[0.0, 0.5, 1.0],\n","        valid_decision_margin=0.0,\n","        validate_every_n_samples_init=4 * 16 * 64,\n","        validate_every_n_samples_thresholded_list=[\n","            # (score, batch_size * grad_accum_steps * num_of_batches)\n","            (0.735, 4 * 16 * 32),\n","            (0.7375, 4 * 16 * 16),\n","            (0.74, 4 * 16 * 8),\n","            (0.742, 4 * 16 * 4),\n","            (0.743, 4 * 16 * 2),\n","        ],\n","        accumulate_gradient_steps=16,\n","        use_score_as_weights_from_epoch=100,\n","        score_weight_pow=2,\n","        score_weight_eps=1e-4)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":4}
